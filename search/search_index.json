{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])"},"docs":[{"location":"","text":"Welcome To SosoTech We specialize in IT training and Hands-on. What we do We train New and Working IT Professionals on the following fields: DevOps SRE Cloud Engineers AI Engineers","title":"About Us"},{"location":"#welcome-to-sosotech","text":"We specialize in IT training and Hands-on.","title":"Welcome To SosoTech"},{"location":"#what-we-do","text":"We train New and Working IT Professionals on the following fields: DevOps SRE Cloud Engineers AI Engineers","title":"What we do"},{"location":"Getting-Started/local-install/","text":"Locally System Requirements Windows OS systems will be the preferred OS that will be used throughout this program. Students with MaC OS can still use their MAC pcs, but may have some diffuculties with running some Windows-specific commands. Install WSL Requirments: windows 10 or greater In the windows search, Type: 'feat' Click -> Turning windows features off or on, and check select [x] virtual machine platform [x] windows Subsystem for Linux Some common WSL commands, use PowerShell $ winver $ wsl -l -v $ wsl --set-version Ubuntu-20.04 2 $ wsl --set-default-version 2 See Youtube link: Click link Install Linux Kernel Step 4 - Download the Linux kernel update package Now go to searchBar and search store Install Chocolatey Open Powershell as Administrative user Link Click-Link Run Command: Get-ExecutionPolicy If it returns Restricted, then run: Set-ExecutionPolicy AllSigned Now run the following command: Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1')) Install AWSCli Run command: msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Install Choco Packages choco install kubectl choco install k9s choco install terraform choco install kubens kubectx Install Docker Desktop/k8s use link: Click link to install Restart System","title":"Local-Installation"},{"location":"Getting-Started/local-install/#locally","text":"System Requirements Windows OS systems will be the preferred OS that will be used throughout this program. Students with MaC OS can still use their MAC pcs, but may have some diffuculties with running some Windows-specific commands.","title":"Locally"},{"location":"Getting-Started/local-install/#install-wsl","text":"Requirments: windows 10 or greater In the windows search, Type: 'feat' Click -> Turning windows features off or on, and check select [x] virtual machine platform [x] windows Subsystem for Linux Some common WSL commands, use PowerShell $ winver $ wsl -l -v $ wsl --set-version Ubuntu-20.04 2 $ wsl --set-default-version 2 See Youtube link: Click link Install Linux Kernel Step 4 - Download the Linux kernel update package Now go to searchBar and search store","title":"Install WSL"},{"location":"Getting-Started/local-install/#install-chocolatey","text":"Open Powershell as Administrative user Link Click-Link Run Command: Get-ExecutionPolicy If it returns Restricted, then run: Set-ExecutionPolicy AllSigned Now run the following command: Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))","title":"Install Chocolatey"},{"location":"Getting-Started/local-install/#install-awscli","text":"Run command: msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi","title":"Install AWSCli"},{"location":"Getting-Started/local-install/#install-choco-packages","text":"choco install kubectl choco install k9s choco install terraform choco install kubens kubectx","title":"Install Choco Packages"},{"location":"Getting-Started/local-install/#install-docker-desktopk8s","text":"use link: Click link to install Restart System","title":"Install Docker Desktop/k8s"},{"location":"Getting-Started/remote-install/","text":"Remote Server Installation links Install IAM EKS authenticator Right-Click to open Link in a New Tab curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator sudo mv ./aws-iam-authenticator /usr/local/bin aws-iam-authenticator help Install docker in ec2 Right-Click to open Link in a New Tab Install AWSCLI Right-Click to open Link in a New Tab Install Terraform Right-Click to open Link in a New Tab Install Kubens + kubectx Right-Click to open Link in a New Tab Install HelM Right-Click to open Link in a New Tab curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Install Kubectl Right-Click to open Link in a New Tab curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(<kubectl.sha256) kubectl\" | sha256sum --check sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --client --output=yaml Install MkDocs Right-Click to open Link in a New Tab pip install mkdocs mkdocs helm mkdocs new my-soso-package cd my-soso-package tree Install PiP on RHeL REDHAT sudo yum info python*-pip //get the pip version, then install the version sudo yum install python39-pip python3 --version curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user Install Trivy Right-Click to open Link in a New Tab sudo yum -y update sudo wget https://github.com/aquasecurity/trivy/releases/download/v0.18.3/trivy_0.18.3_Linux-64bit.deb sudo dpkg -i trivy_0.18.3_Linux-64bit.deb trivy i nginx //scanning nginx image trivy i nginx | grep -i critical trivy i nginx:alpine | grep -i critical","title":"Remote Installation"},{"location":"Getting-Started/remote-install/#remote-server","text":"","title":"Remote Server"},{"location":"Getting-Started/remote-install/#installation-links","text":"","title":"Installation links"},{"location":"Getting-Started/remote-install/#install-iam-eks-authenticator","text":"Right-Click to open Link in a New Tab curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator sudo mv ./aws-iam-authenticator /usr/local/bin aws-iam-authenticator help","title":"Install IAM EKS authenticator"},{"location":"Getting-Started/remote-install/#install-docker-in-ec2","text":"Right-Click to open Link in a New Tab","title":"Install docker in ec2"},{"location":"Getting-Started/remote-install/#install-awscli","text":"Right-Click to open Link in a New Tab","title":"Install AWSCLI"},{"location":"Getting-Started/remote-install/#install-terraform","text":"Right-Click to open Link in a New Tab","title":"Install Terraform"},{"location":"Getting-Started/remote-install/#install-kubens-kubectx","text":"Right-Click to open Link in a New Tab","title":"Install Kubens + kubectx"},{"location":"Getting-Started/remote-install/#install-helm","text":"Right-Click to open Link in a New Tab curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh","title":"Install HelM"},{"location":"Getting-Started/remote-install/#install-kubectl","text":"Right-Click to open Link in a New Tab curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(<kubectl.sha256) kubectl\" | sha256sum --check sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --client --output=yaml","title":"Install Kubectl"},{"location":"Getting-Started/remote-install/#install-mkdocs","text":"Right-Click to open Link in a New Tab pip install mkdocs mkdocs helm mkdocs new my-soso-package cd my-soso-package tree","title":"Install MkDocs"},{"location":"Getting-Started/remote-install/#install-pip-on-rhel","text":"REDHAT sudo yum info python*-pip //get the pip version, then install the version sudo yum install python39-pip python3 --version curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user","title":"Install PiP on RHeL"},{"location":"Getting-Started/remote-install/#install-trivy","text":"Right-Click to open Link in a New Tab sudo yum -y update sudo wget https://github.com/aquasecurity/trivy/releases/download/v0.18.3/trivy_0.18.3_Linux-64bit.deb sudo dpkg -i trivy_0.18.3_Linux-64bit.deb trivy i nginx //scanning nginx image trivy i nginx | grep -i critical trivy i nginx:alpine | grep -i critical","title":"Install Trivy"},{"location":"weekly/AI/ai/","text":"Artificial Intelligence Eventhough AI Machines seem to be a threat to humanity, yet, they cannot beat the power of human intelligence, Machines will never beat the creative or imaginative ability of humans. Always remember, humans created the machine. What is AI? \"It is a branch of computer science by which we can create intelligent machines which can behave like a human, think like humans, and able to make decisions.\" setup google codeLab Open-Link Theres always been this lask of understanding between machine learning and artificial intelligence. Put in context, artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience. Artificial Neural Network (ANN) Artificial Neural Networks the most important part of the deep learning model, they are the brain inspired deep learning tool which replicate the way humans learn. The Neuron In the Human Body, the Neuron is a part of the brain and nervous system. The three broad functions of the CNS are: To take in sensory information process information, and To send out motor signals. But Now How do Neurons Connect? Neurons communicate with each other by sending chemicals, called neurotransmitters, across a tiny space called a synapse, between the axons and dendrites of nearby neurons. So, now that we have that understanding of a human neuron, and how the neurons connect, the goal of AI was to replicate the way the human brain learns. In another sence, create machines that would have the ability to learn, adapt, and make decisions. Neurons in AI As seen in the diagram below, the Input Values are independent Variables of [1, 2, and m]. These variables need to be standardized to make sure all variables contribute evenly to a scale when items are added together, or to make it easier to interpret results of a regression or other analysis. The output Value can be: continuous, binary or a catagorical variable. Neural network is made of interconnected neurons. Each of them is characterized by its weight, bias, and activation function. Weight Weight is the parameter within a neural network that transforms input data within the network's hidden layers. Bias Bias in AI is when the machine gives consistently different outputs for one group of people compared to another. Typically these bias outputs follow classical societal biases like race, gender, biological sex, nationality or age. Neural Network Activation Function? An Activation Function is what makes the decision, if a neuron should be activated or not. This means that an ctivation Function is a mathematical \u201cgate\u201d that decide whether the neuron\u2019s input to the network is important or not in the process of prediction using simpler mathematical operations. Types of Activation Functions Threshold Activation Function is a commonly used activation function.It gives 1 as output of the input is either 0 or positive. If the input is negative, it gives 0 as output. Sigmoid Activation Function Sigmoid Function is the most commonly used activation function in neural networks. It is the go-to model when it comes to predicting the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid has the ability to takes any real value as input and outputs values in the range of 0 to 1. Rectifier Activation Function is the most popularly used activation function in the areas of convolutional neural networks and deep learning. It is of the form: Hyperbolic Tangent Activation Function It is bipolar in nature. Similar to sigmoid, except a different output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0. How do Neural Networks Work? Example Case: Real Estate Price prediction using a trained NN. Looking at the Red arrows, this synapses connects Age, Area, bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Newer, That have at least 3 bedrooms or more, And which the homes have a larger area. Looking at the Blue arrows, this synapses connects Schools Ratings and bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Families with Kids in school Homes that have more bedrooms for the parents and kids. Looking at the Green arrows, this synapses connects Just the Age of the property. So, this neuron is paying attention to properties that maybe: Newly built, or Properties that are historic, valuable properties, like above 100years old. A rectifier function can be used here, stating that, the moment the property turns 100 years, the rectifier functions kicks from 0 to 1. Looking at the Yellow arrows, this synapses connects Area (SqFt) and Bedrooms of the property. So, this neuron is paying attention to properties that maybe: The homes here have a larger Area Sq and have many rooms. These may be homes that are mansions. Looking at the black arrows, this synapses connects all the input functions. So, this neuron is paying attention to properties because all the inputs match the criteria for the price of the property So, all these different neuron communications will predict the price of the property How do Neural Networks Learn? ANN can be categorized by how the data flows from the input node to the output node. B Some of the neural networks are some examples: Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Backpropagation algorithm Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. In simple terms, you can think of the data flowing from the input node to the output node through many different paths in the neural network. Only one path is the correct one that maps the input node to the correct output node. To find this path, the neural network uses a feedback loop, which works as follows: Each node makes a guess about the next node in the path. It checks if the guess was correct. Nodes assign higher weight values to paths that lead to more correct guesses and lower weight values to node paths that lead to incorrect guesses. For the next data point, the nodes make a new prediction using the higher weight paths and then repeat Step 1. Convolutional neural networks The hidden layers in convolutional neural networks perform specific mathematical functions, like summarizing or filtering, called convolutions. They are very useful for image classification because they can extract relevant features from images that are useful for image recognition and classification. The new form is easier to process without losing features that are critical for making a good prediction. Each hidden layer extracts and processes different image features, like edges, color, and depth. Machine Learning in ANNs These ML algorithms help to solve different business problems like Forecasting, Regression, Clustering, Classification, and Associations, etc. Machine learning is divided into mainly four types, which are: Supervised Machine Learning - Ex: Housing Price Prediction. A Supervised machine learning is a technique where we train the machines using the \"labelled\" dataset, and based on the training, the machine predicts the output. In this type of learning, the labelled data specifies that some of the inputs are already mapped to the output. The machine are First trained with the input and corresponding output, and then we ask the machine to predict the output using the test dataset. A classic example will be the example of training a machine with Unsupervised Machine Learning EX: Customer Segmentation and Market Basket Analysis. In unsupervised machine learning, the machine is trained using the unlabeled dataset, and the machine predicts the output without any supervision. In this type, the models are trained with the data that is neither labelled nor classified, and the model acts on that data without any supervision. Unsupervised learning is much similar as a human learns, which makes it closer to the real AI. Semi-Supervised Machine Learning Ex: Lane-Finding on GPS data. It represents the intermediate ground between Supervised and Unsupervised learning algorithms and uses the combination of labelled and unlabeled datasets during the training period. Reinforcement Learning Ex: Drivless Cars. Reinforcement learning works on a feedback-based process, in which an AI agent automatically explore its surrounding by hitting & trail, taking action, learning from experiences, and improving its performance. Agent gets rewarded for each good action and get punished for each bad action; hence the goal of reinforcement learning agent is to maximize the rewards. In reinforcement learning, there is no labelled data like supervised learning, and agents learn from their experiences only. For more on this section, see this javatpoint Link: Click-Link","title":"AI"},{"location":"weekly/AI/ai/#artificial-intelligence","text":"Eventhough AI Machines seem to be a threat to humanity, yet, they cannot beat the power of human intelligence, Machines will never beat the creative or imaginative ability of humans. Always remember, humans created the machine.","title":"Artificial Intelligence"},{"location":"weekly/AI/ai/#what-is-ai","text":"\"It is a branch of computer science by which we can create intelligent machines which can behave like a human, think like humans, and able to make decisions.\" setup google codeLab Open-Link Theres always been this lask of understanding between machine learning and artificial intelligence. Put in context, artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience.","title":"What is AI?"},{"location":"weekly/AI/ai/#artificial-neural-network-ann","text":"Artificial Neural Networks the most important part of the deep learning model, they are the brain inspired deep learning tool which replicate the way humans learn. The Neuron In the Human Body, the Neuron is a part of the brain and nervous system. The three broad functions of the CNS are: To take in sensory information process information, and To send out motor signals. But Now How do Neurons Connect? Neurons communicate with each other by sending chemicals, called neurotransmitters, across a tiny space called a synapse, between the axons and dendrites of nearby neurons. So, now that we have that understanding of a human neuron, and how the neurons connect, the goal of AI was to replicate the way the human brain learns. In another sence, create machines that would have the ability to learn, adapt, and make decisions.","title":"Artificial Neural Network (ANN)"},{"location":"weekly/AI/ai/#neurons-in-ai","text":"As seen in the diagram below, the Input Values are independent Variables of [1, 2, and m]. These variables need to be standardized to make sure all variables contribute evenly to a scale when items are added together, or to make it easier to interpret results of a regression or other analysis. The output Value can be: continuous, binary or a catagorical variable. Neural network is made of interconnected neurons. Each of them is characterized by its weight, bias, and activation function.","title":"Neurons in AI"},{"location":"weekly/AI/ai/#weight","text":"Weight is the parameter within a neural network that transforms input data within the network's hidden layers.","title":"Weight"},{"location":"weekly/AI/ai/#bias","text":"Bias in AI is when the machine gives consistently different outputs for one group of people compared to another. Typically these bias outputs follow classical societal biases like race, gender, biological sex, nationality or age.","title":"Bias"},{"location":"weekly/AI/ai/#neural-network-activation-function","text":"An Activation Function is what makes the decision, if a neuron should be activated or not. This means that an ctivation Function is a mathematical \u201cgate\u201d that decide whether the neuron\u2019s input to the network is important or not in the process of prediction using simpler mathematical operations. Types of Activation Functions Threshold Activation Function is a commonly used activation function.It gives 1 as output of the input is either 0 or positive. If the input is negative, it gives 0 as output. Sigmoid Activation Function Sigmoid Function is the most commonly used activation function in neural networks. It is the go-to model when it comes to predicting the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid has the ability to takes any real value as input and outputs values in the range of 0 to 1. Rectifier Activation Function is the most popularly used activation function in the areas of convolutional neural networks and deep learning. It is of the form: Hyperbolic Tangent Activation Function It is bipolar in nature. Similar to sigmoid, except a different output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.","title":"Neural Network Activation Function?"},{"location":"weekly/AI/ai/#how-do-neural-networks-work","text":"Example Case: Real Estate Price prediction using a trained NN. Looking at the Red arrows, this synapses connects Age, Area, bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Newer, That have at least 3 bedrooms or more, And which the homes have a larger area. Looking at the Blue arrows, this synapses connects Schools Ratings and bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Families with Kids in school Homes that have more bedrooms for the parents and kids. Looking at the Green arrows, this synapses connects Just the Age of the property. So, this neuron is paying attention to properties that maybe: Newly built, or Properties that are historic, valuable properties, like above 100years old. A rectifier function can be used here, stating that, the moment the property turns 100 years, the rectifier functions kicks from 0 to 1. Looking at the Yellow arrows, this synapses connects Area (SqFt) and Bedrooms of the property. So, this neuron is paying attention to properties that maybe: The homes here have a larger Area Sq and have many rooms. These may be homes that are mansions. Looking at the black arrows, this synapses connects all the input functions. So, this neuron is paying attention to properties because all the inputs match the criteria for the price of the property So, all these different neuron communications will predict the price of the property","title":"How do Neural Networks Work?"},{"location":"weekly/AI/ai/#how-do-neural-networks-learn","text":"ANN can be categorized by how the data flows from the input node to the output node. B Some of the neural networks are some examples: Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Backpropagation algorithm Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. In simple terms, you can think of the data flowing from the input node to the output node through many different paths in the neural network. Only one path is the correct one that maps the input node to the correct output node. To find this path, the neural network uses a feedback loop, which works as follows: Each node makes a guess about the next node in the path. It checks if the guess was correct. Nodes assign higher weight values to paths that lead to more correct guesses and lower weight values to node paths that lead to incorrect guesses. For the next data point, the nodes make a new prediction using the higher weight paths and then repeat Step 1. Convolutional neural networks The hidden layers in convolutional neural networks perform specific mathematical functions, like summarizing or filtering, called convolutions. They are very useful for image classification because they can extract relevant features from images that are useful for image recognition and classification. The new form is easier to process without losing features that are critical for making a good prediction. Each hidden layer extracts and processes different image features, like edges, color, and depth.","title":"How do Neural Networks Learn?"},{"location":"weekly/AI/ai/#machine-learning-in-anns","text":"These ML algorithms help to solve different business problems like Forecasting, Regression, Clustering, Classification, and Associations, etc. Machine learning is divided into mainly four types, which are: Supervised Machine Learning - Ex: Housing Price Prediction. A Supervised machine learning is a technique where we train the machines using the \"labelled\" dataset, and based on the training, the machine predicts the output. In this type of learning, the labelled data specifies that some of the inputs are already mapped to the output. The machine are First trained with the input and corresponding output, and then we ask the machine to predict the output using the test dataset. A classic example will be the example of training a machine with Unsupervised Machine Learning EX: Customer Segmentation and Market Basket Analysis. In unsupervised machine learning, the machine is trained using the unlabeled dataset, and the machine predicts the output without any supervision. In this type, the models are trained with the data that is neither labelled nor classified, and the model acts on that data without any supervision. Unsupervised learning is much similar as a human learns, which makes it closer to the real AI. Semi-Supervised Machine Learning Ex: Lane-Finding on GPS data. It represents the intermediate ground between Supervised and Unsupervised learning algorithms and uses the combination of labelled and unlabeled datasets during the training period. Reinforcement Learning Ex: Drivless Cars. Reinforcement learning works on a feedback-based process, in which an AI agent automatically explore its surrounding by hitting & trail, taking action, learning from experiences, and improving its performance. Agent gets rewarded for each good action and get punished for each bad action; hence the goal of reinforcement learning agent is to maximize the rewards. In reinforcement learning, there is no labelled data like supervised learning, and agents learn from their experiences only. For more on this section, see this javatpoint Link: Click-Link","title":"Machine Learning in ANNs"},{"location":"weekly/Azure/azure/","text":"Azure Free azure account and go to the postal link: Free account-Link Portal-Link Whats hierarchy Azure provides four levels of management: management groups -> Subscriptions -> Resource Groups -> Resources Management groups help you manage access, policy, and compliance for multiple subscriptions. All subscriptions in a management group automatically inherit the conditions that are applied to the management group. Subscriptions associate user accounts with the resources that they create. Resource groups are logical containers where you can deploy and manage Azure resources like web apps, databases, and storage accounts. Resources are instances of services that you can create, such as virtual machines, storage, and SQL databases. Set a budget Azure Storage types Azure Storage offers five core services: Blob Storage helps you create data lakes for your analytics needs, and provides storage to build powerful cloud-native and mobile apps. File Storage : Managed file shares for cloud or on-premises deployments. Queue Storage : A messaging store for reliable messaging between application components. Table Storage : A NoSQL store for schemaless storage of structured data. Managed Disk Storage : Block-level storage volumes for Azure VMs. Azure Elastic SAN (preview) : A fully integrated solution that simplifies deploying, scaling, managing, and configuring a SAN in Azure. Azure also offers a list of SQL Data and non-SQL Databases. For More on Azure Storage, see link: Azure Storage Link Azure Networking Service For More on Azure networking, see link: Azure networking Link Azure and Terraform Azure provder Terraform link: Terraform link Azure authentication from the terminal az login az account list az account list --query [*].[name,id] In azure terminal - bash, - Create an AD service principle for RBAC using below command, - Next, go to subscriptions and get the subscription ID. az ad sp create-for-rbac -n \"Terraform-Soso\" --role=\"Contributor\" --scopes=\"/subscriptions/b965a029-32b4-4671-b283-350344c89091\" Go to azure app registration and see that it was created. Copy the info created. In my case: { \"appId\": \"297ec591-b16e-42ce-9e03-1ebd725aa3f9\", \"displayName\": \"Terraform-Soso\", \"password\": \"FZG8Q~-uPKCxRNgIg8JVAKYO2xm4j..S1SpFoaxg\", \"tenant\": \"054da2e5-2fbf-483f-961e-a3b2839bd53c\" } Azure Active Directory Azure AD part of Microsoft Entra , is an identity and Access Management service that provides single sign-on, multifactor authentication, and conditional access to guard against 99.9 percent of cybersecurity attacks. Accounts and subscriptions Account: Subscription: Tenant: Resource Groups: users There are three types of user accounts that you can have in Azure AD: federated synchronized cloud Optional: Create a new tenant for your organization For more info see: To create a new tenant After creating tenant, it comes with nothin installed, so I will have to add subscriptions... Note : You can switch between tenants Storage Storage Access Control (IAM)","title":"Azure"},{"location":"weekly/Azure/azure/#azure","text":"Free azure account and go to the postal link: Free account-Link Portal-Link","title":"Azure"},{"location":"weekly/Azure/azure/#whats-hierarchy","text":"Azure provides four levels of management: management groups -> Subscriptions -> Resource Groups -> Resources Management groups help you manage access, policy, and compliance for multiple subscriptions. All subscriptions in a management group automatically inherit the conditions that are applied to the management group. Subscriptions associate user accounts with the resources that they create. Resource groups are logical containers where you can deploy and manage Azure resources like web apps, databases, and storage accounts. Resources are instances of services that you can create, such as virtual machines, storage, and SQL databases.","title":"Whats hierarchy"},{"location":"weekly/Azure/azure/#set-a-budget","text":"","title":"Set a budget"},{"location":"weekly/Azure/azure/#azure-storage-types","text":"Azure Storage offers five core services: Blob Storage helps you create data lakes for your analytics needs, and provides storage to build powerful cloud-native and mobile apps. File Storage : Managed file shares for cloud or on-premises deployments. Queue Storage : A messaging store for reliable messaging between application components. Table Storage : A NoSQL store for schemaless storage of structured data. Managed Disk Storage : Block-level storage volumes for Azure VMs. Azure Elastic SAN (preview) : A fully integrated solution that simplifies deploying, scaling, managing, and configuring a SAN in Azure. Azure also offers a list of SQL Data and non-SQL Databases. For More on Azure Storage, see link: Azure Storage Link","title":"Azure Storage types"},{"location":"weekly/Azure/azure/#azure-networking-service","text":"For More on Azure networking, see link: Azure networking Link","title":"Azure Networking Service"},{"location":"weekly/Azure/azure/#azure-and-terraform","text":"Azure provder Terraform link: Terraform link","title":"Azure and Terraform"},{"location":"weekly/Azure/azure/#azure-authentication-from-the-terminal","text":"az login az account list az account list --query [*].[name,id] In azure terminal - bash, - Create an AD service principle for RBAC using below command, - Next, go to subscriptions and get the subscription ID. az ad sp create-for-rbac -n \"Terraform-Soso\" --role=\"Contributor\" --scopes=\"/subscriptions/b965a029-32b4-4671-b283-350344c89091\" Go to azure app registration and see that it was created. Copy the info created. In my case: { \"appId\": \"297ec591-b16e-42ce-9e03-1ebd725aa3f9\", \"displayName\": \"Terraform-Soso\", \"password\": \"FZG8Q~-uPKCxRNgIg8JVAKYO2xm4j..S1SpFoaxg\", \"tenant\": \"054da2e5-2fbf-483f-961e-a3b2839bd53c\" }","title":"Azure authentication from the terminal"},{"location":"weekly/Azure/azure/#azure-active-directory","text":"Azure AD part of Microsoft Entra , is an identity and Access Management service that provides single sign-on, multifactor authentication, and conditional access to guard against 99.9 percent of cybersecurity attacks.","title":"Azure Active Directory"},{"location":"weekly/Azure/azure/#accounts-and-subscriptions","text":"Account: Subscription: Tenant: Resource Groups:","title":"Accounts and subscriptions"},{"location":"weekly/Azure/azure/#users","text":"There are three types of user accounts that you can have in Azure AD: federated synchronized cloud","title":"users"},{"location":"weekly/Azure/azure/#optional-create-a-new-tenant-for-your-organization","text":"For more info see: To create a new tenant After creating tenant, it comes with nothin installed, so I will have to add subscriptions... Note : You can switch between tenants","title":"Optional: Create a new tenant for your organization"},{"location":"weekly/Azure/azure/#storage","text":"","title":"Storage"},{"location":"weekly/Azure/azure/#storage-access-control-iam","text":"","title":"Storage Access Control (IAM)"},{"location":"weekly/CICD/cicd/","text":"Install servers Install Individuals servers for: - Nexus - Sonarqube - Jenkins Nexus Centos 7 (Amazon Market place) TCP Port 8081 from MyIP and Jenkins-SG #!/bin/bash yum install java-1.8.0-openjdk.x86_64 wget -y mkdir -p /opt/nexus/ mkdir -p /tmp/nexus/ cd /tmp/nexus/ NEXUSURL=\"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\" wget $NEXUSURL -O nexus.tar.gz EXTOUT=`tar xzvf nexus.tar.gz` NEXUSDIR=`echo $EXTOUT | cut -d '/' -f1` rm -rf /tmp/nexus/nexus.tar.gz rsync -avzh /tmp/nexus/ /opt/nexus/ useradd nexus chown -R nexus.nexus /opt/nexus cat <<EOT>> /etc/systemd/system/nexus.service [Unit] Description=nexus service After=network.target [Service] Type=forking LimitNOFILE=65536 ExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start ExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop User=nexus Restart=on-abort [Install] WantedBy=multi-user.target EOT echo 'run_as_user=\"nexus\"' > /opt/nexus/$NEXUSDIR/bin/nexus.rc systemctl daemon-reload systemctl start nexus systemctl enable nexus Configure Nexus Login: - username: admin - Get Password: cat /opt/nexus/sonatype-work/nexus3/admin.password Check and start the nexus service sudo systemctl status nexus SonarQube Ubuntu VERSION=\"18.04\" TCP Port 9000 TCP Port 80 from MyIP and Jenkins-SG Sonar Installation Script #!/bin/bash cp /etc/sysctl.conf /root/sysctl.conf_backup cat <<EOT> /etc/sysctl.conf vm.max_map_count=262144 fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 EOT cp /etc/security/limits.conf /root/sec_limit.conf_backup cat <<EOT> /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 409 EOT sudo apt-get update -y sudo apt-get install openjdk-11-jdk -y sudo update-alternatives --config java java -version sudo apt update wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' sudo apt install postgresql postgresql-contrib -y #sudo -u postgres psql -c \"SELECT version();\" sudo systemctl enable postgresql.service sudo systemctl start postgresql.service sudo echo \"postgres:admin123\" | chpasswd runuser -l postgres -c \"createuser sonar\" sudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\" sudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\" sudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\" systemctl restart postgresql #systemctl status -l postgresql netstat -tulpena | grep postgres sudo mkdir -p /sonarqube/ cd /sonarqube/ sudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip sudo apt-get install zip -y sudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/ sudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube sudo groupadd sonar sudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar sudo chown sonar:sonar /opt/sonarqube/ -R cp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup cat <<EOT> /opt/sonarqube/conf/sonar.properties sonar.jdbc.username=sonar sonar.jdbc.password=admin123 sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.web.host=0.0.0.0 sonar.web.port=9000 sonar.web.javaAdditionalOpts=-server sonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError sonar.log.level=INFO sonar.path.logs=logs EOT cat <<EOT> /etc/systemd/system/sonarqube.service [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target EOT systemctl daemon-reload systemctl enable sonarqube.service #systemctl start sonarqube.service #systemctl status -l sonarqube.service apt-get install nginx -y rm -rf /etc/nginx/sites-enabled/default rm -rf /etc/nginx/sites-available/default cat <<EOT> /etc/nginx/sites-available/sonarqube server{ listen 80; server_name sonarqube.groophy.in; access_log /var/log/nginx/sonar.access.log; error_log /var/log/nginx/sonar.error.log; proxy_buffers 16 64k; proxy_buffer_size 128k; location / { proxy_pass http://127.0.0.1:9000; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_set_header Host \\$host; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto http; } } EOT ln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube systemctl enable nginx.service #systemctl restart nginx.service sudo ufw allow 80,9000,9001/tcp echo \"System reboot in 30 sec\" sleep 30 reboot Configure Sonar Check and start the sonarqube service sudo systemctl status sonarqube Login: - username: admin - Password: admin Slack SetUp Slack Steps: - A workspace: sosotech - Create a Channel(s): sosochannel1 - Add teammates to the channel - Add Jenkins credentials: sososlacktoken Get the Jenkins app from the : Slack App Directory Add to Slack and select the channel Add the CI Jenkins Integration: Im using #sosochannel1 Copy the Token in Step 3 and go create a Slack Credential In Jenkins credentials Scroll docn and save. No Go to jenkins and configure credentials called: sososlacktoken Jenkins Ubuntu VERSION=\"20.04.6 LTS TCP Port 8080 from Anywhere - IPv4 and IPv6 Install If you have any issues, then: curl the IP address if you had any issues. curl http://[your-put-IP]/latest/user-data LIKE SO: --> curl http://56.22.1.2/latest/user-data Also refer to site to update your code: Optional-Link Ubuntu installation script for VERSION=\"20.04.6 LTS #!/bin/bash sudo apt update sudo apt install openjdk-11-jdk -y sudo apt install maven -y curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc > /dev/null echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list > /dev/null sudo apt-get update sudo apt-get install jenkins -y Check and start the jenkins service sudo systemctl status jenkins sudo systemctl status jenkins java -version whereis git Get Jenkins Password sudo cat /var/lib/jenkins/secrets/initialAdminPassword INSTALL JDK8 On the Jenkins Server In Server terminal, Install Maven and JDK8 sudo apt update sudo apt install openjdk-8-jdk -y Get JDK8 Path from the Jenkins Server CD to ROOT and Copy the java path. Copy the path in a node Path for use in Jenkins Global Tool Configuration. /usr/lib/jvm/java-1.8.0-openjdk-amd64 . See below Photo sudo su - ls /usr/lib/jvm INSTALL MAVEN On the Jenkins Server Go to the Maven site and get latest version: Right-click and copy .tar link sudo su - cd /opt apt install wget wget https://dlcdn.apache.org/maven/maven-3/3.9.1/binaries/apache-maven-3.9.1-bin.tar.gz tar -xvzf apache-maven-3.9.1-bin.tar mv apache-maven-3.9.1 maven rm -rf apache-maven-3.9.1-bin.tar.gz Install Jenkins plugins Dashboard --> Manage Jenkins --> Plugin Manager Pipeline Maven Integration Pipeline Utility Steps Github Integration Plugin Nexus Artifact Uploader SonarQube Scanner for Jenkins Slack Notification Plugin Build Timestamp Plugin Global Tool Configuration Navigate to: Jenkins UI --> manage Jenkins --> Manage Credentials --> System --> Global credentials Configure CI [Git, Maven, JVM, SonarQube Scanner ] on Jenkins GUI . In the Jenkins UI --> manage Jenkins --> Global Tool Configuration [save] Services Configured Names JDK SosoJDK8 git Git MAVEN SOSOMAVEN3 SonarQube Scanner sososonar4.7 See the Maven, Git and JDK configuration images See the SonarQube Configuration image Configure Systems Navigate to: Jenkins UI --> manage Jenkins --> Configure System Services Configured Names xxx xxx Slack sosotech SonarQube Servers sososonar Configure SonarQube Server Configure the sonar server in Jenkins uring the SonarQube Public IP and the sonar credentials. For qualirt gate and analysis, see the sonarQube section Configure TimeStamp change the timestamp pattern yy-MM-dd_HH-mm as seen in the image: Configure Slack Notification configure the folloring : - A workspace: sosotech - A Channel(s): sosochannel1 Configure Credential Services Credential ID UserName/Password/secret-text Docker sosodockertoken secret-text AWS sosoawstoken MAVEN SonarQube sososonartoken secret-text Slack sososlacktoken secret-text In the Jenkins UI: Configure the following credentials AWS DockerHub --> (generate Token) My account --> security --> secret text k8s Config sonarqube --> (generate Token) My account --> security --> secret text Configure Dockerhub Credential(Token) Log into your dockerhub account and create a token in settings --> security: LINK Configure SonarQube Credential(Token) Login to the sonarQube UI, go to Myaccount --> security create a Token Add a Token Add the Token as Credential To jenkins Global credentials Configure SLACK Credential Configure AWS Credential Jenkins Jobs There are some Jenkins Jobs Demo'd here, like Pipeline, Freestyle: Freestyle Project Use this repo: https://github.com/sosotechnologies/cicd-maven.git It's a public repo, so credentials are optional See the image to guide you during setup. Pipeline There are 2 Options to use here: - Pipeline script - Pipeline script from SCM Some Sample Pipeline Scripts: Jenkins, Maven simple pipeline pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages { stage('Fetch code') { steps { git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build'){ steps{ sh 'mvn install -DskipTests' } post { success { echo 'Now Archiving it...' archiveArtifacts artifacts: '**/target/*.war' } } } stage('UNIT TEST') { steps{ sh 'mvn test' } } } } Jenkins, Maven, Checkstyle, Sonar-Analysis and Quality Gate - pipeline pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages{ stage('Fetch code') { steps{ git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build') { steps { sh 'mvn clean install -DskipTests' } post { success { echo \"Now Archiving.\" archiveArtifacts artifacts: '**/*.war' } } } stage('Test'){ steps { sh 'mvn test' } } stage('Checkstyle Analysis'){ steps { sh 'mvn checkstyle:checkstyle' } } stage('Sonar Analysis') { environment { scannerHome = tool 'sososonar4.7' } steps { withSonarQubeEnv('sososonar') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=sosotech \\ -Dsonar.projectName=sosotech \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } } } stage(\"Quality Gate\") { steps { timeout(time: 1, unit: 'HOURS') { waitForQualityGate abortPipeline: true } } } } post { always { echo 'Slack Notifications.' slackSend channel: '#sosochannel1', color: COLOR_MAP[currentBuild.currentResult], message: \"*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \\n More info at: ${env.BUILD_URL}\" } } } The nexus server IP in the pipeline is a Private IP of the ec2 nexus server Configure a time stamp Configure a repo in the nexus server called: FULL Pipeline def COLOR_MAP = [ 'SUCCESS': 'good', 'FAILURE': 'danger', ] pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages{ stage('Fetch code') { steps{ git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build') { steps { sh 'mvn clean install -DskipTests' } post { success { echo \"Now Archiving.\" archiveArtifacts artifacts: '**/*.war' } } } stage('Test'){ steps { sh 'mvn test' } } stage('Checkstyle Analysis'){ steps { sh 'mvn checkstyle:checkstyle' } } stage('Sonar Analysis') { environment { scannerHome = tool 'sososonar4.7' } steps { withSonarQubeEnv('sososonar') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \\ -Dsonar.projectName=vprofile \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } } } stage(\"Quality Gate\") { steps { timeout(time: 1, unit: 'HOURS') { waitForQualityGate abortPipeline: true } } } stage(\"UploadArtifact\"){ steps{ nexusArtifactUploader( nexusVersion: 'nexus3', protocol: 'http', nexusUrl: '172.31.18.28:8081', groupId: 'QA', version: \"${env.BUILD_ID}-${env.BUILD_TIMESTAMP}\", repository: 'vprofile-repo', credentialsId: 'nexuslogin', artifacts: [ [artifactId: 'vproapp', classifier: '', file: 'target/vprofile-v2.war', type: 'war'] ] ) } } } } In Global Tools Configuration, I named Maven and Jenkins like so Maven His: MAVEN3 mine: SOSOMAVEN3 JDK His: OracleJDK8 Mine: SosoJDK8 Docker","title":"CICD"},{"location":"weekly/CICD/cicd/#install-servers","text":"Install Individuals servers for: - Nexus - Sonarqube - Jenkins","title":"Install servers"},{"location":"weekly/CICD/cicd/#nexus","text":"Centos 7 (Amazon Market place) TCP Port 8081 from MyIP and Jenkins-SG #!/bin/bash yum install java-1.8.0-openjdk.x86_64 wget -y mkdir -p /opt/nexus/ mkdir -p /tmp/nexus/ cd /tmp/nexus/ NEXUSURL=\"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\" wget $NEXUSURL -O nexus.tar.gz EXTOUT=`tar xzvf nexus.tar.gz` NEXUSDIR=`echo $EXTOUT | cut -d '/' -f1` rm -rf /tmp/nexus/nexus.tar.gz rsync -avzh /tmp/nexus/ /opt/nexus/ useradd nexus chown -R nexus.nexus /opt/nexus cat <<EOT>> /etc/systemd/system/nexus.service [Unit] Description=nexus service After=network.target [Service] Type=forking LimitNOFILE=65536 ExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start ExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop User=nexus Restart=on-abort [Install] WantedBy=multi-user.target EOT echo 'run_as_user=\"nexus\"' > /opt/nexus/$NEXUSDIR/bin/nexus.rc systemctl daemon-reload systemctl start nexus systemctl enable nexus","title":"Nexus"},{"location":"weekly/CICD/cicd/#configure-nexus","text":"Login: - username: admin - Get Password: cat /opt/nexus/sonatype-work/nexus3/admin.password Check and start the nexus service sudo systemctl status nexus","title":"Configure Nexus"},{"location":"weekly/CICD/cicd/#sonarqube","text":"Ubuntu VERSION=\"18.04\" TCP Port 9000 TCP Port 80 from MyIP and Jenkins-SG","title":"SonarQube"},{"location":"weekly/CICD/cicd/#sonar-installation","text":"Script #!/bin/bash cp /etc/sysctl.conf /root/sysctl.conf_backup cat <<EOT> /etc/sysctl.conf vm.max_map_count=262144 fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 EOT cp /etc/security/limits.conf /root/sec_limit.conf_backup cat <<EOT> /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 409 EOT sudo apt-get update -y sudo apt-get install openjdk-11-jdk -y sudo update-alternatives --config java java -version sudo apt update wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' sudo apt install postgresql postgresql-contrib -y #sudo -u postgres psql -c \"SELECT version();\" sudo systemctl enable postgresql.service sudo systemctl start postgresql.service sudo echo \"postgres:admin123\" | chpasswd runuser -l postgres -c \"createuser sonar\" sudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\" sudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\" sudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\" systemctl restart postgresql #systemctl status -l postgresql netstat -tulpena | grep postgres sudo mkdir -p /sonarqube/ cd /sonarqube/ sudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip sudo apt-get install zip -y sudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/ sudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube sudo groupadd sonar sudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar sudo chown sonar:sonar /opt/sonarqube/ -R cp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup cat <<EOT> /opt/sonarqube/conf/sonar.properties sonar.jdbc.username=sonar sonar.jdbc.password=admin123 sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.web.host=0.0.0.0 sonar.web.port=9000 sonar.web.javaAdditionalOpts=-server sonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError sonar.log.level=INFO sonar.path.logs=logs EOT cat <<EOT> /etc/systemd/system/sonarqube.service [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target EOT systemctl daemon-reload systemctl enable sonarqube.service #systemctl start sonarqube.service #systemctl status -l sonarqube.service apt-get install nginx -y rm -rf /etc/nginx/sites-enabled/default rm -rf /etc/nginx/sites-available/default cat <<EOT> /etc/nginx/sites-available/sonarqube server{ listen 80; server_name sonarqube.groophy.in; access_log /var/log/nginx/sonar.access.log; error_log /var/log/nginx/sonar.error.log; proxy_buffers 16 64k; proxy_buffer_size 128k; location / { proxy_pass http://127.0.0.1:9000; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_set_header Host \\$host; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto http; } } EOT ln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube systemctl enable nginx.service #systemctl restart nginx.service sudo ufw allow 80,9000,9001/tcp echo \"System reboot in 30 sec\" sleep 30 reboot","title":"Sonar Installation"},{"location":"weekly/CICD/cicd/#configure-sonar","text":"Check and start the sonarqube service sudo systemctl status sonarqube Login: - username: admin - Password: admin","title":"Configure Sonar"},{"location":"weekly/CICD/cicd/#slack","text":"SetUp Slack Steps: - A workspace: sosotech - Create a Channel(s): sosochannel1 - Add teammates to the channel - Add Jenkins credentials: sososlacktoken Get the Jenkins app from the : Slack App Directory Add to Slack and select the channel Add the CI Jenkins Integration: Im using #sosochannel1 Copy the Token in Step 3 and go create a Slack Credential In Jenkins credentials Scroll docn and save. No Go to jenkins and configure credentials called: sososlacktoken","title":"Slack"},{"location":"weekly/CICD/cicd/#jenkins","text":"Ubuntu VERSION=\"20.04.6 LTS TCP Port 8080 from Anywhere - IPv4 and IPv6","title":"Jenkins"},{"location":"weekly/CICD/cicd/#install","text":"If you have any issues, then: curl the IP address if you had any issues. curl http://[your-put-IP]/latest/user-data LIKE SO: --> curl http://56.22.1.2/latest/user-data Also refer to site to update your code: Optional-Link Ubuntu installation script for VERSION=\"20.04.6 LTS #!/bin/bash sudo apt update sudo apt install openjdk-11-jdk -y sudo apt install maven -y curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc > /dev/null echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list > /dev/null sudo apt-get update sudo apt-get install jenkins -y Check and start the jenkins service sudo systemctl status jenkins sudo systemctl status jenkins java -version whereis git Get Jenkins Password sudo cat /var/lib/jenkins/secrets/initialAdminPassword INSTALL JDK8 On the Jenkins Server In Server terminal, Install Maven and JDK8 sudo apt update sudo apt install openjdk-8-jdk -y Get JDK8 Path from the Jenkins Server CD to ROOT and Copy the java path. Copy the path in a node Path for use in Jenkins Global Tool Configuration. /usr/lib/jvm/java-1.8.0-openjdk-amd64 . See below Photo sudo su - ls /usr/lib/jvm INSTALL MAVEN On the Jenkins Server Go to the Maven site and get latest version: Right-click and copy .tar link sudo su - cd /opt apt install wget wget https://dlcdn.apache.org/maven/maven-3/3.9.1/binaries/apache-maven-3.9.1-bin.tar.gz tar -xvzf apache-maven-3.9.1-bin.tar mv apache-maven-3.9.1 maven rm -rf apache-maven-3.9.1-bin.tar.gz","title":"Install"},{"location":"weekly/CICD/cicd/#install-jenkins-plugins","text":"Dashboard --> Manage Jenkins --> Plugin Manager Pipeline Maven Integration Pipeline Utility Steps Github Integration Plugin Nexus Artifact Uploader SonarQube Scanner for Jenkins Slack Notification Plugin Build Timestamp Plugin","title":"Install Jenkins plugins"},{"location":"weekly/CICD/cicd/#global-tool-configuration","text":"Navigate to: Jenkins UI --> manage Jenkins --> Manage Credentials --> System --> Global credentials Configure CI [Git, Maven, JVM, SonarQube Scanner ] on Jenkins GUI . In the Jenkins UI --> manage Jenkins --> Global Tool Configuration [save] Services Configured Names JDK SosoJDK8 git Git MAVEN SOSOMAVEN3 SonarQube Scanner sososonar4.7 See the Maven, Git and JDK configuration images See the SonarQube Configuration image","title":"Global Tool Configuration"},{"location":"weekly/CICD/cicd/#configure-systems","text":"Navigate to: Jenkins UI --> manage Jenkins --> Configure System Services Configured Names xxx xxx Slack sosotech SonarQube Servers sososonar","title":"Configure Systems"},{"location":"weekly/CICD/cicd/#configure-sonarqube-server","text":"Configure the sonar server in Jenkins uring the SonarQube Public IP and the sonar credentials. For qualirt gate and analysis, see the sonarQube section","title":"Configure SonarQube Server"},{"location":"weekly/CICD/cicd/#configure-timestamp","text":"change the timestamp pattern yy-MM-dd_HH-mm as seen in the image:","title":"Configure TimeStamp"},{"location":"weekly/CICD/cicd/#configure-slack-notification","text":"configure the folloring : - A workspace: sosotech - A Channel(s): sosochannel1","title":"Configure Slack Notification"},{"location":"weekly/CICD/cicd/#configure-credential","text":"Services Credential ID UserName/Password/secret-text Docker sosodockertoken secret-text AWS sosoawstoken MAVEN SonarQube sososonartoken secret-text Slack sososlacktoken secret-text In the Jenkins UI: Configure the following credentials AWS DockerHub --> (generate Token) My account --> security --> secret text k8s Config sonarqube --> (generate Token) My account --> security --> secret text","title":"Configure Credential"},{"location":"weekly/CICD/cicd/#configure-dockerhub-credentialtoken","text":"Log into your dockerhub account and create a token in settings --> security: LINK","title":"Configure Dockerhub Credential(Token)"},{"location":"weekly/CICD/cicd/#configure-sonarqube-credentialtoken","text":"Login to the sonarQube UI, go to Myaccount --> security create a Token Add a Token Add the Token as Credential To jenkins Global credentials","title":"Configure SonarQube Credential(Token)"},{"location":"weekly/CICD/cicd/#configure-slack-credential","text":"","title":"Configure SLACK Credential"},{"location":"weekly/CICD/cicd/#configure-aws-credential","text":"","title":"Configure AWS Credential"},{"location":"weekly/CICD/cicd/#jenkins-jobs","text":"There are some Jenkins Jobs Demo'd here, like Pipeline, Freestyle:","title":"Jenkins Jobs"},{"location":"weekly/CICD/cicd/#freestyle-project","text":"Use this repo: https://github.com/sosotechnologies/cicd-maven.git It's a public repo, so credentials are optional See the image to guide you during setup.","title":"Freestyle Project"},{"location":"weekly/CICD/cicd/#pipeline","text":"There are 2 Options to use here: - Pipeline script - Pipeline script from SCM Some Sample Pipeline Scripts: Jenkins, Maven simple pipeline pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages { stage('Fetch code') { steps { git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build'){ steps{ sh 'mvn install -DskipTests' } post { success { echo 'Now Archiving it...' archiveArtifacts artifacts: '**/target/*.war' } } } stage('UNIT TEST') { steps{ sh 'mvn test' } } } } Jenkins, Maven, Checkstyle, Sonar-Analysis and Quality Gate - pipeline pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages{ stage('Fetch code') { steps{ git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build') { steps { sh 'mvn clean install -DskipTests' } post { success { echo \"Now Archiving.\" archiveArtifacts artifacts: '**/*.war' } } } stage('Test'){ steps { sh 'mvn test' } } stage('Checkstyle Analysis'){ steps { sh 'mvn checkstyle:checkstyle' } } stage('Sonar Analysis') { environment { scannerHome = tool 'sososonar4.7' } steps { withSonarQubeEnv('sososonar') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=sosotech \\ -Dsonar.projectName=sosotech \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } } } stage(\"Quality Gate\") { steps { timeout(time: 1, unit: 'HOURS') { waitForQualityGate abortPipeline: true } } } } post { always { echo 'Slack Notifications.' slackSend channel: '#sosochannel1', color: COLOR_MAP[currentBuild.currentResult], message: \"*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \\n More info at: ${env.BUILD_URL}\" } } } The nexus server IP in the pipeline is a Private IP of the ec2 nexus server Configure a time stamp Configure a repo in the nexus server called: FULL Pipeline def COLOR_MAP = [ 'SUCCESS': 'good', 'FAILURE': 'danger', ] pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages{ stage('Fetch code') { steps{ git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build') { steps { sh 'mvn clean install -DskipTests' } post { success { echo \"Now Archiving.\" archiveArtifacts artifacts: '**/*.war' } } } stage('Test'){ steps { sh 'mvn test' } } stage('Checkstyle Analysis'){ steps { sh 'mvn checkstyle:checkstyle' } } stage('Sonar Analysis') { environment { scannerHome = tool 'sososonar4.7' } steps { withSonarQubeEnv('sososonar') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \\ -Dsonar.projectName=vprofile \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } } } stage(\"Quality Gate\") { steps { timeout(time: 1, unit: 'HOURS') { waitForQualityGate abortPipeline: true } } } stage(\"UploadArtifact\"){ steps{ nexusArtifactUploader( nexusVersion: 'nexus3', protocol: 'http', nexusUrl: '172.31.18.28:8081', groupId: 'QA', version: \"${env.BUILD_ID}-${env.BUILD_TIMESTAMP}\", repository: 'vprofile-repo', credentialsId: 'nexuslogin', artifacts: [ [artifactId: 'vproapp', classifier: '', file: 'target/vprofile-v2.war', type: 'war'] ] ) } } } } In Global Tools Configuration, I named Maven and Jenkins like so","title":"Pipeline"},{"location":"weekly/CICD/cicd/#maven","text":"His: MAVEN3 mine: SOSOMAVEN3 JDK His: OracleJDK8 Mine: SosoJDK8","title":"Maven"},{"location":"weekly/CICD/cicd/#docker","text":"","title":"Docker"},{"location":"weekly/Cloud-Technologies/intro-aws/","text":"Cloud Technologies Defining cloud computing Defining private, public, and hybrid cloud Exploring the components of cloud infrastructure Defining the different Service Models What is Cloud Computing? Cloud computing is the use of hosted services, such as servers, data storage, databases, software, and networking over the internet\u00ac\u200a\u2014\u200ainstead of directly accessing your computer hard drive. Types of cloud computing Cloud computing is offered in 3 major forms: private, public, and hybrid cloud. Private Cloud: The private cloud computing service is a cloud computing model where the infrastructure is dedicated only to select users instead of the general public, either over the Internet or a private internal network. Other names for the private cloud are internal or corporate cloud. Some private cloud providers are VMware-vRealize Suite Cloud Management Platform, AWS-Virtual Private Cloud, Microsoft-Azure Private Cloud\u2026 Public Cloud: The public cloud is defined as computing services offered by third-party providers over the public Internet, making them available to anyone who wants to use or purchase them. They may be free or sold on-demand, allowing customers to pay only per usage for the CPU cycles, storage, or bandwidth they consume. Some public cloud services are AWS, GCP, Azure, IBM Cloud, Alibaba Cloud\u2026 Hybrid Cloud: A hybrid cloud is a computing environment that combines an on-premises data center (also called a private cloud) with a public cloud, allowing data and applications to be shared between them. Some public cloud providers are VMware, Microsoft Azure\u2026 Components of cloud infrastructure Client-side Infrastructure: Here, users interact with the cloud through a front-end GUI (Graphical User Interface). Desktop, laptops, mobiles, tablets, and others are used on the client-side to access services. Application: The application may be any software or platform that a client wants to access. The operating system: The cloud operating system manages the operation, the virtual servers, execution and processes of virtual machines and virtual infrastructure, as well as the back-end hardware and software resources. Service: Cloud Services are offered in different models. These services are being provided based on the client\u2019s request. Some of the service models are seen below Service Models Cloud-Computing providers offer their \u201cservices\u201d according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). IAAS: Infrastructure As A Service (IAAS) is means of delivering computing infrastructure as on-demand services. It is one of the three fundamental cloud service model servers\u2019 storage network operating systems. In the user purchasing servers, software data center space, or network equipment and renting those resources as a fully outsourced service can demand model. It allows dynamic scaling, and the resources are distributed as a service. It generally includes multiple users on a single piece of hardware. PAAS: Platform as A Service (PAAS) is a cloud delivery model for applications composed of services managed by a third party. It provides elastic scaling of your application which allows developers to build applications and services over the internet and the deployment models include public, private and hybrid. SAAS: Software As A Service (SAAS) allows users to run existing online applications and it is a model software that is deployed as a hosting service and is accessed over Output Rephrased/Re-written Text the internet or software delivery model during which software and its associated data are hosted centrally and accessed using their client, usually an online browser over the web. SAAS services are used for the development and deployment of modern applications. The emergence of Cloud computing has given rise to more as-a-service offerings. Some of them include: - AIaaS: AI as a service - DaaS: Desktop as a service - ITaaS: IT as a service - RaaS: Ransomware as a service. Top benefits of cloud computing High Speed: the ability to spin up new instances in a matter of seconds. Efficiency and Cost Reduction Provides efficient Data Security Scalable: ease with quickly\u200a\u2014\u200ascale up/down infrastructures. Collaboration: Cloud environments enable better collaboration across teams. Provides unlimited Storage Capacity. Provides ease in Back-up and Restoring of Data Reliability: Cloud computing makes data backup, disaster recovery, and business continuity easier and less expensive because data can be mirrored at multiple redundant sites on the cloud provider\u2019s network. FOCUS: AWS CLOUD What Is Amazon Web Services(AWS): Amazon Web Services(AWS) is Amazon\u2019s cloud computing platform that offers a mix of packaged platform as a service (PaaS), software as a service (SaaS), and infrastructure as a service (IaaS). ### EC2 Amazon EC2 is AWS primary web service that provides resizable compute capacity in the cloud. Instance types: Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. Example instance Family: c4 Compute optimized\u2014For workloads requiring significant processing r3 Memory optimized\u2014For memory-intensive workloads i2 Storage optimized\u2014For workloads requiring high amounts of fast SSD storage g2 GPU-based instances\u2014Intended for graphics and general-purpose GPU compute workloads Amazon Machine Images (AMIs): Defines the initial software that will be on an instance when it is launched. An AMI defines; The Operating System (OS) and its configuration, The initial state of any patches, Application or system software. AMI\u2019s can be AWS published, from the AWS Marketplace, Generated from Existing Instances, or Uploaded Virtual Servers Security Groups: This is a VPC concept. If an instance is running in an Amazon VPC, you can change which security groups are associated with an instance while the instance is running. EBS: Is type of \u201cblock storage\u201d volume offering by AWS. EBS provides persistent block-level storage volumes for use with Amazon EC2 instances. Instance Stores: Is another type of \u201cblock storage\u201d volume offering by AWS for your instance. This storage is located on disks that are physically attached to the host computer. ### S3 Amazon Simple Storage Service is a secure, durable, and highly-scalable cloud storage. Some use cases for cloud storage include: - Backup and archive for on-premises or cloud data. - Content, media, and software storage and distribution. - Big data analytics. - Static website hosting. - Cloud-native mobile and Internet application hosting. ### VPC The Amazon Virtual Private Cloud (Amazon VPC) is a custom-defined virtual network within the AWS Cloud. ![cloud1](photos/cloud5.png) VPC COMPONENTS: An Amazon VPC consists of the following main components: Subnets, Route tables, Dynamic Host Configuration Protocol (DHCP) option sets Security groups, Network Access Control Lists (ACLs). Also, optional components: Internet Gateways (IGWs), Elastic IP (EIP) addresses, Elastic Network Interfaces (ENIs), Endpoints, Peering, Network Address Translation (NATs) instances and NAT gateways, Virtual Private Gateway (VPG), Customer Gateways (CGWs), and Virtual Private Networks (VPNs) ***Subnets:*** A subnet is a segment of an Amazon VPC\u2019s IP address range. Subnets reside within one Availability Zone and cannot span zones; You can, however, have multiple subnets in one Availability Zone. Each subnet must be associated with a route table, which controls the routing for the subnet. ***Route Tables:*** A route table is a logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where network traffic is directed. ***Internet Gateways:*** IGW allows communication between instances in your Amazon VPC and the Internet. ***Dynamic Host Configuration*** Protocol (DHCP) Option Sets: provides a standard for passing configuration information to hosts on a TCP/IP network. AWS automatically creates and associates a DHCP option set for your Amazon VPC upon creation and sets two options: domain-name-servers (defaulted to AmazonProvidedDNS) and domain-name (defaulted to the domain name for your region). ### IAM With AWS IAM Identity Center (successor to AWS Single Sign-On), you can manage sign-in security for your workforce identities, also known as workforce users. IAM Identity Center provides one place where you can create or connect workforce users and centrally manage their access across all their AWS accounts and applications. For more on IAM chack the link: IAM ### AWS ELASTIC LOAD BALANCER (ELB) AWS has an Elastic Load Balancer that is used to distribute incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing supports routing and load balancing of Hypertext. Transfer Protocol (HTTP), Hypertext Transfer Protocol Secure (HTTPS), Transmission Control Protocol (TCP), and Secure Sockets Layer (SSL) traffic to Amazon EC2 instances. Types of AWS ELB: The two major Elastic load balancers are: ***Application Load Balancer*** Application layer that describes the use of HTTP and HTTPS connections from clients to the load balancer and from the load balancer to your back-end instance. ***Network Load Balancer*** Are used to route traffic through layer 4, based IPs and TCP or UDP ports. ### EKS Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. For more on EKS chack the link: [EKS]https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html","title":"Cloud Technologies"},{"location":"weekly/Cloud-Technologies/intro-aws/#cloud-technologies","text":"Defining cloud computing Defining private, public, and hybrid cloud Exploring the components of cloud infrastructure Defining the different Service Models","title":"Cloud Technologies"},{"location":"weekly/Cloud-Technologies/intro-aws/#what-is-cloud-computing","text":"Cloud computing is the use of hosted services, such as servers, data storage, databases, software, and networking over the internet\u00ac\u200a\u2014\u200ainstead of directly accessing your computer hard drive.","title":"What is Cloud Computing?"},{"location":"weekly/Cloud-Technologies/intro-aws/#types-of-cloud-computing","text":"Cloud computing is offered in 3 major forms: private, public, and hybrid cloud. Private Cloud: The private cloud computing service is a cloud computing model where the infrastructure is dedicated only to select users instead of the general public, either over the Internet or a private internal network. Other names for the private cloud are internal or corporate cloud. Some private cloud providers are VMware-vRealize Suite Cloud Management Platform, AWS-Virtual Private Cloud, Microsoft-Azure Private Cloud\u2026 Public Cloud: The public cloud is defined as computing services offered by third-party providers over the public Internet, making them available to anyone who wants to use or purchase them. They may be free or sold on-demand, allowing customers to pay only per usage for the CPU cycles, storage, or bandwidth they consume. Some public cloud services are AWS, GCP, Azure, IBM Cloud, Alibaba Cloud\u2026 Hybrid Cloud: A hybrid cloud is a computing environment that combines an on-premises data center (also called a private cloud) with a public cloud, allowing data and applications to be shared between them. Some public cloud providers are VMware, Microsoft Azure\u2026","title":"Types of cloud computing"},{"location":"weekly/Cloud-Technologies/intro-aws/#components-of-cloud-infrastructure","text":"Client-side Infrastructure: Here, users interact with the cloud through a front-end GUI (Graphical User Interface). Desktop, laptops, mobiles, tablets, and others are used on the client-side to access services. Application: The application may be any software or platform that a client wants to access. The operating system: The cloud operating system manages the operation, the virtual servers, execution and processes of virtual machines and virtual infrastructure, as well as the back-end hardware and software resources. Service: Cloud Services are offered in different models. These services are being provided based on the client\u2019s request. Some of the service models are seen below","title":"Components of cloud infrastructure"},{"location":"weekly/Cloud-Technologies/intro-aws/#service-models","text":"Cloud-Computing providers offer their \u201cservices\u201d according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). IAAS: Infrastructure As A Service (IAAS) is means of delivering computing infrastructure as on-demand services. It is one of the three fundamental cloud service model servers\u2019 storage network operating systems. In the user purchasing servers, software data center space, or network equipment and renting those resources as a fully outsourced service can demand model. It allows dynamic scaling, and the resources are distributed as a service. It generally includes multiple users on a single piece of hardware. PAAS: Platform as A Service (PAAS) is a cloud delivery model for applications composed of services managed by a third party. It provides elastic scaling of your application which allows developers to build applications and services over the internet and the deployment models include public, private and hybrid. SAAS: Software As A Service (SAAS) allows users to run existing online applications and it is a model software that is deployed as a hosting service and is accessed over Output Rephrased/Re-written Text the internet or software delivery model during which software and its associated data are hosted centrally and accessed using their client, usually an online browser over the web. SAAS services are used for the development and deployment of modern applications. The emergence of Cloud computing has given rise to more as-a-service offerings. Some of them include: - AIaaS: AI as a service - DaaS: Desktop as a service - ITaaS: IT as a service - RaaS: Ransomware as a service.","title":"Service Models"},{"location":"weekly/Cloud-Technologies/intro-aws/#top-benefits-of-cloud-computing","text":"High Speed: the ability to spin up new instances in a matter of seconds. Efficiency and Cost Reduction Provides efficient Data Security Scalable: ease with quickly\u200a\u2014\u200ascale up/down infrastructures. Collaboration: Cloud environments enable better collaboration across teams. Provides unlimited Storage Capacity. Provides ease in Back-up and Restoring of Data Reliability: Cloud computing makes data backup, disaster recovery, and business continuity easier and less expensive because data can be mirrored at multiple redundant sites on the cloud provider\u2019s network.","title":"Top benefits of cloud computing"},{"location":"weekly/Cloud-Technologies/intro-aws/#focus-aws-cloud","text":"What Is Amazon Web Services(AWS): Amazon Web Services(AWS) is Amazon\u2019s cloud computing platform that offers a mix of packaged platform as a service (PaaS), software as a service (SaaS), and infrastructure as a service (IaaS). ### EC2 Amazon EC2 is AWS primary web service that provides resizable compute capacity in the cloud. Instance types: Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. Example instance Family: c4 Compute optimized\u2014For workloads requiring significant processing r3 Memory optimized\u2014For memory-intensive workloads i2 Storage optimized\u2014For workloads requiring high amounts of fast SSD storage g2 GPU-based instances\u2014Intended for graphics and general-purpose GPU compute workloads Amazon Machine Images (AMIs): Defines the initial software that will be on an instance when it is launched. An AMI defines; The Operating System (OS) and its configuration, The initial state of any patches, Application or system software. AMI\u2019s can be AWS published, from the AWS Marketplace, Generated from Existing Instances, or Uploaded Virtual Servers Security Groups: This is a VPC concept. If an instance is running in an Amazon VPC, you can change which security groups are associated with an instance while the instance is running. EBS: Is type of \u201cblock storage\u201d volume offering by AWS. EBS provides persistent block-level storage volumes for use with Amazon EC2 instances. Instance Stores: Is another type of \u201cblock storage\u201d volume offering by AWS for your instance. This storage is located on disks that are physically attached to the host computer. ### S3 Amazon Simple Storage Service is a secure, durable, and highly-scalable cloud storage. Some use cases for cloud storage include: - Backup and archive for on-premises or cloud data. - Content, media, and software storage and distribution. - Big data analytics. - Static website hosting. - Cloud-native mobile and Internet application hosting. ### VPC The Amazon Virtual Private Cloud (Amazon VPC) is a custom-defined virtual network within the AWS Cloud. ![cloud1](photos/cloud5.png) VPC COMPONENTS: An Amazon VPC consists of the following main components: Subnets, Route tables, Dynamic Host Configuration Protocol (DHCP) option sets Security groups, Network Access Control Lists (ACLs). Also, optional components: Internet Gateways (IGWs), Elastic IP (EIP) addresses, Elastic Network Interfaces (ENIs), Endpoints, Peering, Network Address Translation (NATs) instances and NAT gateways, Virtual Private Gateway (VPG), Customer Gateways (CGWs), and Virtual Private Networks (VPNs) ***Subnets:*** A subnet is a segment of an Amazon VPC\u2019s IP address range. Subnets reside within one Availability Zone and cannot span zones; You can, however, have multiple subnets in one Availability Zone. Each subnet must be associated with a route table, which controls the routing for the subnet. ***Route Tables:*** A route table is a logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where network traffic is directed. ***Internet Gateways:*** IGW allows communication between instances in your Amazon VPC and the Internet. ***Dynamic Host Configuration*** Protocol (DHCP) Option Sets: provides a standard for passing configuration information to hosts on a TCP/IP network. AWS automatically creates and associates a DHCP option set for your Amazon VPC upon creation and sets two options: domain-name-servers (defaulted to AmazonProvidedDNS) and domain-name (defaulted to the domain name for your region). ### IAM With AWS IAM Identity Center (successor to AWS Single Sign-On), you can manage sign-in security for your workforce identities, also known as workforce users. IAM Identity Center provides one place where you can create or connect workforce users and centrally manage their access across all their AWS accounts and applications. For more on IAM chack the link: IAM ### AWS ELASTIC LOAD BALANCER (ELB) AWS has an Elastic Load Balancer that is used to distribute incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing supports routing and load balancing of Hypertext. Transfer Protocol (HTTP), Hypertext Transfer Protocol Secure (HTTPS), Transmission Control Protocol (TCP), and Secure Sockets Layer (SSL) traffic to Amazon EC2 instances. Types of AWS ELB: The two major Elastic load balancers are: ***Application Load Balancer*** Application layer that describes the use of HTTP and HTTPS connections from clients to the load balancer and from the load balancer to your back-end instance. ***Network Load Balancer*** Are used to route traffic through layer 4, based IPs and TCP or UDP ports. ### EKS Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. For more on EKS chack the link: [EKS]https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html","title":"FOCUS: AWS CLOUD"},{"location":"weekly/Docker/docker/","text":"Install docker Sample use cases 5 Sample Build Deploy 1. Docker and Mkdocs Build and deploy push Mkdocs to ECR/DockerHub Clone this repo: docs_docker_io Install Docker sudo yum install docker -y sudo systemctl status docker sudo systemctl start docker sudo systemctl enable docker Build and Push Docker sudo docker build -t sosodocs . Tag the sosodoc image sudo docker tag sosodocs 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Push image to repo sudo docker push 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Run the image with any one of the 2 commands, set yout ip:80 docker run -itd -p 80:80 --rm sosodocs docker run -t -i -p 80:80 sosodocs And that is all, you should be able to navigate to http://127.0.0.1:80 and see the documentation website running. Optional BONUS!!!: DEPLOY TO AN EXISTING EKS CLUSTER k expose deploy mkdocs --name=mkdocs-svc --port=80 --type=LoadBalancer --targetPort=80 --dry-run=client -o yaml > deploy.yaml k expose deploy mkdocs --name=mkdocs-svc --port=80 --type=LoadBalancer --target-port=80 --dry-run=client -o yaml > service.yaml Optional Tag Your docker image with a version sudo docker tag sosodocs sosodocs:v1 Delete the existing running container, 26b43376e040 is mine, get yours. sudo docker rm 26b43376e040 OR sudo docker rm 26b43376e040 --force Re-run the new versioned-image sudo docker run -itd -p 80:80 --rm sosodocs:v1 To remove container or image: docker rm [container] docker rmi [image]","title":"Docker"},{"location":"weekly/Docker/docker/#install-docker","text":"","title":"Install docker"},{"location":"weekly/Docker/docker/#sample-use-cases","text":"5 Sample Build Deploy","title":"Sample use cases"},{"location":"weekly/Docker/docker/#1-docker-and-mkdocs","text":"Build and deploy push Mkdocs to ECR/DockerHub Clone this repo: docs_docker_io Install Docker sudo yum install docker -y sudo systemctl status docker sudo systemctl start docker sudo systemctl enable docker","title":"1. Docker and Mkdocs"},{"location":"weekly/Docker/docker/#build-and-push-docker","text":"sudo docker build -t sosodocs . Tag the sosodoc image sudo docker tag sosodocs 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Push image to repo sudo docker push 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Run the image with any one of the 2 commands, set yout ip:80 docker run -itd -p 80:80 --rm sosodocs docker run -t -i -p 80:80 sosodocs And that is all, you should be able to navigate to http://127.0.0.1:80 and see the documentation website running. Optional BONUS!!!: DEPLOY TO AN EXISTING EKS CLUSTER k expose deploy mkdocs --name=mkdocs-svc --port=80 --type=LoadBalancer --targetPort=80 --dry-run=client -o yaml > deploy.yaml k expose deploy mkdocs --name=mkdocs-svc --port=80 --type=LoadBalancer --target-port=80 --dry-run=client -o yaml > service.yaml Optional Tag Your docker image with a version sudo docker tag sosodocs sosodocs:v1 Delete the existing running container, 26b43376e040 is mine, get yours. sudo docker rm 26b43376e040 OR sudo docker rm 26b43376e040 --force Re-run the new versioned-image sudo docker run -itd -p 80:80 --rm sosodocs:v1 To remove container or image: docker rm [container] docker rmi [image]","title":"Build and Push Docker"},{"location":"weekly/ELK/elk/","text":"ELK - Elastic Stack ELK Stack is the world\u2019s most popular log management platform. The ELK stack is an acronym used to describe a stack that comprises of three popular projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a distributed search and analytics engine built on Apache Lucene Logstash is used for both shipping as well as processing and storing logs Kibana is a visualization tool which is hosted through Nginx or Apache Some popular Use Cases for ELK are: Logging, Matrics, Security Analysis, Business Analysis - Logging - logging and analysing Data, like gaming data - Matrics - like NASA Rover can sent telematrics, sensor data - Security Analysis - Helps with securing comunication in Apps like Slack - Business Analysis - Helps with matching people ElasticSearch With ElasticSearch, User can sends search quary to your website and your server send those requests to ElasticSearch, Elasticsearch then processes this request and Responds back to the user, through the Server. Elasticsearch Architecture: Key Components Elasticsearch Cluster is composed of a group of nodes that store data. Elasticsearch Node is an instance that runs in a cluster. Multiple nodes can be run on a single machine. When Elastic is running, you will have an instance of elastic known as Node. Each Node Has a unique ID and a node. Each Node belongs to a single cluster. Data is stored as documents in Elasticsearch. Indices are used to group documents that are related to each other. When an Index is created, it comes with a Shard, by default. There are the three main options to configure an Elasticsearch node: Elasticsearch master node Elasticsearch data node Elasticsearch client node Search movement - client --> servers --> ElasticSearch Shards is where data is stored. What you create an Index, you can create multiple shards that are distributed across nodes. Each shard has its replica to recover data in case of node failure. The primary shard and replica shard is always placed on different nodes. Horizontal scaling or scale-out is the main reason to shard a database. The number of shards depends heavily on the amount of data we have. Shard Example use Case 1 A BCBSNC Elastic engineer has 900k documents that they want to store. Each Shard can only Hold a maximum of 300k Documents Two more shards of 300GB each will be added bringing the total storage capacity to 900K Documents Shard Example use Case 2 Scenario 1: An Elastic engineer searches 500k documents that is stored in a single Shard The response time for searching 500k Documents in a single shard is 10 seconds. Scenario 2: An Elastic engineer runs a parallel searches of 500k documents that are distributed in 5 Shard. The response time for searching 100k Documents/Shard will be 2 seconds. So, the response time for searching all 500k Documents that are distributed amongst the 5 shard will still be 2 seconds. Kibana Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. Kibana Aggregations There are two main types of aggregations Metric aggregations are used to calculate a value for each bucket based on the documents inside the bucket. Bucket aggregations are used groups documents together in one bucket according to your logic and requirements, while the Metric aggregations For More information on Kibana,: Click link Adding data into Elasticsearch The main consideration for indexing or adding data into Elasticsearch largely depends on whether you are indexing general content or timestamped data. General content: Index content like HTML pages, catalogs and other files. Send data directly to Elasticseach from your application using an Elastic language client. Otherwise use Workplace Search content connectors or the Enterprise Search web crawler. Timestamped data: The preferred way to index timestamped data is to use Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. How to ingest data into Elasticsearch Service There are 4 main ways in Adding Data to ElasticSearch Service: By using Beats and Logstash By using Kibana By programmatically indexing data Manually adding data Read more about data ingestion in ElasticSearch Click Link The index The index is the basis of Elasticsearch, it is where you store your documents. You can compare it to a table in a database. An index has settings and a mapping which defines how to store and index the documents. Mapping Mapping is the process of defining how a document, and the fields it contains, are stored and indexed. Each document is a collection of fields, which each have their own data type. When mapping your data, you create a mapping definition, which contains a list of fields that are pertinent to the document. What is Elastic integrations Elastic integrations are a streamlined way to connect your data to the Elastic Stack. Integrations are available for popular services and platforms, like Nginx, AWS, and MongoDB, as well as many generic input types like log files. For more on Data Integration Click link: Elastic Agent Elastic integrations are powered by Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. It can also protect hosts from security threats, query data from operating systems, forward data from remote services or hardware, and more. Sample Hands-on I will : Use curl commands to talk to the Elasticsearch REST API. Or if you choose to use Kibana Devtools, thats also ok, because Kibana\u2019s Devtools can automatically recognizes and converts curl commands . create an index Called middleware curl -X PUT \u201clocalhost:9200/middleware Delete the index Called middleware curl -X DELETE \u201clocalhost:9200/middleware\u201d Create an index Called middleware with just one shard and no replicas PUT /middleware?pretty { \"settings\" : { \"number_of_shards\" : 1, \"number_of_replicas\" : 0 } } loading data into that index Called middleware PUT /middleware/_mapping { \"properties\": { \"post_time\": { \"type\": \"date\" }, \"username\": { \"type\": \"keyword\" }, \"message\": { \"type\": \"text\" } } } Now Load some data Manually POST /middleware/_doc/?pretty { \"user\" : \"collins\", \"post_date\" : \"2023-16-03T14:10:10\", \"message\" : \"Welcome to Elasticsearch?\" } see this mediam link, its cool Click-Link","title":"ELK"},{"location":"weekly/ELK/elk/#elk-elastic-stack","text":"ELK Stack is the world\u2019s most popular log management platform. The ELK stack is an acronym used to describe a stack that comprises of three popular projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a distributed search and analytics engine built on Apache Lucene Logstash is used for both shipping as well as processing and storing logs Kibana is a visualization tool which is hosted through Nginx or Apache Some popular Use Cases for ELK are: Logging, Matrics, Security Analysis, Business Analysis - Logging - logging and analysing Data, like gaming data - Matrics - like NASA Rover can sent telematrics, sensor data - Security Analysis - Helps with securing comunication in Apps like Slack - Business Analysis - Helps with matching people","title":"ELK - Elastic Stack"},{"location":"weekly/ELK/elk/#elasticsearch","text":"With ElasticSearch, User can sends search quary to your website and your server send those requests to ElasticSearch, Elasticsearch then processes this request and Responds back to the user, through the Server.","title":"ElasticSearch"},{"location":"weekly/ELK/elk/#elasticsearch-architecture-key-components","text":"Elasticsearch Cluster is composed of a group of nodes that store data. Elasticsearch Node is an instance that runs in a cluster. Multiple nodes can be run on a single machine. When Elastic is running, you will have an instance of elastic known as Node. Each Node Has a unique ID and a node. Each Node belongs to a single cluster. Data is stored as documents in Elasticsearch. Indices are used to group documents that are related to each other. When an Index is created, it comes with a Shard, by default. There are the three main options to configure an Elasticsearch node: Elasticsearch master node Elasticsearch data node Elasticsearch client node Search movement - client --> servers --> ElasticSearch Shards is where data is stored. What you create an Index, you can create multiple shards that are distributed across nodes. Each shard has its replica to recover data in case of node failure. The primary shard and replica shard is always placed on different nodes. Horizontal scaling or scale-out is the main reason to shard a database. The number of shards depends heavily on the amount of data we have. Shard Example use Case 1 A BCBSNC Elastic engineer has 900k documents that they want to store. Each Shard can only Hold a maximum of 300k Documents Two more shards of 300GB each will be added bringing the total storage capacity to 900K Documents Shard Example use Case 2 Scenario 1: An Elastic engineer searches 500k documents that is stored in a single Shard The response time for searching 500k Documents in a single shard is 10 seconds. Scenario 2: An Elastic engineer runs a parallel searches of 500k documents that are distributed in 5 Shard. The response time for searching 100k Documents/Shard will be 2 seconds. So, the response time for searching all 500k Documents that are distributed amongst the 5 shard will still be 2 seconds.","title":"Elasticsearch Architecture: Key Components"},{"location":"weekly/ELK/elk/#kibana","text":"Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. Kibana Aggregations There are two main types of aggregations Metric aggregations are used to calculate a value for each bucket based on the documents inside the bucket. Bucket aggregations are used groups documents together in one bucket according to your logic and requirements, while the Metric aggregations For More information on Kibana,: Click link","title":"Kibana"},{"location":"weekly/ELK/elk/#adding-data-into-elasticsearch","text":"The main consideration for indexing or adding data into Elasticsearch largely depends on whether you are indexing general content or timestamped data. General content: Index content like HTML pages, catalogs and other files. Send data directly to Elasticseach from your application using an Elastic language client. Otherwise use Workplace Search content connectors or the Enterprise Search web crawler. Timestamped data: The preferred way to index timestamped data is to use Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host.","title":"Adding data into Elasticsearch"},{"location":"weekly/ELK/elk/#how-to-ingest-data-into-elasticsearch-service","text":"There are 4 main ways in Adding Data to ElasticSearch Service: By using Beats and Logstash By using Kibana By programmatically indexing data Manually adding data Read more about data ingestion in ElasticSearch Click Link","title":"How to ingest data into Elasticsearch Service"},{"location":"weekly/ELK/elk/#the-index","text":"The index is the basis of Elasticsearch, it is where you store your documents. You can compare it to a table in a database. An index has settings and a mapping which defines how to store and index the documents.","title":"The index"},{"location":"weekly/ELK/elk/#mapping","text":"Mapping is the process of defining how a document, and the fields it contains, are stored and indexed. Each document is a collection of fields, which each have their own data type. When mapping your data, you create a mapping definition, which contains a list of fields that are pertinent to the document.","title":"Mapping"},{"location":"weekly/ELK/elk/#what-is-elastic-integrations","text":"Elastic integrations are a streamlined way to connect your data to the Elastic Stack. Integrations are available for popular services and platforms, like Nginx, AWS, and MongoDB, as well as many generic input types like log files. For more on Data Integration Click link:","title":"What is Elastic integrations"},{"location":"weekly/ELK/elk/#elastic-agent","text":"Elastic integrations are powered by Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. It can also protect hosts from security threats, query data from operating systems, forward data from remote services or hardware, and more.","title":"Elastic Agent"},{"location":"weekly/ELK/elk/#sample-hands-on","text":"I will : Use curl commands to talk to the Elasticsearch REST API. Or if you choose to use Kibana Devtools, thats also ok, because Kibana\u2019s Devtools can automatically recognizes and converts curl commands . create an index Called middleware curl -X PUT \u201clocalhost:9200/middleware Delete the index Called middleware curl -X DELETE \u201clocalhost:9200/middleware\u201d Create an index Called middleware with just one shard and no replicas PUT /middleware?pretty { \"settings\" : { \"number_of_shards\" : 1, \"number_of_replicas\" : 0 } } loading data into that index Called middleware PUT /middleware/_mapping { \"properties\": { \"post_time\": { \"type\": \"date\" }, \"username\": { \"type\": \"keyword\" }, \"message\": { \"type\": \"text\" } } } Now Load some data Manually POST /middleware/_doc/?pretty { \"user\" : \"collins\", \"post_date\" : \"2023-16-03T14:10:10\", \"message\" : \"Welcome to Elasticsearch?\" } see this mediam link, its cool Click-Link","title":"Sample Hands-on"},{"location":"weekly/GitHub/github/","text":"What GitHub? git config git config --global user.name \"sosotechnologies\" git config --global user.email \"a@gmail.com\" git config --global \u2013-list Changing GIT Remote ~~~git remote -``` ***OUTPUT*** [ec2-user@ip-172-31-201-145 terraform-efs]$ git remote -v origin https://github.com/sosotechnologies/terraform-EFS-Dynamic.git (fetch) origin https://github.com/sosotechnologies/terraform-EFS-Dynamic.git (push) git remote rm origin","title":"GitHub"},{"location":"weekly/GitHub/github/#what-github","text":"","title":"What GitHub?"},{"location":"weekly/GitHub/github/#git-config","text":"git config --global user.name \"sosotechnologies\" git config --global user.email \"a@gmail.com\" git config --global \u2013-list Changing GIT Remote ~~~git remote -``` ***OUTPUT*** [ec2-user@ip-172-31-201-145 terraform-efs]$ git remote -v origin https://github.com/sosotechnologies/terraform-EFS-Dynamic.git (fetch) origin https://github.com/sosotechnologies/terraform-EFS-Dynamic.git (push) git remote rm origin","title":"git config"},{"location":"weekly/Gitaction/gitaction/","text":"Getting Started GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production. Github actions has three main components: Events -> Workflows -> Actions Workflow triggers are events that cause a workflow to run. Events provide status updates from GitHub repos from code commits or other event of a project. For more information on workflows. see site-link: For more information about how to use workflow triggers: See workflow triggers Description of the below yaml file name: is straightforward, you can name what ever name you choose The events is the line thet starts with On. In the yaml file, we have an event to push. actions/checkout@v3: is already defined in Github. You can see more Here: github actions create a personal access token in your HitHub account and name: PERSONAL_TOKEN name: Publish docs via GitHub Pages on: push: branches: - main permissions: contents: write jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - uses: actions/cache@v2 with: key: ${{ github.ref }} path: .cache - run: pip install mkdocs-material - run: mkdocs gh-deploy --force env: PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }} CONFIG_FILE: folder/mkdocs.yml #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} .github/workflows mkdocs github actions managing-a-custom-domain","title":"GitHub Action"},{"location":"weekly/Gitaction/gitaction/#getting-started","text":"GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production. Github actions has three main components: Events -> Workflows -> Actions Workflow triggers are events that cause a workflow to run. Events provide status updates from GitHub repos from code commits or other event of a project. For more information on workflows. see site-link: For more information about how to use workflow triggers: See workflow triggers","title":"Getting Started"},{"location":"weekly/Gitaction/gitaction/#description-of-the-below-yaml-file","text":"name: is straightforward, you can name what ever name you choose The events is the line thet starts with On. In the yaml file, we have an event to push. actions/checkout@v3: is already defined in Github. You can see more Here: github actions create a personal access token in your HitHub account and name: PERSONAL_TOKEN name: Publish docs via GitHub Pages on: push: branches: - main permissions: contents: write jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - uses: actions/cache@v2 with: key: ${{ github.ref }} path: .cache - run: pip install mkdocs-material - run: mkdocs gh-deploy --force env: PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }} CONFIG_FILE: folder/mkdocs.yml #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} .github/workflows mkdocs github actions managing-a-custom-domain","title":"Description of the below yaml file"},{"location":"weekly/Jenkins/jenkins/","text":"kubectl exec -i -t my-pod --container main-app -- /bin/bash [ec2-user@ip-172-31-201-145 ~]$ sudo cp agent.jar ../../opt sudo chmod 777 -R /opt java -jar agent.jar -jnlpUrl http://ab5223ff23ed749b3ac51f92d244edcb-183061686.us-east-1.elb.amazonaws.com:8080/manage/computer/macaz/jenkins-agent.jnlp -secret 32d9d58f3296eb3d80c4d37836f5ed5ee6ed6cbfba258ad7e5c7c6e291f0ac37 -workDir \"/opt/build\" & Install Jdk click-here https://www.ashnik.com/install-jenkins-on-aws-ec2-instance-using-terraform/ after installing, you have to connect slave with the master if your jenkins server restarts and you forgot the password, run this command: sudo cat /var/lib/jenkins/users","title":"Jenkins"},{"location":"weekly/Kafka/kafka/","text":"Kafka Apache Kafka is a distributed event store and stream-processing platform. Source Systems --> stream data to Kafka --> Kafka --> target systems consume the data dtored in Kafka Kafka and Zookeeper Kafka and ZooKeeper work in conjunction to form a complete Kafka Cluster \u2060\u2014 with ZooKeeper providing the distributed clustering services, and Kafka handling the actual data streams and connectivity to clients. ZooKeeper kinda handles the leadership election of Kafka brokers and manages service discovery as well as cluster topology so each broker knows when brokers have entered or exited the cluster, when a broker dies and who the preferred leader node is for a given topic/partition pair. It also tracks when topics are created or deleted from the cluster and maintains a topic list. In general, ZooKeeper provides an in-sync view of the Kafka cluster. Kafka, on the other hand, is dedicated to handling the actual connections from the clients (producers and consumers) as well as managing the topic logs, topic log partitions, consumer groups ,and individual offsets. Forward messages from Pub/Sub to Kafka Use the gcloud CLI to create a Pub/Sub topic with a subscription. ... Open the file named /config/cps-source-connector.properties in a text editor. ... From the Kafka directory, run the following command: ... Use the gcloud CLI to publish a message to Pub/Sub. ... Read the message from Kafka. Streaming Kafka Messages to Google Cloud Pub/Sub Publisher publish messages that subscriber consumed. (a) Firstly, Configure the Pub/Sub topics to communicate with Kafka: Secondly to create a subscription for the to-kafka topic: create a subscription for traffic published from Kafka for Data exchange between Kafka and Pub/Sub BigQuery is Google's fully managed, serverless data warehouse that enables scalable analysis over petabytes of data.","title":"Kafka"},{"location":"weekly/Kafka/kafka/#kafka","text":"Apache Kafka is a distributed event store and stream-processing platform. Source Systems --> stream data to Kafka --> Kafka --> target systems consume the data dtored in Kafka","title":"Kafka"},{"location":"weekly/Kafka/kafka/#kafka-and-zookeeper","text":"Kafka and ZooKeeper work in conjunction to form a complete Kafka Cluster \u2060\u2014 with ZooKeeper providing the distributed clustering services, and Kafka handling the actual data streams and connectivity to clients. ZooKeeper kinda handles the leadership election of Kafka brokers and manages service discovery as well as cluster topology so each broker knows when brokers have entered or exited the cluster, when a broker dies and who the preferred leader node is for a given topic/partition pair. It also tracks when topics are created or deleted from the cluster and maintains a topic list. In general, ZooKeeper provides an in-sync view of the Kafka cluster. Kafka, on the other hand, is dedicated to handling the actual connections from the clients (producers and consumers) as well as managing the topic logs, topic log partitions, consumer groups ,and individual offsets.","title":"Kafka and Zookeeper"},{"location":"weekly/Kafka/kafka/#forward-messages-from-pubsub-to-kafka","text":"Use the gcloud CLI to create a Pub/Sub topic with a subscription. ... Open the file named /config/cps-source-connector.properties in a text editor. ... From the Kafka directory, run the following command: ... Use the gcloud CLI to publish a message to Pub/Sub. ... Read the message from Kafka.","title":"Forward messages from Pub/Sub to Kafka"},{"location":"weekly/Kafka/kafka/#streaming-kafka-messages-to-google-cloud-pubsub","text":"Publisher publish messages that subscriber consumed. (a) Firstly, Configure the Pub/Sub topics to communicate with Kafka: Secondly to create a subscription for the to-kafka topic: create a subscription for traffic published from Kafka for Data exchange between Kafka and Pub/Sub","title":"Streaming Kafka Messages to Google Cloud Pub/Sub"},{"location":"weekly/Kafka/kafka/#bigquery","text":"is Google's fully managed, serverless data warehouse that enables scalable analysis over petabytes of data.","title":"BigQuery"},{"location":"weekly/Kubernetes/kubernetes/","text":"To deploy EFS-EKS-using Terraform, clone My repo You can clone and build this Documentation and use the image mkdocs-docker-build","title":"Kubernetes"},{"location":"weekly/Openshift/openshift/","text":"What's Red Hat OpenShift Red Hat OpenShift, the industry's leading hybrid cloud application platform powered by Kubernetes, brings together tested and trusted services to reduce the friction of developing, modernizing, deploying, running, and managing applications. See official link: Openshift Getting Started - Steps Signup/signin to Openshift account: See-Link Navigate to: Clusters -> Cluster Type -> Amazon Web Services -> Installer-provisioned infrastructure Install OpenShift on AWS with user-provisioned infrastructure Select the option [Full control] See the site: /openshift/install/aws Install OpenShift on AWS Setup an AWS Instance and add security group number [6443] Setup a Route53 DNS make a new directory for the installation, mine is soso-dir mkdir soso-dir/ AWS Openshift installation Link: See-Link There are Two Cluster-Setup options to choose: a customized cluster or quickly install. Install the Openshift installer and client on your local computer. Move that file to your remote linux machine. You're see the client and installer. Extract the installer and client: tar xvf openshift-install-linux.tar.gz tar xvf openshift-client-linux.tar.gz Move the oc, kubectl and installer to usr/bin directoory sudo mv oc /usr/local/bin/ sudo mv kubectl /usr/local/bin/ sudo cp openshift-install /usr/local/bin/ sudo cp openshift-install /root/ which openshift-install oc help Install AWSCLI curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Copy the secret content and paste in a file. This secret is from the redhad account, used to Map the AWS Cluster with the redHat account. In the AWS Console, setup the following: Route53 public Hoster Zone Add OpenShift port 6443 to instance security groups Create Create Access and Secret Keys for the Root-user and configure aws aws configure CD to ROOT and Create an SSH Key in the root directory sudo su - ssh-keygen -t rsa -b 4096 -N '' -f id_rsa Evaluate and add to root. This will starts ssh-agent and configures the environment (via eval) of the running shell to point to that agent. eval \"$(ssh-agent -s)\" ssh-add /root/id_rsa Now start the prompt to install Openshift install Openshift using the config, so you can customize . A prompt will begin. The last prompt will be the secret Copy and paste the secret characters that we had saved earlier. openshift-install create install-config OPTIONAL: Make a copy of the file: cp -r install-config.yaml soso-config.yaml paste the below content in the file. edit the file to suite ur options. Note : Two keys will be added to the yaml file: The sshKey: cat id_rsa.pub The openShift Key we copied and pasted. apiVersion: v1 baseDomain: macazzzzz.com controlPlane: hyperthreading: Enabled name: master platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 3 compute: - hyperthreading: Enabled name: worker platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 1 metadata: name: openshift networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.0.0/20 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: aws: region: us-east-1 userTags: adminContact: Collins costCenter: 123 email: cafanwi@sosotechnologies.com publish: External pullSecret: '' sshKey: Install the cluster In the same directory of the openshift-install file, Run command to install: openshift-install create cluster --log-level debug ***Cluster done installing, you should see as below image, your creds and url. After installation, you should have your results as seen in the below image: Cat and export the konfig file cat auth/kubeconfig export KUBECONFIG=/root/auth/kubeconfig oc whoami Get Cluster URL with the below command oc cluster-info cd auth/ Working on cluster Some command commands: Create a new project called soso-project oc new-project soso-project --display-name 'Soso Project' oc project soso-project Sample use case: - Clone this repo: My docs dockerfile repo - Install Docker on terminal - Create a repo in Dockerdesktop called: sosotech/docs-repo-sosodocs docker build -t sosodocs . docker tag sosodocs sosotech/docs-repo/sosodocs:v1 oc cluster-info Openshift Image oc get is oc new-app --image=\"sosotech/docs-repo/sosodocs:v1\" --as-deployment-config oc expose service/docs-repo-sosodocs OTHER COMMANDS 1. Delete image string called: macaz oc delete is macaz Sample use case: - Clone this repo: My docs dockerfile repo - Install Docker/podman: docker build -t sosodocs . oc cluster-info sudo docker tag sosodocs default-route-openshift-image-registry.apps.openshift.macazzz.com/sosorepo:v1 docker login docker search registry.redhat.io/nginx For more commands on OpenShift: See the developer-cli-commands Link AWS Openshift installation Link: See-Link ./openshift-install create cluster --dir /root/ --log-level debug ### Destroy the cluster ./openshift-install destroy cluster --dir /root/ --log-level debug web console Link: https://console-openshift-console.apps.openshift.macazzz.com/k8s/ns/soso-project/image.openshift.io~v1~ImageStream kubeadmin 7JdHf-X34nV-ubmSh-ijSA4 docker-registry-default.127.0.0.1.nip.io Deploy Image Docker config path sudo vi /root/.docker/config.json OC TROUBLESHOOT oc get service -n default kubernetes -o 'jsonpath={.spec.clusterIP}'","title":"OpenShift"},{"location":"weekly/Openshift/openshift/#whats-red-hat-openshift","text":"Red Hat OpenShift, the industry's leading hybrid cloud application platform powered by Kubernetes, brings together tested and trusted services to reduce the friction of developing, modernizing, deploying, running, and managing applications. See official link: Openshift","title":"What's Red Hat OpenShift"},{"location":"weekly/Openshift/openshift/#getting-started-steps","text":"Signup/signin to Openshift account: See-Link Navigate to: Clusters -> Cluster Type -> Amazon Web Services -> Installer-provisioned infrastructure Install OpenShift on AWS with user-provisioned infrastructure Select the option [Full control] See the site: /openshift/install/aws","title":"Getting Started - Steps"},{"location":"weekly/Openshift/openshift/#install-openshift-on-aws","text":"Setup an AWS Instance and add security group number [6443] Setup a Route53 DNS make a new directory for the installation, mine is soso-dir mkdir soso-dir/ AWS Openshift installation Link: See-Link There are Two Cluster-Setup options to choose: a customized cluster or quickly install. Install the Openshift installer and client on your local computer. Move that file to your remote linux machine. You're see the client and installer. Extract the installer and client: tar xvf openshift-install-linux.tar.gz tar xvf openshift-client-linux.tar.gz Move the oc, kubectl and installer to usr/bin directoory sudo mv oc /usr/local/bin/ sudo mv kubectl /usr/local/bin/ sudo cp openshift-install /usr/local/bin/ sudo cp openshift-install /root/ which openshift-install oc help Install AWSCLI curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Copy the secret content and paste in a file. This secret is from the redhad account, used to Map the AWS Cluster with the redHat account. In the AWS Console, setup the following: Route53 public Hoster Zone Add OpenShift port 6443 to instance security groups Create Create Access and Secret Keys for the Root-user and configure aws aws configure CD to ROOT and Create an SSH Key in the root directory sudo su - ssh-keygen -t rsa -b 4096 -N '' -f id_rsa Evaluate and add to root. This will starts ssh-agent and configures the environment (via eval) of the running shell to point to that agent. eval \"$(ssh-agent -s)\" ssh-add /root/id_rsa Now start the prompt to install Openshift install Openshift using the config, so you can customize . A prompt will begin. The last prompt will be the secret Copy and paste the secret characters that we had saved earlier. openshift-install create install-config OPTIONAL: Make a copy of the file: cp -r install-config.yaml soso-config.yaml paste the below content in the file. edit the file to suite ur options. Note : Two keys will be added to the yaml file: The sshKey: cat id_rsa.pub The openShift Key we copied and pasted. apiVersion: v1 baseDomain: macazzzzz.com controlPlane: hyperthreading: Enabled name: master platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 3 compute: - hyperthreading: Enabled name: worker platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 1 metadata: name: openshift networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.0.0/20 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: aws: region: us-east-1 userTags: adminContact: Collins costCenter: 123 email: cafanwi@sosotechnologies.com publish: External pullSecret: '' sshKey:","title":"Install OpenShift on AWS"},{"location":"weekly/Openshift/openshift/#install-the-cluster","text":"In the same directory of the openshift-install file, Run command to install: openshift-install create cluster --log-level debug ***Cluster done installing, you should see as below image, your creds and url. After installation, you should have your results as seen in the below image: Cat and export the konfig file cat auth/kubeconfig export KUBECONFIG=/root/auth/kubeconfig oc whoami Get Cluster URL with the below command oc cluster-info cd auth/","title":"Install the cluster"},{"location":"weekly/Openshift/openshift/#working-on-cluster","text":"Some command commands: Create a new project called soso-project oc new-project soso-project --display-name 'Soso Project' oc project soso-project Sample use case: - Clone this repo: My docs dockerfile repo - Install Docker on terminal - Create a repo in Dockerdesktop called: sosotech/docs-repo-sosodocs docker build -t sosodocs . docker tag sosodocs sosotech/docs-repo/sosodocs:v1 oc cluster-info Openshift Image oc get is oc new-app --image=\"sosotech/docs-repo/sosodocs:v1\" --as-deployment-config oc expose service/docs-repo-sosodocs OTHER COMMANDS 1. Delete image string called: macaz oc delete is macaz Sample use case: - Clone this repo: My docs dockerfile repo - Install Docker/podman: docker build -t sosodocs . oc cluster-info sudo docker tag sosodocs default-route-openshift-image-registry.apps.openshift.macazzz.com/sosorepo:v1 docker login docker search registry.redhat.io/nginx For more commands on OpenShift: See the developer-cli-commands Link AWS Openshift installation Link: See-Link ./openshift-install create cluster --dir /root/ --log-level debug ### Destroy the cluster ./openshift-install destroy cluster --dir /root/ --log-level debug web console Link: https://console-openshift-console.apps.openshift.macazzz.com/k8s/ns/soso-project/image.openshift.io~v1~ImageStream kubeadmin 7JdHf-X34nV-ubmSh-ijSA4 docker-registry-default.127.0.0.1.nip.io Deploy Image Docker config path sudo vi /root/.docker/config.json OC TROUBLESHOOT oc get service -n default kubernetes -o 'jsonpath={.spec.clusterIP}'","title":"Working on cluster"},{"location":"weekly/Random/3tier/","text":"What is a 3 tier Application? A 3-tier application will compose of 3 layers: a Presentation tier, an Application tier, and a Data tier. Benefits of a 3 tier Application? The benefits of using a 3-tier architecture include improved horizontal scalability, performance, and availability. With three tiers, each part can be developed concurrently by a different team of programmers coding in different languages from the other tier developers. Because the programming for a tier can be changed or relocated without affecting the other tiers, the 3-tier model makes it easier for an enterprise or software packager to continually evolve an application as new needs and opportunities arise. Existing applications or critical parts can be permanently or temporarily retained and encapsulated within the new tier of which it becomes a component. The 3 different layers explained An example 3 tier application will compose of 3 layers: a Presentation tier, an application tier, and a Data tier. The Presentation tier is a graphical user interface (GUI) that communicates with the other two tiers\u2026in this layer, the users can directly access the web page, or an operating system's GUI. The is written in languages like built with HTML5, cascading style sheets (CSS) and JavaScript, and the presentation tier communicates with the other tiers through application program interface (API) calls. Application tier also known as the business logic tier\u2026 controls how the application functions. The application layer is written in a programming language such as Java and contains the business logic that supports the application's core functions. The underlying application tier can either be hosted on distributed servers in the cloud or on a dedicated in-house server, depending on how much processing power the application requires. Data tier contains the database servers, file shares, and anything that will be saved in the database... it can be hosted on-prem or in the cloud. Popular database systems for managing read/write access include MySQL, PostgreSQL, Microsoft Sample AWS 3-Tier Application The Presentation tier 1. A Public Route Table \u2014 associated with 2 Public Subnets (1/AZ) 2. At least 2 EC2 instances with a boot strapped Static Web Page \u2014 managed by an Auto Scaling Group 3. EC2 Web Server Security Group The Application tier 1. A Private Route Table \u2014 associated with 2 Private Subnets (1/AZ) 2. At least 2 EC2 instances managed by an Auto Scaling Group 3. EC2 Application Server Security Group The Data tier 1. A Private Route Table \u2014 associated with 2 Private Subnets (1/AZ) 2. A Database Security Group 3. A free Tier MySQL RDS Database Kubernetes API Users access the Kubernetes API using kubectl, client libraries, or by making REST requests. Both human users and Kubernetes service accounts can be authorized for API access. When a request reaches the API, it goes through several stages, illustrated in the following diagram: Steps: A request is sent through an API serves on port 443, protected by TLS. The API server presents a certificate. This certificate may be signed using a private ertificate authority (CA), or based on a public key infrastructure linked to a generally recognized CA. STEP 1: Authentication: Once TLS is established, the HTTP request moves to the Authentication step. Authentication modules include client certificates, password, and plain tokens, bootstrap tokens, and JSON Web Tokens (used for service accounts). Multiple authentication modules can be specified, in which case each one is tried in sequence, until one of them succeeds. If the request cannot be authenticated, it is rejected with HTTP status code 401. Otherwise, the user is authenticated as a specific username, and the username is available to subsequent steps to use in their decisions. Some authenticators also provide the group memberships of the user, while other authenticators do not. For more information about the different ways to Authenticate, check this url: See link STEP 2: Authorization So, after the request has been authenticated to be coming from a specific user, the request must next be authorized. A request MUST include: - the username of the requester - the requested action, and - the object affected by the action. Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode. When an administrator creates a cluster, they configure the authorization modules that should be used in the API server. If more than one authorization modules are configured, Kubernetes checks each module, and if any module authorizes the request, then the request can proceed. If all of the modules deny the request, then the request is denied (HTTP status code 403). See the below sample request: For more information about Authorization: check this url STEP 3: Admission control Admission Control modules are software modules that can modify or reject requests. They act on requests that create, modify, delete, or connect to (proxy) an object. Note: if any admission controller module rejects, then the request is immediately rejected. When multiple admission controllers are configured, they are called in order. STEP 4: Validation Once a request passes all admission controllers, it is validated using the validation routines for the corresponding API object, and then written to the object store. For more information about Admission control, check this url: For more information: check this url OpenSSL Setup steps: # Generate a ca.key with 2048bit $ openssl genrsa -out ca.key 2048 # According to the ca.key generate a ca.crt (use -days to set the certificate effective time): $ openssl req -x509 -new -nodes -key ca.key -subj \"/CN=${MASTER_IP}\" -days 10000 -out ca.crt # Generate a server.key with 2048bit: $ openssl genrsa -out server.key 2048 # Generate the certificate signing request based on the config file: $ openssl req -new -key server.key -out server.csr -config csr.conf # Generate the server certificate using the ca.key, ca.crt and server.csr: $ openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 10000 \\ -extensions v3_ext -extfile csr.conf # View the certificate signing request: $ openssl req -noout -text -in ./server.csr # View the certificate: $ openssl x509 -noout -text -in ./server.crt Cloud Native Security The 4C\u2019s of cloud Native Security Cloud: Each cloud provider makes security recommendations for running workloads securely in their environment. If you are running a Kubernetes cluster on your own hardware or a different cloud provider, consult your documentation for security best practices. For example, if you are running an EKS cluster you should understand Amazon Web Services Checkout the security rules: Common security Rules Cluster: When dealing with cluster security, there are two areas of concern for securing Kubernetes: 1.1. Securing the cluster components that are configurable 1.2. Securing the Components in the cluster (Your applications) 1.1. Securing the cluster For more info: Click-link 1.2. Securing the applications When it comes to securing the applications running n the cluster, the key question here is: how to secure the entire chain of applications in the cluster? For example: If you are running a service (Service Y) that is critical in a chain of other resources and a separate workload (Service Z) which is vulnerable to a resource exhaustion attack, then the risk of compromising Service Y is high if you do not limit the resources of Service Z. The Recommended security Areas for your application workloads To manage application security at the cluster level, a deep understanding of the following key concepts is required: RBAC Authorization (Access to the Kubernetes API) Authentication Application secrets management (and encrypting them in etcd at rest) Pod Security Quality of Service (and Cluster resource management) Network Policies TLS for Kubernetes Ingress RBAC Authorization: Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. The RBAC API declares four kinds of Kubernetes object: Role, ClusterRole, RoleBinding and ClusterRoleBinding. Role: Role contains rules that represent a set of permissions. Role is namespace specific, which means that a Role always sets permissions within a particular namespace. ClusterRole: clusterrole is used to define permissions on namespaced resources and be granted access within individual namespace(s). It\u2019s not namespace specific and can define permissions on cluster-scoped resources. Container: Container Security is a critical part of a comprehensive security assessment. For more on Container Security, read VMware Container Security Code: Some Coding Language Paradigms are comprise a variety of styles. styles are imperative, functional, logical, and object-oriented languages. Programmers can choose from these coding language paradigms to best-serve their needs for a specific project. The following are examples of each paradigm. Object-Oriented: Python, Java, C++, Imperative or Procedural: Cobol, Fortran, C Functional: Clojure Scala, Logical: Prolog, SQL Cloud code security focuses on code with several use cases, including infrastructure as code (IaC) security, application code security and software supply chain security. IaC Security: The key to a successful code security strategy for IaC is ensuring security is embedded directly in developer tools and workflows. By surfacing actionable feedback in code and embedding security guardrails in the build pipeline, IaC security empowers developers to ship infrastructure that\u2019s secure by default. Application Code Security: A strong code security strategy relies on secure coding best practices and code reviews to identify vulnerabilities. Through automated testing with technologies such as static application security testing (SAST) for custom code and software composition analysis (SCA) for open source code, code security solutions complement cloud workload protection by identifying CVEs as early as possible. Software Supply Chain Security Software supply chains comprise application and infrastructure components as well as the underlying pipelines, including version control systems (VCS), continuous integration and continuous deployment (CI/CD) pipelines, and registries. Software supply chain security is an important part of a strong code security strategy, as is understanding the connections between pipelines and infrastructure and application code across the development lifecycle. Operation systems An operating system is the most important software that runs on a computer. It manages the computer's memory and processes, as well as all of its software and hardware. It also allows you to communicate with the computer without knowing how to speak the computer's language. Without an operating system, a computer is useless. For the most part, the IT industry largely focuses on the top five OSs, including Apple macOS, Microsoft Windows, Google's Android OS, Linux Operating System, and Apple iOS Focus: Linux Linux is a family of open-source operating systems, which means they can be modified and distributed by anyone around the world. This is different from proprietary software like Windows, which can only be modified by the company that owns it. The advantages of Linux are that it is free, and there are many different distributions\u2014or versions\u2014you can choose from. Some common Linux distributions are Debian, Fedora and Red Hat, Ubuntu, and Linux Mint. SNo. Description Windows Linux 1. Directory listing dir ls -l 2. Rename a file ren mv 3. Copying a file copy cp 4. Moving a file move mv 5. Clear Screen cls clear 6. Delete file del rm 7. Check disk content chkdsk c: df 8. Search for a string in a file find grep 9. Create a new file type nul > soso.py touch/nano/vi 10. Returns your current directory location chdir pwd 11. Displays the time time date 12. Change the current directory cd cd 13. To create a new directory/folder md mkdir 14. To print something on the screen echo echo 15. To write in to files. edit vim(depends on editor) 16. To leave the terminal/command window. exit exit 17. To format a drive/partition. Format (C:) mke2fs or mformat 18. To list directory recursively. tree ls -R 19. To delete a directory. rmdir rm -rf/rmdir 22. To set environment variables. set var=value export var=value 23. To change file permissions. attribattrib +R collins.yamlattrib -R collins.yaml chown/chmod 24. To print the route packets trace to network host. tracert traceroute 25. Get systems network configuration ipconfig ifconfig 26. To print contents of a file. type cat 27. To send ICMP ECHO_REQUEST to network hosts. Ping Google.com ping 28. To query Internet name servers interactively. nslookup nslookup 29. For disk usage. chdisk du -s https://www.geeksforgeeks.org/linux-vs-windows-commands/ https://home.csulb.edu/~murdock/attrib.html#:~:text=Using%20the%20ATTRIB%20command%2C%20you,to%20as%20read%2Fwrite ). //for windows attributed","title":"3 Tier App"},{"location":"weekly/Random/3tier/#what-is-a-3-tier-application","text":"A 3-tier application will compose of 3 layers: a Presentation tier, an Application tier, and a Data tier.","title":"What is a 3 tier Application?"},{"location":"weekly/Random/3tier/#benefits-of-a-3-tier-application","text":"The benefits of using a 3-tier architecture include improved horizontal scalability, performance, and availability. With three tiers, each part can be developed concurrently by a different team of programmers coding in different languages from the other tier developers. Because the programming for a tier can be changed or relocated without affecting the other tiers, the 3-tier model makes it easier for an enterprise or software packager to continually evolve an application as new needs and opportunities arise. Existing applications or critical parts can be permanently or temporarily retained and encapsulated within the new tier of which it becomes a component.","title":"Benefits of a 3 tier Application?"},{"location":"weekly/Random/3tier/#the-3-different-layers-explained","text":"An example 3 tier application will compose of 3 layers: a Presentation tier, an application tier, and a Data tier. The Presentation tier is a graphical user interface (GUI) that communicates with the other two tiers\u2026in this layer, the users can directly access the web page, or an operating system's GUI. The is written in languages like built with HTML5, cascading style sheets (CSS) and JavaScript, and the presentation tier communicates with the other tiers through application program interface (API) calls. Application tier also known as the business logic tier\u2026 controls how the application functions. The application layer is written in a programming language such as Java and contains the business logic that supports the application's core functions. The underlying application tier can either be hosted on distributed servers in the cloud or on a dedicated in-house server, depending on how much processing power the application requires. Data tier contains the database servers, file shares, and anything that will be saved in the database... it can be hosted on-prem or in the cloud. Popular database systems for managing read/write access include MySQL, PostgreSQL, Microsoft Sample AWS 3-Tier Application The Presentation tier 1. A Public Route Table \u2014 associated with 2 Public Subnets (1/AZ) 2. At least 2 EC2 instances with a boot strapped Static Web Page \u2014 managed by an Auto Scaling Group 3. EC2 Web Server Security Group The Application tier 1. A Private Route Table \u2014 associated with 2 Private Subnets (1/AZ) 2. At least 2 EC2 instances managed by an Auto Scaling Group 3. EC2 Application Server Security Group The Data tier 1. A Private Route Table \u2014 associated with 2 Private Subnets (1/AZ) 2. A Database Security Group 3. A free Tier MySQL RDS Database","title":"The 3 different layers explained"},{"location":"weekly/Random/3tier/#kubernetes-api","text":"Users access the Kubernetes API using kubectl, client libraries, or by making REST requests. Both human users and Kubernetes service accounts can be authorized for API access. When a request reaches the API, it goes through several stages, illustrated in the following diagram:","title":"Kubernetes API"},{"location":"weekly/Random/3tier/#steps","text":"A request is sent through an API serves on port 443, protected by TLS. The API server presents a certificate. This certificate may be signed using a private ertificate authority (CA), or based on a public key infrastructure linked to a generally recognized CA. STEP 1: Authentication: Once TLS is established, the HTTP request moves to the Authentication step. Authentication modules include client certificates, password, and plain tokens, bootstrap tokens, and JSON Web Tokens (used for service accounts). Multiple authentication modules can be specified, in which case each one is tried in sequence, until one of them succeeds. If the request cannot be authenticated, it is rejected with HTTP status code 401. Otherwise, the user is authenticated as a specific username, and the username is available to subsequent steps to use in their decisions. Some authenticators also provide the group memberships of the user, while other authenticators do not. For more information about the different ways to Authenticate, check this url: See link STEP 2: Authorization So, after the request has been authenticated to be coming from a specific user, the request must next be authorized. A request MUST include: - the username of the requester - the requested action, and - the object affected by the action. Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode. When an administrator creates a cluster, they configure the authorization modules that should be used in the API server. If more than one authorization modules are configured, Kubernetes checks each module, and if any module authorizes the request, then the request can proceed. If all of the modules deny the request, then the request is denied (HTTP status code 403). See the below sample request: For more information about Authorization: check this url STEP 3: Admission control Admission Control modules are software modules that can modify or reject requests. They act on requests that create, modify, delete, or connect to (proxy) an object. Note: if any admission controller module rejects, then the request is immediately rejected. When multiple admission controllers are configured, they are called in order. STEP 4: Validation Once a request passes all admission controllers, it is validated using the validation routines for the corresponding API object, and then written to the object store. For more information about Admission control, check this url: For more information: check this url","title":"Steps:"},{"location":"weekly/Random/3tier/#openssl","text":"Setup steps: # Generate a ca.key with 2048bit $ openssl genrsa -out ca.key 2048 # According to the ca.key generate a ca.crt (use -days to set the certificate effective time): $ openssl req -x509 -new -nodes -key ca.key -subj \"/CN=${MASTER_IP}\" -days 10000 -out ca.crt # Generate a server.key with 2048bit: $ openssl genrsa -out server.key 2048 # Generate the certificate signing request based on the config file: $ openssl req -new -key server.key -out server.csr -config csr.conf # Generate the server certificate using the ca.key, ca.crt and server.csr: $ openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 10000 \\ -extensions v3_ext -extfile csr.conf # View the certificate signing request: $ openssl req -noout -text -in ./server.csr # View the certificate: $ openssl x509 -noout -text -in ./server.crt","title":"OpenSSL"},{"location":"weekly/Random/3tier/#cloud-native-security","text":"The 4C\u2019s of cloud Native Security Cloud: Each cloud provider makes security recommendations for running workloads securely in their environment. If you are running a Kubernetes cluster on your own hardware or a different cloud provider, consult your documentation for security best practices. For example, if you are running an EKS cluster you should understand Amazon Web Services Checkout the security rules: Common security Rules Cluster: When dealing with cluster security, there are two areas of concern for securing Kubernetes: 1.1. Securing the cluster components that are configurable 1.2. Securing the Components in the cluster (Your applications) 1.1. Securing the cluster For more info: Click-link 1.2. Securing the applications When it comes to securing the applications running n the cluster, the key question here is: how to secure the entire chain of applications in the cluster? For example: If you are running a service (Service Y) that is critical in a chain of other resources and a separate workload (Service Z) which is vulnerable to a resource exhaustion attack, then the risk of compromising Service Y is high if you do not limit the resources of Service Z. The Recommended security Areas for your application workloads To manage application security at the cluster level, a deep understanding of the following key concepts is required: RBAC Authorization (Access to the Kubernetes API) Authentication Application secrets management (and encrypting them in etcd at rest) Pod Security Quality of Service (and Cluster resource management) Network Policies TLS for Kubernetes Ingress RBAC Authorization: Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. The RBAC API declares four kinds of Kubernetes object: Role, ClusterRole, RoleBinding and ClusterRoleBinding. Role: Role contains rules that represent a set of permissions. Role is namespace specific, which means that a Role always sets permissions within a particular namespace. ClusterRole: clusterrole is used to define permissions on namespaced resources and be granted access within individual namespace(s). It\u2019s not namespace specific and can define permissions on cluster-scoped resources. Container: Container Security is a critical part of a comprehensive security assessment. For more on Container Security, read VMware Container Security Code: Some Coding Language Paradigms are comprise a variety of styles. styles are imperative, functional, logical, and object-oriented languages. Programmers can choose from these coding language paradigms to best-serve their needs for a specific project. The following are examples of each paradigm. Object-Oriented: Python, Java, C++, Imperative or Procedural: Cobol, Fortran, C Functional: Clojure Scala, Logical: Prolog, SQL Cloud code security focuses on code with several use cases, including infrastructure as code (IaC) security, application code security and software supply chain security. IaC Security: The key to a successful code security strategy for IaC is ensuring security is embedded directly in developer tools and workflows. By surfacing actionable feedback in code and embedding security guardrails in the build pipeline, IaC security empowers developers to ship infrastructure that\u2019s secure by default. Application Code Security: A strong code security strategy relies on secure coding best practices and code reviews to identify vulnerabilities. Through automated testing with technologies such as static application security testing (SAST) for custom code and software composition analysis (SCA) for open source code, code security solutions complement cloud workload protection by identifying CVEs as early as possible. Software Supply Chain Security Software supply chains comprise application and infrastructure components as well as the underlying pipelines, including version control systems (VCS), continuous integration and continuous deployment (CI/CD) pipelines, and registries. Software supply chain security is an important part of a strong code security strategy, as is understanding the connections between pipelines and infrastructure and application code across the development lifecycle.","title":"Cloud Native Security"},{"location":"weekly/Random/3tier/#operation-systems","text":"An operating system is the most important software that runs on a computer. It manages the computer's memory and processes, as well as all of its software and hardware. It also allows you to communicate with the computer without knowing how to speak the computer's language. Without an operating system, a computer is useless. For the most part, the IT industry largely focuses on the top five OSs, including Apple macOS, Microsoft Windows, Google's Android OS, Linux Operating System, and Apple iOS","title":"Operation systems"},{"location":"weekly/Random/3tier/#focus-linux","text":"Linux is a family of open-source operating systems, which means they can be modified and distributed by anyone around the world. This is different from proprietary software like Windows, which can only be modified by the company that owns it. The advantages of Linux are that it is free, and there are many different distributions\u2014or versions\u2014you can choose from. Some common Linux distributions are Debian, Fedora and Red Hat, Ubuntu, and Linux Mint. SNo. Description Windows Linux 1. Directory listing dir ls -l 2. Rename a file ren mv 3. Copying a file copy cp 4. Moving a file move mv 5. Clear Screen cls clear 6. Delete file del rm 7. Check disk content chkdsk c: df 8. Search for a string in a file find grep 9. Create a new file type nul > soso.py touch/nano/vi 10. Returns your current directory location chdir pwd 11. Displays the time time date 12. Change the current directory cd cd 13. To create a new directory/folder md mkdir 14. To print something on the screen echo echo 15. To write in to files. edit vim(depends on editor) 16. To leave the terminal/command window. exit exit 17. To format a drive/partition. Format (C:) mke2fs or mformat 18. To list directory recursively. tree ls -R 19. To delete a directory. rmdir rm -rf/rmdir 22. To set environment variables. set var=value export var=value 23. To change file permissions. attribattrib +R collins.yamlattrib -R collins.yaml chown/chmod 24. To print the route packets trace to network host. tracert traceroute 25. Get systems network configuration ipconfig ifconfig 26. To print contents of a file. type cat 27. To send ICMP ECHO_REQUEST to network hosts. Ping Google.com ping 28. To query Internet name servers interactively. nslookup nslookup 29. For disk usage. chdisk du -s https://www.geeksforgeeks.org/linux-vs-windows-commands/ https://home.csulb.edu/~murdock/attrib.html#:~:text=Using%20the%20ATTRIB%20command%2C%20you,to%20as%20read%2Fwrite ). //for windows attributed","title":"Focus: Linux"},{"location":"weekly/Random/Disaster-rec/","text":"Disaster Recovery","title":"Disaster Recovery"},{"location":"weekly/Random/Disaster-rec/#disaster-recovery","text":"","title":"Disaster Recovery"},{"location":"weekly/Random/Helm-Terraform-jenkins/","text":"What are we going to achieve here? See the official terraform repo install terraform: Link Jenkins EKS LoadBalancer Reverse Proxy If you see this: 'It appears that your reverse proxy setup is broken', Follow the 3 image steps to resolve: see the issue apply changes Use IaC(terraform) Deploy sample application with Helm Case study: bitnami Example chart $ helm repo list //to list the repos $ helm repo add collins-bitnami https://charts.bitnami.com/bitnami //add the bitnami chart and name the chart $ helm repo list //to list the repos $ helm search repo mysql // Search for charts within the Bitnami repository $ helm install mydb collins-bitnami/mysql $ helm repo remove bitnami //to remove the chart Case study: Nginx Example chart $ helm repo add nginx-stable https://helm.nginx.com/stable $ helm repo update followed by: $ helm install nginxingress nginx-stable/nginx-ingress $ helm install ---name nginxingress nginx-stable/nginx-ingress","title":"Helm-Terraform-jenkins"},{"location":"weekly/Random/Helm-Terraform-jenkins/#what-are-we-going-to-achieve-here","text":"See the official terraform repo install terraform: Link","title":"What are we going to achieve here?"},{"location":"weekly/Random/Helm-Terraform-jenkins/#jenkins-eks-loadbalancer-reverse-proxy","text":"If you see this: 'It appears that your reverse proxy setup is broken', Follow the 3 image steps to resolve: see the issue apply changes","title":"Jenkins EKS LoadBalancer Reverse Proxy"},{"location":"weekly/Random/Helm-Terraform-jenkins/#use-iacterraform","text":"","title":"Use IaC(terraform)"},{"location":"weekly/Random/Helm-Terraform-jenkins/#deploy-sample-application-with-helm","text":"Case study: bitnami Example chart $ helm repo list //to list the repos $ helm repo add collins-bitnami https://charts.bitnami.com/bitnami //add the bitnami chart and name the chart $ helm repo list //to list the repos $ helm search repo mysql // Search for charts within the Bitnami repository $ helm install mydb collins-bitnami/mysql $ helm repo remove bitnami //to remove the chart Case study: Nginx Example chart $ helm repo add nginx-stable https://helm.nginx.com/stable $ helm repo update followed by: $ helm install nginxingress nginx-stable/nginx-ingress $ helm install ---name nginxingress nginx-stable/nginx-ingress","title":"Deploy sample application with Helm"},{"location":"weekly/Random/domain-github/","text":"GITHUB custom Domain with GOdaddy Deploy your site and go to settings --> pages Go to GoDaddy site, purchase and click on domain u wanna use and configure. Go to ths GitHub-doc: To create A records, point your apex domain to the IP addresses for GitHub Pages-Link Add the Domain to GitHub pages In GitHub, go to settings --> pages and add the Custom domain configure an APEX domain Copy the IP'S and create individual records for all the IP's As og 04/2023, these are the 4 IP's: 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Save the 4 A-RECORDS each as seen in my example of the 1'st IP Save 1 CNAME-RECORD as seen in my example Get the value that will be added in the Godaddy value section from ur github url: Go now and recheck the domain in GitHub pages and all should be good now.","title":"Domain Git GoDaddy"},{"location":"weekly/Random/domain-github/#github-custom-domain-with-godaddy","text":"Deploy your site and go to settings --> pages Go to GoDaddy site, purchase and click on domain u wanna use and configure. Go to ths GitHub-doc: To create A records, point your apex domain to the IP addresses for GitHub Pages-Link","title":"GITHUB custom Domain with GOdaddy"},{"location":"weekly/Random/domain-github/#add-the-domain-to-github-pages","text":"In GitHub, go to settings --> pages and add the Custom domain","title":"Add the Domain to GitHub pages"},{"location":"weekly/Random/domain-github/#configure-an-apex-domain","text":"Copy the IP'S and create individual records for all the IP's As og 04/2023, these are the 4 IP's: 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Save the 4 A-RECORDS each as seen in my example of the 1'st IP Save 1 CNAME-RECORD as seen in my example Get the value that will be added in the Godaddy value section from ur github url: Go now and recheck the domain in GitHub pages and all should be good now.","title":"configure an APEX domain"},{"location":"weekly/Random/eks-iam/","text":"EKS ADmins authentication to Clusters Setting UP Cluster IAM Groups for EKS Admins STS Assume Role Trust Policy (STS Assume Role) IAM Policy (EKS Fill Access) IAM Role (you will add this rolearn to the cluster authentication-configmap) IAM Group Policy (reference the IAM role) IAM Groups (create Iam group called: sosoadmins ) Setting UP Cluster IAM Users for the Group Create the users and add them to the sosoadmins group Ex: If you create these admins sosoadmin1 , sosoadmin2 , and add to the group, they will have access to the cluster To get the users, run command: aws sts get-caller-identity","title":"EKS-IAM"},{"location":"weekly/Random/eks-iam/#eks-admins-authentication-to-clusters","text":"","title":"EKS ADmins authentication to Clusters"},{"location":"weekly/Random/eks-iam/#setting-up-cluster-iam-groups-for-eks-admins","text":"STS Assume Role Trust Policy (STS Assume Role) IAM Policy (EKS Fill Access) IAM Role (you will add this rolearn to the cluster authentication-configmap) IAM Group Policy (reference the IAM role) IAM Groups (create Iam group called: sosoadmins )","title":"Setting UP Cluster IAM Groups for EKS Admins"},{"location":"weekly/Random/eks-iam/#setting-up-cluster-iam-users-for-the-group","text":"Create the users and add them to the sosoadmins group Ex: If you create these admins sosoadmin1 , sosoadmin2 , and add to the group, they will have access to the cluster To get the users, run command: aws sts get-caller-identity","title":"Setting UP Cluster IAM Users for the Group"},{"location":"weekly/Random/git-ecr-jenkins/","text":"What are we going to achieve here? Our developers have written some java base web-app and they are pushing that code to a git repo. We need a DevOps envineer who will comein and help setup our infrastruucture, Build an automation mechanish, that we can utilize to integrate this code for testing, Deliver, Deployment. Also, we will need these artifacts to be built into containerized application, build an ocastration and deploy this app to our end users. Steps Use IaC(terraform) to setup the required ingrastructures \"EC2, IAM, EKS\" install terraform: Link install jenkins server install jenkins server Link Install Docker Install Docker Link Install Git sudo yum install git -y Add Jenkins user to the Docker Group sudo usermod \u2013aG docker sosojenkinsadmin OR sudo usermod \u2013a -G docker jenkins restart the Jenkins server sudo service jenkins restart Reload the system Daemon sudo systemctl daemon-reload Restart the Docker Service sudo service docker restart Return to the Jenkins UI and install these 2 plugins: Install the pluginn called Docker and Docker Pipeline Update IAM role and and attach to instance, To Give instance ECR permissions Create an IAM role with Admin Or with the needed permissions for ECR and other services. Create ECR Repo vis same link, scroll to bottom of the page aws ecr create-repository --repository-name soso-repository --region us-east-1 When you create the ECR, go to the ECR in AWS Console and retreive the commands to push image. See the link for Creating a container image: Link Check authentication to ECR check to see if you can successfully login to ecr [my example ecr code below, USE YOURS ], you have to be root, sudo su - aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 088789840359.dkr.ecr.us-east-1.amazonaws.com Build and Push Docker sudo docker build -t sosodocs . Tag the sosodoc image sudo docker tag sosodocs 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Push image to repo sudo docker push 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Install AWS [ CloudBees AWS Credentials ] Plugin and configure Dashboard --> Manage Jenkins --> Plugin Manager setup aws credentials, get your AWS Access Key ID & Secret Access Key Dashboard --> Manage Jenkins --> Manage Credentials --> System --> Global credentials (unrestricted) AWS Credentials |_ID |_Description |_Access Key ID |_Secret Access Key Create new Pipeline Job using below pipeline script with my aws creds use pool scm * * * pipeline { agent any environment { AWS_ACCOUNT_ID=\"368085106192\" AWS_DEFAULT_REGION=\"us-east-1\" IMAGE_REPO_NAME=\"soso-repository\" IMAGE_TAG=\"latest\" REPOSITORY_URI = \"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${IMAGE_REPO_NAME}\" } stages { stage('Logging into AWS ECR') { steps { script { sh \"aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com\" } } } stage('Cloning Git') { steps { checkout([$class: 'GitSCM', branches: [[name: '*/main']], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: '', url: 'https://github.com/sosotechnologies/aws-nodeJs-ecr-jenkins.git']]]) } } // Building Docker images stage('Building image') { steps{ script { dockerImage = docker.build \"${IMAGE_REPO_NAME}:${IMAGE_TAG}\" } } } // Uploading Docker images into AWS ECR stage('Pushing to ECR') { steps{ script { sh \"docker tag ${IMAGE_REPO_NAME}:${IMAGE_TAG} ${REPOSITORY_URI}:$IMAGE_TAG\" sh \"docker push ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${IMAGE_REPO_NAME}:${IMAGE_TAG}\" } } } } } Bonus!!! Test your pipeline with example codes Simple Jenkins pipeline Scripts for AWS Pipeline version 1 pipeline { agent any stages { stage('Welcome to sosotech') { steps { sh ''' aws --version ''' } } } } Pipeline version 2 pipeline { agent any stages { stage('Welcome to sosotech') { steps { sh ''' aws --version aws ec2 describe-instances ''' } } } } Pipeline version 3 pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } } Pipeline version 4 pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { withCredentials([aws(accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'all-in-one-devops', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY')]) { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } } } Pipeline version 5 For this step, i'll list buckets pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { withCredentials([aws(accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'all-in-one-devops', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY')]) { sh ''' aws --version aws s3api list-buckets --query \"Buckets[].Name\" ''' } } } } } Pipeline version 6 For this step, i'll create an IRSA and save IAM role in Jenkins credentials as 'irsa-creds' pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" WE_KNOW_TECHNOLOGY=credentials('irsa-creds') } stages { stage('Hello') { steps { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } }","title":"ECR-Jenkins"},{"location":"weekly/Random/git-ecr-jenkins/#what-are-we-going-to-achieve-here","text":"Our developers have written some java base web-app and they are pushing that code to a git repo. We need a DevOps envineer who will comein and help setup our infrastruucture, Build an automation mechanish, that we can utilize to integrate this code for testing, Deliver, Deployment. Also, we will need these artifacts to be built into containerized application, build an ocastration and deploy this app to our end users.","title":"What are we going to achieve here?"},{"location":"weekly/Random/git-ecr-jenkins/#steps","text":"","title":"Steps"},{"location":"weekly/Random/git-ecr-jenkins/#use-iacterraform-to-setup-the-required-ingrastructures-ec2-iam-eks","text":"install terraform: Link","title":"Use IaC(terraform) to setup the required ingrastructures \"EC2, IAM, EKS\""},{"location":"weekly/Random/git-ecr-jenkins/#install-jenkins-server","text":"install jenkins server Link","title":"install jenkins server"},{"location":"weekly/Random/git-ecr-jenkins/#install-docker","text":"Install Docker Link","title":"Install Docker"},{"location":"weekly/Random/git-ecr-jenkins/#install-git","text":"sudo yum install git -y","title":"Install Git"},{"location":"weekly/Random/git-ecr-jenkins/#add-jenkins-user-to-the-docker-group","text":"sudo usermod \u2013aG docker sosojenkinsadmin OR sudo usermod \u2013a -G docker jenkins","title":"Add Jenkins user to the Docker Group"},{"location":"weekly/Random/git-ecr-jenkins/#restart-the-jenkins-server","text":"sudo service jenkins restart","title":"restart the Jenkins server"},{"location":"weekly/Random/git-ecr-jenkins/#reload-the-system-daemon","text":"sudo systemctl daemon-reload","title":"Reload the system Daemon"},{"location":"weekly/Random/git-ecr-jenkins/#restart-the-docker-service","text":"sudo service docker restart","title":"Restart the Docker Service"},{"location":"weekly/Random/git-ecr-jenkins/#return-to-the-jenkins-ui-and-install-these-2-plugins","text":"Install the pluginn called Docker and Docker Pipeline","title":"Return to the Jenkins UI and install these 2 plugins:"},{"location":"weekly/Random/git-ecr-jenkins/#update-iam-role-and-and-attach-to-instance-to-give-instance-ecr-permissions","text":"Create an IAM role with Admin Or with the needed permissions for ECR and other services.","title":"Update IAM role and and attach to instance, To Give instance ECR permissions"},{"location":"weekly/Random/git-ecr-jenkins/#create-ecr-repo-vis-same-link-scroll-to-bottom-of-the-page","text":"aws ecr create-repository --repository-name soso-repository --region us-east-1 When you create the ECR, go to the ECR in AWS Console and retreive the commands to push image. See the link for Creating a container image: Link","title":"Create ECR Repo vis same link, scroll to bottom of the page"},{"location":"weekly/Random/git-ecr-jenkins/#check-authentication-to-ecr","text":"check to see if you can successfully login to ecr [my example ecr code below, USE YOURS ], you have to be root, sudo su - aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 088789840359.dkr.ecr.us-east-1.amazonaws.com","title":"Check authentication to ECR"},{"location":"weekly/Random/git-ecr-jenkins/#build-and-push-docker","text":"sudo docker build -t sosodocs . Tag the sosodoc image sudo docker tag sosodocs 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Push image to repo sudo docker push 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1","title":"Build and Push Docker"},{"location":"weekly/Random/git-ecr-jenkins/#install-aws-cloudbees-aws-credentials-plugin-and-configure","text":"Dashboard --> Manage Jenkins --> Plugin Manager","title":"Install AWS [ CloudBees AWS Credentials ] Plugin and configure"},{"location":"weekly/Random/git-ecr-jenkins/#setup-aws-credentials-get-your-aws-access-key-id-secret-access-key","text":"Dashboard --> Manage Jenkins --> Manage Credentials --> System --> Global credentials (unrestricted) AWS Credentials |_ID |_Description |_Access Key ID |_Secret Access Key","title":"setup aws credentials, get your AWS Access Key ID &amp; Secret Access Key"},{"location":"weekly/Random/git-ecr-jenkins/#create-new-pipeline-job-using-below-pipeline-script-with-my-aws-creds","text":"use pool scm * * * pipeline { agent any environment { AWS_ACCOUNT_ID=\"368085106192\" AWS_DEFAULT_REGION=\"us-east-1\" IMAGE_REPO_NAME=\"soso-repository\" IMAGE_TAG=\"latest\" REPOSITORY_URI = \"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${IMAGE_REPO_NAME}\" } stages { stage('Logging into AWS ECR') { steps { script { sh \"aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com\" } } } stage('Cloning Git') { steps { checkout([$class: 'GitSCM', branches: [[name: '*/main']], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: '', url: 'https://github.com/sosotechnologies/aws-nodeJs-ecr-jenkins.git']]]) } } // Building Docker images stage('Building image') { steps{ script { dockerImage = docker.build \"${IMAGE_REPO_NAME}:${IMAGE_TAG}\" } } } // Uploading Docker images into AWS ECR stage('Pushing to ECR') { steps{ script { sh \"docker tag ${IMAGE_REPO_NAME}:${IMAGE_TAG} ${REPOSITORY_URI}:$IMAGE_TAG\" sh \"docker push ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${IMAGE_REPO_NAME}:${IMAGE_TAG}\" } } } } }","title":"Create new Pipeline Job using below pipeline script with my aws creds"},{"location":"weekly/Random/git-ecr-jenkins/#bonus","text":"Test your pipeline with example codes","title":"Bonus!!!"},{"location":"weekly/Random/git-ecr-jenkins/#simple-jenkins-pipeline-scripts-for-aws","text":"Pipeline version 1 pipeline { agent any stages { stage('Welcome to sosotech') { steps { sh ''' aws --version ''' } } } }","title":"Simple Jenkins pipeline Scripts for AWS"},{"location":"weekly/Random/git-ecr-jenkins/#pipeline-version-2","text":"pipeline { agent any stages { stage('Welcome to sosotech') { steps { sh ''' aws --version aws ec2 describe-instances ''' } } } }","title":"Pipeline version 2"},{"location":"weekly/Random/git-ecr-jenkins/#pipeline-version-3","text":"pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } }","title":"Pipeline version 3"},{"location":"weekly/Random/git-ecr-jenkins/#pipeline-version-4","text":"pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { withCredentials([aws(accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'all-in-one-devops', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY')]) { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } } }","title":"Pipeline version 4"},{"location":"weekly/Random/git-ecr-jenkins/#pipeline-version-5","text":"For this step, i'll list buckets pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { withCredentials([aws(accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'all-in-one-devops', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY')]) { sh ''' aws --version aws s3api list-buckets --query \"Buckets[].Name\" ''' } } } } }","title":"Pipeline version 5"},{"location":"weekly/Random/git-ecr-jenkins/#pipeline-version-6","text":"For this step, i'll create an IRSA and save IAM role in Jenkins credentials as 'irsa-creds' pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" WE_KNOW_TECHNOLOGY=credentials('irsa-creds') } stages { stage('Hello') { steps { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } }","title":"Pipeline version 6"},{"location":"weekly/Random/k8sapi%20copy/","text":"Tables Are Cool col 1 is b sdvvrbbbbbbbbrggg $1600 SNo. Windows Linux Description","title":"K8sapi copy"},{"location":"weekly/Random/k8sapi/","text":"","title":"Kubernetes API"},{"location":"weekly/Random/operating-systems/","text":"linux commands cat /etc/os-release","title":"OS Types Commands"},{"location":"weekly/SonarQube/sonarqube/","text":"Wheck Style analysis Quality Check, Gate https://docs.sonarqube.org/latest/analyzing-source-code/scanners/jenkins-extension-sonarqube/ https://docs.sonarqube.org/latest/analyzing-source-code/scanners/sonarscanner/ Create project Create a new project called: sosotech Generate a token called: sosotech-token 847ff2f312def534eb71b3a81750227f96673c24 Quality gate Create a New Quality gate Add conditions Next: Add the quality gate to the sonarqube project Go To: Projects --> Quality Gate --> Projects Setting --> webhooks Create a webhook called: soso-sonar-jenkins-webhook . you can name watever you choose - The webhook url is my jenkins url and the name - sonarqube-webhook like so http://34.230.86.243:8080/sonarqube-webhook","title":"Sonarqube"},{"location":"weekly/SonarQube/sonarqube/#create-project","text":"Create a new project called: sosotech Generate a token called: sosotech-token 847ff2f312def534eb71b3a81750227f96673c24","title":"Create project"},{"location":"weekly/SonarQube/sonarqube/#quality-gate","text":"Create a New Quality gate Add conditions Next: Add the quality gate to the sonarqube project Go To: Projects --> Quality Gate --> Projects Setting --> webhooks Create a webhook called: soso-sonar-jenkins-webhook . you can name watever you choose - The webhook url is my jenkins url and the name - sonarqube-webhook like so http://34.230.86.243:8080/sonarqube-webhook","title":"Quality gate"},{"location":"weekly/Terraform/terraform/","text":"Terraform Working Links for more information see link: Sosotech Terraform Repo For deploying Helm Charts with Terraform: Helm, Terraform, Jenkins Github Repositories used for this course Terraform on AWS EKS Kubernetes IaC SRE- 50 Real-World Demos Course Presentation Kubernetes Fundamentals Important Note: Please go to these repositories and FORK these repositories and make use of them during the course. Scorage class: Link Persistent volume: Link Note . You Must have an understanding of volume_handle - (Required) A map that specifies static properties of a volume. For more info see Kubernetes reference. This is the way I am mapping the EFS I.D to the Volume. The volume handles is not neccessary for dynamic provisioning, because you will not creating a PV file, and you will be using storage Provisioner to dynamically create the PV's. Pass the provisioner in your storage Class Object. For your mount_path = \"/data\", any data in the mountpath will be store in your EFS. You can name to whatever u choose k exec --stdin --tty soso-pod --/bin/sh MY ADVISE - use different S3 buckets for critical backends, so If an s3 is mistakenly deleted some how, you can create a new bucket and re-initialize that specific resource group. For modules: See link StateFile ```terraform state list","title":"Terraform Working Links"},{"location":"weekly/Terraform/terraform/#terraform-working-links","text":"for more information see link: Sosotech Terraform Repo For deploying Helm Charts with Terraform: Helm, Terraform, Jenkins","title":"Terraform Working Links"},{"location":"weekly/Terraform/terraform/#github-repositories-used-for-this-course","text":"Terraform on AWS EKS Kubernetes IaC SRE- 50 Real-World Demos Course Presentation Kubernetes Fundamentals Important Note: Please go to these repositories and FORK these repositories and make use of them during the course. Scorage class: Link Persistent volume: Link Note . You Must have an understanding of volume_handle - (Required) A map that specifies static properties of a volume. For more info see Kubernetes reference. This is the way I am mapping the EFS I.D to the Volume. The volume handles is not neccessary for dynamic provisioning, because you will not creating a PV file, and you will be using storage Provisioner to dynamically create the PV's. Pass the provisioner in your storage Class Object. For your mount_path = \"/data\", any data in the mountpath will be store in your EFS. You can name to whatever u choose k exec --stdin --tty soso-pod --/bin/sh MY ADVISE - use different S3 buckets for critical backends, so If an s3 is mistakenly deleted some how, you can create a new bucket and re-initialize that specific resource group. For modules: See link","title":"Github Repositories used for this course"},{"location":"weekly/Terraform/terraform/#statefile","text":"```terraform state list","title":"StateFile"}]}