{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])"},"docs":[{"location":"","text":"Welcome To SosoTech We specialize in IT training and Hands-on. What we do We train New and Working IT Professionals on the following fields: DevOps SRE Cloud Engineers AI Engineers","title":"About Us"},{"location":"#welcome-to-sosotech","text":"We specialize in IT training and Hands-on.","title":"Welcome To SosoTech"},{"location":"#what-we-do","text":"We train New and Working IT Professionals on the following fields: DevOps SRE Cloud Engineers AI Engineers","title":"What we do"},{"location":"weekly/AI/ai/","text":"Artificial Intelligence setup google codeLab Open-Link Theres always been this lask of understanding between machine learning and artificial intelligence. Put in context, artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience. Artificial Neural Network (ANN) Artificial Neural Networks the most important part of the deep learning model, they are the brain inspired deep learning tool which replicate the way humans learn. The Neuron In the Human Body, the Neuron is a part of the brain and nervous system. The three broad functions of the CNS are: To take in sensory information process information, and To send out motor signals. But Now How do Neurons Connect? Neurons communicate with each other by sending chemicals, called neurotransmitters, across a tiny space called a synapse, between the axons and dendrites of nearby neurons. So, now that we have that understanding of a human neuron, and how the neurons connect, the goal of AI was to replicate the way the human brain learns. In another sence, create machines that would have the ability to learn, adapt, and make decisions. ### Neurona in AI As seen in the diagram below, the Input Values are independent Variables of [1, 2, and m]. These variables need to be standardized to make sure all variables contribute evenly to a scale when items are added together, or to make it easier to interpret results of a regression or other analysis.","title":"AI"},{"location":"weekly/AI/ai/#artificial-intelligence","text":"setup google codeLab Open-Link Theres always been this lask of understanding between machine learning and artificial intelligence. Put in context, artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience.","title":"Artificial Intelligence"},{"location":"weekly/AI/ai/#artificial-neural-network-ann","text":"Artificial Neural Networks the most important part of the deep learning model, they are the brain inspired deep learning tool which replicate the way humans learn. The Neuron In the Human Body, the Neuron is a part of the brain and nervous system. The three broad functions of the CNS are: To take in sensory information process information, and To send out motor signals. But Now How do Neurons Connect? Neurons communicate with each other by sending chemicals, called neurotransmitters, across a tiny space called a synapse, between the axons and dendrites of nearby neurons. So, now that we have that understanding of a human neuron, and how the neurons connect, the goal of AI was to replicate the way the human brain learns. In another sence, create machines that would have the ability to learn, adapt, and make decisions. ### Neurona in AI As seen in the diagram below, the Input Values are independent Variables of [1, 2, and m]. These variables need to be standardized to make sure all variables contribute evenly to a scale when items are added together, or to make it easier to interpret results of a regression or other analysis.","title":"Artificial Neural Network (ANN)"},{"location":"weekly/Cloud-Technologies/azure/","text":"","title":"AZURE"},{"location":"weekly/Cloud-Technologies/gcp/","text":"","title":"GCP"},{"location":"weekly/Cloud-Technologies/intro-aws/","text":"Cloud Technologies Defining cloud computing Defining private, public, and hybrid cloud Exploring the components of cloud infrastructure Defining the different Service Models What is Cloud Computing? Cloud computing is the use of hosted services, such as servers, data storage, databases, software, and networking over the internet\u00ac\u200a\u2014\u200ainstead of directly accessing your computer hard drive. Types of cloud computing Cloud computing is offered in 3 major forms: private, public, and hybrid cloud. Private Cloud: The private cloud computing service is a cloud computing model where the infrastructure is dedicated only to select users instead of the general public, either over the Internet or a private internal network. Other names for the private cloud are internal or corporate cloud. Some private cloud providers are VMware-vRealize Suite Cloud Management Platform, AWS-Virtual Private Cloud, Microsoft-Azure Private Cloud\u2026 Public Cloud: The public cloud is defined as computing services offered by third-party providers over the public Internet, making them available to anyone who wants to use or purchase them. They may be free or sold on-demand, allowing customers to pay only per usage for the CPU cycles, storage, or bandwidth they consume. Some public cloud services are AWS, GCP, Azure, IBM Cloud, Alibaba Cloud\u2026 Hybrid Cloud: A hybrid cloud is a computing environment that combines an on-premises data center (also called a private cloud) with a public cloud, allowing data and applications to be shared between them. Some public cloud providers are VMware, Microsoft Azure\u2026 Components of cloud infrastructure Client-side Infrastructure: Here, users interact with the cloud through a front-end GUI (Graphical User Interface). Desktop, laptops, mobiles, tablets, and others are used on the client-side to access services. Application: The application may be any software or platform that a client wants to access. The operating system: The cloud operating system manages the operation, the virtual servers, execution and processes of virtual machines and virtual infrastructure, as well as the back-end hardware and software resources. Service: Cloud Services are offered in different models. These services are being provided based on the client\u2019s request. Some of the service models are seen below Service Models Cloud-Computing providers offer their \u201cservices\u201d according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). IAAS: Infrastructure As A Service (IAAS) is means of delivering computing infrastructure as on-demand services. It is one of the three fundamental cloud service model servers\u2019 storage network operating systems. In the user purchasing servers, software data center space, or network equipment and renting those resources as a fully outsourced service can demand model. It allows dynamic scaling, and the resources are distributed as a service. It generally includes multiple users on a single piece of hardware. PAAS: Platform as A Service (PAAS) is a cloud delivery model for applications composed of services managed by a third party. It provides elastic scaling of your application which allows developers to build applications and services over the internet and the deployment models include public, private and hybrid. SAAS: Software As A Service (SAAS) allows users to run existing online applications and it is a model software that is deployed as a hosting service and is accessed over Output Rephrased/Re-written Text the internet or software delivery model during which software and its associated data are hosted centrally and accessed using their client, usually an online browser over the web. SAAS services are used for the development and deployment of modern applications. The emergence of Cloud computing has given rise to more as-a-service offerings. Some of them include: - AIaaS: AI as a service - DaaS: Desktop as a service - ITaaS: IT as a service - RaaS: Ransomware as a service. Top benefits of cloud computing High Speed: the ability to spin up new instances in a matter of seconds. Efficiency and Cost Reduction Provides efficient Data Security Scalable: ease with quickly\u200a\u2014\u200ascale up/down infrastructures. Collaboration: Cloud environments enable better collaboration across teams. Provides unlimited Storage Capacity. Provides ease in Back-up and Restoring of Data Reliability: Cloud computing makes data backup, disaster recovery, and business continuity easier and less expensive because data can be mirrored at multiple redundant sites on the cloud provider\u2019s network. FOCUS: AWS CLOUD What Is Amazon Web Services(AWS): Amazon Web Services(AWS) is Amazon\u2019s cloud computing platform that offers a mix of packaged platform as a service (PaaS), software as a service (SaaS), and infrastructure as a service (IaaS). ### EC2 Amazon EC2 is AWS primary web service that provides resizable compute capacity in the cloud. Instance types: Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. Example instance Family: c4 Compute optimized\u2014For workloads requiring significant processing r3 Memory optimized\u2014For memory-intensive workloads i2 Storage optimized\u2014For workloads requiring high amounts of fast SSD storage g2 GPU-based instances\u2014Intended for graphics and general-purpose GPU compute workloads Amazon Machine Images (AMIs): Defines the initial software that will be on an instance when it is launched. An AMI defines; The Operating System (OS) and its configuration, The initial state of any patches, Application or system software. AMI\u2019s can be AWS published, from the AWS Marketplace, Generated from Existing Instances, or Uploaded Virtual Servers Security Groups: This is a VPC concept. If an instance is running in an Amazon VPC, you can change which security groups are associated with an instance while the instance is running. EBS: Is type of \u201cblock storage\u201d volume offering by AWS. EBS provides persistent block-level storage volumes for use with Amazon EC2 instances. Instance Stores: Is another type of \u201cblock storage\u201d volume offering by AWS for your instance. This storage is located on disks that are physically attached to the host computer. ### S3 Amazon Simple Storage Service is a secure, durable, and highly-scalable cloud storage. Some use cases for cloud storage include: - Backup and archive for on-premises or cloud data. - Content, media, and software storage and distribution. - Big data analytics. - Static website hosting. - Cloud-native mobile and Internet application hosting. ### VPC The Amazon Virtual Private Cloud (Amazon VPC) is a custom-defined virtual network within the AWS Cloud. ![cloud1](photos/cloud5.png) VPC COMPONENTS: An Amazon VPC consists of the following main components: Subnets, Route tables, Dynamic Host Configuration Protocol (DHCP) option sets Security groups, Network Access Control Lists (ACLs). Also, optional components: Internet Gateways (IGWs), Elastic IP (EIP) addresses, Elastic Network Interfaces (ENIs), Endpoints, Peering, Network Address Translation (NATs) instances and NAT gateways, Virtual Private Gateway (VPG), Customer Gateways (CGWs), and Virtual Private Networks (VPNs) ***Subnets:*** A subnet is a segment of an Amazon VPC\u2019s IP address range. Subnets reside within one Availability Zone and cannot span zones; You can, however, have multiple subnets in one Availability Zone. Each subnet must be associated with a route table, which controls the routing for the subnet. ***Route Tables:*** A route table is a logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where network traffic is directed. ***Internet Gateways:*** IGW allows communication between instances in your Amazon VPC and the Internet. ***Dynamic Host Configuration*** Protocol (DHCP) Option Sets: provides a standard for passing configuration information to hosts on a TCP/IP network. AWS automatically creates and associates a DHCP option set for your Amazon VPC upon creation and sets two options: domain-name-servers (defaulted to AmazonProvidedDNS) and domain-name (defaulted to the domain name for your region). ### IAM With AWS IAM Identity Center (successor to AWS Single Sign-On), you can manage sign-in security for your workforce identities, also known as workforce users. IAM Identity Center provides one place where you can create or connect workforce users and centrally manage their access across all their AWS accounts and applications. For more on IAM chack the link: IAM ### AWS ELASTIC LOAD BALANCER (ELB) AWS has an Elastic Load Balancer that is used to distribute incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing supports routing and load balancing of Hypertext. Transfer Protocol (HTTP), Hypertext Transfer Protocol Secure (HTTPS), Transmission Control Protocol (TCP), and Secure Sockets Layer (SSL) traffic to Amazon EC2 instances. Types of AWS ELB: The two major Elastic load balancers are: ***Application Load Balancer*** Application layer that describes the use of HTTP and HTTPS connections from clients to the load balancer and from the load balancer to your back-end instance. ***Network Load Balancer*** Are used to route traffic through layer 4, based IPs and TCP or UDP ports. ### EKS Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. For more on EKS chack the link: [EKS]https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html","title":"AWS"},{"location":"weekly/Cloud-Technologies/intro-aws/#cloud-technologies","text":"Defining cloud computing Defining private, public, and hybrid cloud Exploring the components of cloud infrastructure Defining the different Service Models","title":"Cloud Technologies"},{"location":"weekly/Cloud-Technologies/intro-aws/#what-is-cloud-computing","text":"Cloud computing is the use of hosted services, such as servers, data storage, databases, software, and networking over the internet\u00ac\u200a\u2014\u200ainstead of directly accessing your computer hard drive.","title":"What is Cloud Computing?"},{"location":"weekly/Cloud-Technologies/intro-aws/#types-of-cloud-computing","text":"Cloud computing is offered in 3 major forms: private, public, and hybrid cloud. Private Cloud: The private cloud computing service is a cloud computing model where the infrastructure is dedicated only to select users instead of the general public, either over the Internet or a private internal network. Other names for the private cloud are internal or corporate cloud. Some private cloud providers are VMware-vRealize Suite Cloud Management Platform, AWS-Virtual Private Cloud, Microsoft-Azure Private Cloud\u2026 Public Cloud: The public cloud is defined as computing services offered by third-party providers over the public Internet, making them available to anyone who wants to use or purchase them. They may be free or sold on-demand, allowing customers to pay only per usage for the CPU cycles, storage, or bandwidth they consume. Some public cloud services are AWS, GCP, Azure, IBM Cloud, Alibaba Cloud\u2026 Hybrid Cloud: A hybrid cloud is a computing environment that combines an on-premises data center (also called a private cloud) with a public cloud, allowing data and applications to be shared between them. Some public cloud providers are VMware, Microsoft Azure\u2026","title":"Types of cloud computing"},{"location":"weekly/Cloud-Technologies/intro-aws/#components-of-cloud-infrastructure","text":"Client-side Infrastructure: Here, users interact with the cloud through a front-end GUI (Graphical User Interface). Desktop, laptops, mobiles, tablets, and others are used on the client-side to access services. Application: The application may be any software or platform that a client wants to access. The operating system: The cloud operating system manages the operation, the virtual servers, execution and processes of virtual machines and virtual infrastructure, as well as the back-end hardware and software resources. Service: Cloud Services are offered in different models. These services are being provided based on the client\u2019s request. Some of the service models are seen below","title":"Components of cloud infrastructure"},{"location":"weekly/Cloud-Technologies/intro-aws/#service-models","text":"Cloud-Computing providers offer their \u201cservices\u201d according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). IAAS: Infrastructure As A Service (IAAS) is means of delivering computing infrastructure as on-demand services. It is one of the three fundamental cloud service model servers\u2019 storage network operating systems. In the user purchasing servers, software data center space, or network equipment and renting those resources as a fully outsourced service can demand model. It allows dynamic scaling, and the resources are distributed as a service. It generally includes multiple users on a single piece of hardware. PAAS: Platform as A Service (PAAS) is a cloud delivery model for applications composed of services managed by a third party. It provides elastic scaling of your application which allows developers to build applications and services over the internet and the deployment models include public, private and hybrid. SAAS: Software As A Service (SAAS) allows users to run existing online applications and it is a model software that is deployed as a hosting service and is accessed over Output Rephrased/Re-written Text the internet or software delivery model during which software and its associated data are hosted centrally and accessed using their client, usually an online browser over the web. SAAS services are used for the development and deployment of modern applications. The emergence of Cloud computing has given rise to more as-a-service offerings. Some of them include: - AIaaS: AI as a service - DaaS: Desktop as a service - ITaaS: IT as a service - RaaS: Ransomware as a service.","title":"Service Models"},{"location":"weekly/Cloud-Technologies/intro-aws/#top-benefits-of-cloud-computing","text":"High Speed: the ability to spin up new instances in a matter of seconds. Efficiency and Cost Reduction Provides efficient Data Security Scalable: ease with quickly\u200a\u2014\u200ascale up/down infrastructures. Collaboration: Cloud environments enable better collaboration across teams. Provides unlimited Storage Capacity. Provides ease in Back-up and Restoring of Data Reliability: Cloud computing makes data backup, disaster recovery, and business continuity easier and less expensive because data can be mirrored at multiple redundant sites on the cloud provider\u2019s network.","title":"Top benefits of cloud computing"},{"location":"weekly/Cloud-Technologies/intro-aws/#focus-aws-cloud","text":"What Is Amazon Web Services(AWS): Amazon Web Services(AWS) is Amazon\u2019s cloud computing platform that offers a mix of packaged platform as a service (PaaS), software as a service (SaaS), and infrastructure as a service (IaaS). ### EC2 Amazon EC2 is AWS primary web service that provides resizable compute capacity in the cloud. Instance types: Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. Example instance Family: c4 Compute optimized\u2014For workloads requiring significant processing r3 Memory optimized\u2014For memory-intensive workloads i2 Storage optimized\u2014For workloads requiring high amounts of fast SSD storage g2 GPU-based instances\u2014Intended for graphics and general-purpose GPU compute workloads Amazon Machine Images (AMIs): Defines the initial software that will be on an instance when it is launched. An AMI defines; The Operating System (OS) and its configuration, The initial state of any patches, Application or system software. AMI\u2019s can be AWS published, from the AWS Marketplace, Generated from Existing Instances, or Uploaded Virtual Servers Security Groups: This is a VPC concept. If an instance is running in an Amazon VPC, you can change which security groups are associated with an instance while the instance is running. EBS: Is type of \u201cblock storage\u201d volume offering by AWS. EBS provides persistent block-level storage volumes for use with Amazon EC2 instances. Instance Stores: Is another type of \u201cblock storage\u201d volume offering by AWS for your instance. This storage is located on disks that are physically attached to the host computer. ### S3 Amazon Simple Storage Service is a secure, durable, and highly-scalable cloud storage. Some use cases for cloud storage include: - Backup and archive for on-premises or cloud data. - Content, media, and software storage and distribution. - Big data analytics. - Static website hosting. - Cloud-native mobile and Internet application hosting. ### VPC The Amazon Virtual Private Cloud (Amazon VPC) is a custom-defined virtual network within the AWS Cloud. ![cloud1](photos/cloud5.png) VPC COMPONENTS: An Amazon VPC consists of the following main components: Subnets, Route tables, Dynamic Host Configuration Protocol (DHCP) option sets Security groups, Network Access Control Lists (ACLs). Also, optional components: Internet Gateways (IGWs), Elastic IP (EIP) addresses, Elastic Network Interfaces (ENIs), Endpoints, Peering, Network Address Translation (NATs) instances and NAT gateways, Virtual Private Gateway (VPG), Customer Gateways (CGWs), and Virtual Private Networks (VPNs) ***Subnets:*** A subnet is a segment of an Amazon VPC\u2019s IP address range. Subnets reside within one Availability Zone and cannot span zones; You can, however, have multiple subnets in one Availability Zone. Each subnet must be associated with a route table, which controls the routing for the subnet. ***Route Tables:*** A route table is a logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where network traffic is directed. ***Internet Gateways:*** IGW allows communication between instances in your Amazon VPC and the Internet. ***Dynamic Host Configuration*** Protocol (DHCP) Option Sets: provides a standard for passing configuration information to hosts on a TCP/IP network. AWS automatically creates and associates a DHCP option set for your Amazon VPC upon creation and sets two options: domain-name-servers (defaulted to AmazonProvidedDNS) and domain-name (defaulted to the domain name for your region). ### IAM With AWS IAM Identity Center (successor to AWS Single Sign-On), you can manage sign-in security for your workforce identities, also known as workforce users. IAM Identity Center provides one place where you can create or connect workforce users and centrally manage their access across all their AWS accounts and applications. For more on IAM chack the link: IAM ### AWS ELASTIC LOAD BALANCER (ELB) AWS has an Elastic Load Balancer that is used to distribute incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing supports routing and load balancing of Hypertext. Transfer Protocol (HTTP), Hypertext Transfer Protocol Secure (HTTPS), Transmission Control Protocol (TCP), and Secure Sockets Layer (SSL) traffic to Amazon EC2 instances. Types of AWS ELB: The two major Elastic load balancers are: ***Application Load Balancer*** Application layer that describes the use of HTTP and HTTPS connections from clients to the load balancer and from the load balancer to your back-end instance. ***Network Load Balancer*** Are used to route traffic through layer 4, based IPs and TCP or UDP ports. ### EKS Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. For more on EKS chack the link: [EKS]https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html","title":"FOCUS: AWS CLOUD"},{"location":"weekly/Getting-Started/local-install/","text":"Locally System Requirements Windows OS systems will be the preferred OS that will be used throughout this program. Students with MaC OS can still use their MAC pcs, but may have some diffuculties with running some Windows-specific commands. Install WSL Requirments: windows 10 or greater In the windows search, Type: 'feat' Click -> Turning windows features off or on, and check select [x] virtual machine platform [x] windows Subsystem for Linux Some common WSL commands, use PowerShell $ winver $ wsl -l -v $ wsl --set-version Ubuntu-20.04 2 $ wsl --set-default-version 2 See Youtube link: Click link Install Linux Kernel Step 4 - Download the Linux kernel update package Now go to searchBar and search store Install Chocolatey Open Powershell as Administrative user Link Click-Link Run Command: Get-ExecutionPolicy If it returns Restricted, then run: Set-ExecutionPolicy AllSigned Now run the following command: Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1')) Install AWSCli Run command: msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Install Choco Packages choco install kubectl choco install k9s choco install terraform choco install kubens kubectx Install Docker Desktop/k8s use link: Click link to install Restart System","title":"Locally"},{"location":"weekly/Getting-Started/local-install/#locally","text":"System Requirements Windows OS systems will be the preferred OS that will be used throughout this program. Students with MaC OS can still use their MAC pcs, but may have some diffuculties with running some Windows-specific commands.","title":"Locally"},{"location":"weekly/Getting-Started/local-install/#install-wsl","text":"Requirments: windows 10 or greater In the windows search, Type: 'feat' Click -> Turning windows features off or on, and check select [x] virtual machine platform [x] windows Subsystem for Linux Some common WSL commands, use PowerShell $ winver $ wsl -l -v $ wsl --set-version Ubuntu-20.04 2 $ wsl --set-default-version 2 See Youtube link: Click link Install Linux Kernel Step 4 - Download the Linux kernel update package Now go to searchBar and search store","title":"Install WSL"},{"location":"weekly/Getting-Started/local-install/#install-chocolatey","text":"Open Powershell as Administrative user Link Click-Link Run Command: Get-ExecutionPolicy If it returns Restricted, then run: Set-ExecutionPolicy AllSigned Now run the following command: Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))","title":"Install Chocolatey"},{"location":"weekly/Getting-Started/local-install/#install-awscli","text":"Run command: msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi","title":"Install AWSCli"},{"location":"weekly/Getting-Started/local-install/#install-choco-packages","text":"choco install kubectl choco install k9s choco install terraform choco install kubens kubectx","title":"Install Choco Packages"},{"location":"weekly/Getting-Started/local-install/#install-docker-desktopk8s","text":"use link: Click link to install Restart System","title":"Install Docker Desktop/k8s"},{"location":"weekly/Getting-Started/remote-install/","text":"Remote Server ## Installation links Install IAM EKS authenticator Right-Click to open Link in a New Tab curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator sudo mv ./aws-iam-authenticator /usr/local/bin aws-iam-authenticator help Install docker in ec2 Right-Click to open Link in a New Tab Install AWSCLI Right-Click to open Link in a New Tab Install Terraform Right-Click to open Link in a New Tab Install Kubens + kubectx Right-Click to open Link in a New Tab Install HelM Right-Click to open Link in a New Tab curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Install Kubectl Right-Click to open Link in a New Tab curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(<kubectl.sha256) kubectl\" | sha256sum --check sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --client --output=yaml Install MkDocs Right-Click to open Link in a New Tab pip install mkdocs mkdocs helm mkdocs new my-soso-package cd my-soso-package tree Install PiP on RHeL REDHAT sudo yum info python*-pip //get the pip version, then install the version sudo yum install python39-pip python3 --version curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user Install Trivy Right-Click to open Link in a New Tab sudo yum -y update sudo wget https://github.com/aquasecurity/trivy/releases/download/v0.18.3/trivy_0.18.3_Linux-64bit.deb sudo dpkg -i trivy_0.18.3_Linux-64bit.deb trivy i nginx //scanning nginx image trivy i nginx | grep -i critical trivy i nginx:alpine | grep -i critical","title":"Remote Server"},{"location":"weekly/Getting-Started/remote-install/#remote-server","text":"## Installation links","title":"Remote Server"},{"location":"weekly/Getting-Started/remote-install/#install-iam-eks-authenticator","text":"Right-Click to open Link in a New Tab curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator sudo mv ./aws-iam-authenticator /usr/local/bin aws-iam-authenticator help","title":"Install IAM EKS authenticator"},{"location":"weekly/Getting-Started/remote-install/#install-docker-in-ec2","text":"Right-Click to open Link in a New Tab","title":"Install docker in ec2"},{"location":"weekly/Getting-Started/remote-install/#install-awscli","text":"Right-Click to open Link in a New Tab","title":"Install AWSCLI"},{"location":"weekly/Getting-Started/remote-install/#install-terraform","text":"Right-Click to open Link in a New Tab","title":"Install Terraform"},{"location":"weekly/Getting-Started/remote-install/#install-kubens-kubectx","text":"Right-Click to open Link in a New Tab","title":"Install Kubens + kubectx"},{"location":"weekly/Getting-Started/remote-install/#install-helm","text":"Right-Click to open Link in a New Tab curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh","title":"Install HelM"},{"location":"weekly/Getting-Started/remote-install/#install-kubectl","text":"Right-Click to open Link in a New Tab curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(<kubectl.sha256) kubectl\" | sha256sum --check sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --client --output=yaml","title":"Install Kubectl"},{"location":"weekly/Getting-Started/remote-install/#install-mkdocs","text":"Right-Click to open Link in a New Tab pip install mkdocs mkdocs helm mkdocs new my-soso-package cd my-soso-package tree","title":"Install MkDocs"},{"location":"weekly/Getting-Started/remote-install/#install-pip-on-rhel","text":"REDHAT sudo yum info python*-pip //get the pip version, then install the version sudo yum install python39-pip python3 --version curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user","title":"Install PiP on RHeL"},{"location":"weekly/Getting-Started/remote-install/#install-trivy","text":"Right-Click to open Link in a New Tab sudo yum -y update sudo wget https://github.com/aquasecurity/trivy/releases/download/v0.18.3/trivy_0.18.3_Linux-64bit.deb sudo dpkg -i trivy_0.18.3_Linux-64bit.deb trivy i nginx //scanning nginx image trivy i nginx | grep -i critical trivy i nginx:alpine | grep -i critical","title":"Install Trivy"},{"location":"weekly/week2/elk/","text":"ELK - Elastic Stack ELK Stack is the world\u2019s most popular log management platform. The ELK stack is an acronym used to describe a stack that comprises of three popular projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a distributed search and analytics engine built on Apache Lucene Logstash is used for both shipping as well as processing and storing logs Kibana is a visualization tool which is hosted through Nginx or Apache Some popular Use Cases for ELK are: Logging, Matrics, Security Analysis, Business Analysis - Logging - logging and analysing Data, like gaming data - Matrics - like NASA Rover can sent telematrics, sensor data - Security Analysis - Helps with securing comunication in Apps like Slack - Business Analysis - Helps with matching people ElasticSearch With ElasticSearch, User can sends search quary to your website and your server send those requests to ElasticSearch, Elasticsearch then processes this request and Responds back to the user, through the Server. Elasticsearch Architecture: Key Components Elasticsearch Cluster is composed of a group of nodes that store data. Elasticsearch Node is an instance that runs in a cluster. Multiple nodes can be run on a single machine. When Elastic is running, you will have an instance of elastic known as Node. Each Node Has a unique ID and a node. Each Node belongs to a single cluster. Data is stored as documents in Elasticsearch. Indices are used to group documents that are related to each other. When an Index is created, it comes with a Shard, by default. There are the three main options to configure an Elasticsearch node: Elasticsearch master node Elasticsearch data node Elasticsearch client node Search movement - client --> servers --> ElasticSearch Shards is where data is stored. What you create an Index, you can create multiple shards that are distributed across nodes. Each shard has its replica to recover data in case of node failure. The primary shard and replica shard is always placed on different nodes. Horizontal scaling or scale-out is the main reason to shard a database. The number of shards depends heavily on the amount of data we have. Shard Example use Case 1 A BCBSNC Elastic engineer has 900k documents that they want to store. Each Shard can only Hold a maximum of 300k Documents Two more shards of 300GB each will be added bringing the total storage capacity to 900K Documents Shard Example use Case 2 Scenario 1: An Elastic engineer searches 500k documents that is stored in a single Shard The response time for searching 500k Documents in a single shard is 10 seconds. Scenario 2: An Elastic engineer runs a parallel searches of 500k documents that are distributed in 5 Shard. The response time for searching 100k Documents/Shard will be 2 seconds. So, the response time for searching all 500k Documents that are distributed amongst the 5 shard will still be 2 seconds. Kibana Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. Kibana Aggregations There are two main types of aggregations Metric aggregations are used to calculate a value for each bucket based on the documents inside the bucket. Bucket aggregations are used groups documents together in one bucket according to your logic and requirements, while the Metric aggregations For More information on Kibana,: Click link Adding data into Elasticsearch The main consideration for indexing or adding data into Elasticsearch largely depends on whether you are indexing general content or timestamped data. General content: Index content like HTML pages, catalogs and other files. Send data directly to Elasticseach from your application using an Elastic language client. Otherwise use Workplace Search content connectors or the Enterprise Search web crawler. Timestamped data: The preferred way to index timestamped data is to use Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. How to ingest data into Elasticsearch Service There are 4 main ways in Adding Data to ElasticSearch Service: By using Beats and Logstash By using Kibana By programmatically indexing data Manually adding data Read more about data ingestion in ElasticSearch Click Link The index The index is the basis of Elasticsearch, it is where you store your documents. You can compare it to a table in a database. An index has settings and a mapping which defines how to store and index the documents. Mapping Mapping is the process of defining how a document, and the fields it contains, are stored and indexed. Each document is a collection of fields, which each have their own data type. When mapping your data, you create a mapping definition, which contains a list of fields that are pertinent to the document. What is Elastic integrations Elastic integrations are a streamlined way to connect your data to the Elastic Stack. Integrations are available for popular services and platforms, like Nginx, AWS, and MongoDB, as well as many generic input types like log files. For more on Data Integration Click link: Elastic Agent Elastic integrations are powered by Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. It can also protect hosts from security threats, query data from operating systems, forward data from remote services or hardware, and more. Sample Hands-on I will : Use curl commands to talk to the Elasticsearch REST API. Or if you choose to use Kibana Devtools, thats also ok, because Kibana\u2019s Devtools can automatically recognizes and converts curl commands . create an index Called middleware curl -X PUT \u201clocalhost:9200/middleware Delete the index Called middleware curl -X DELETE \u201clocalhost:9200/middleware\u201d Create an index Called middleware with just one shard and no replicas PUT /middleware?pretty { \"settings\" : { \"number_of_shards\" : 1, \"number_of_replicas\" : 0 } } loading data into that index Called middleware PUT /middleware/_mapping { \"properties\": { \"post_time\": { \"type\": \"date\" }, \"username\": { \"type\": \"keyword\" }, \"message\": { \"type\": \"text\" } } } Now Load some data Manually POST /middleware/_doc/?pretty { \"user\" : \"collins\", \"post_date\" : \"2023-16-03T14:10:10\", \"message\" : \"Welcome to Elasticsearch?\" } see this mediam link, its cool Click-Link Shell Scripting A shell in a Linux operating system takes input from you in the form of commands, processes it, and then gives an output. ## Types of Shell There are four types, but two main shells in Linux: The Bourne Shell: The prompt for this shell is $ and its derivatives are listed below: POSIX shell also is known as sh Korn Shell also knew as sh Bourne Again SHell also knew as bash (most popular) The C shell: The prompt for this shell is %, and its subcategories are: C shell also is known as csh Tops C shell also is known as tcsh ### Hands-On Create script called: sososcript.sh $ nano sososcript.sh $ chmod +x sososcript.sh In the script, add the following content: #!/bin/bash echo \u201cWelcome to soso Devops master course sosoengineers\u201d sleep 2 echo \u201cyou\u2019re all welcome sosoengineers\u201d sleep 2 echo \u201chappy learning sosoengineers\u201d run the script: $ sh sososcript.sh EX 2: #!/bin/bash name=\"sosoengineers\" #this name is a variable echo \u201cWelcome to soso Devops master course $name\u201d sleep 2 echo \u201cyou\u2019re all welcome to $name\u201d sleep 2 echo \u201chappy learning at $name\u201d run the script: $ sh sososcript.sh ### VARIABLES A variable is a temporary store for a piece of information. There are two actions for variables: Setting a value for a variable. Reading the value for a variable. Variables are defined as follows \u2212 [variable_name=variable_value] EX: NAME=\"soso technologies\" Variable Types: When a shell is running, three main types of variables are present Local Variables are present within the current instance of the shell. They are set at the command prompt. Environment Variables are available to any child process of the shell. Shell Variables is a special variable that is set by the shell and is required by the shell to function correctly. EX 1: set the name soso engineers as a variable[read] ```sh #!/bin/bash echo \"what is the schools name\" read name echo \u201cWelcome to soso Devops master course $name\u201d sleep 2 echo \u201cyou\u2019re all welcome to $name\u201d sleep 2 echo \u201chappy learning at $name\u201d ***EX 2:*** set the name soso engineers as a variable[read] #!/bin/sh echo \"what is your study program?\" read name echo \"How is $name going so far?\" read remark echo \"It\u2019s also going $remark with me too!\" ***EX 3:*** set the name soso engineers as a variable[positional parameter-argument] #!/bin/bash name=$1 echo \u201cWelcome to soso Devops master course $name\u201d sleep 2 echo \u201cyou\u2019re all welcome to sosotech $name\u201d sleep 2 echo \u201chappy learning $name\u201d ***EX 4:*** set the name soso engineers multi variables #!/bin/bash sosouser=$(whoami) sosodate=$(date) serverconfig=$(ifconfig) sosodir=$(pwd) echo \"$sosouser $sosodate $serverconfig $sosodir\" ``` 3 Tier Application A 3-tier application will compose of 3 layers: a Presentation tier, an Application tier, and a Data tier. Benefits of a 3 tier Application? The benefits of using a 3-tier architecture include improved horizontal scalability, performance, and availability. With three tiers, each part can be developed concurrently by a different team of programmers coding in different languages from the other tier developers. Because the programming for a tier can be changed or relocated without affecting the other tiers, the 3-tier model makes it easier for an enterprise or software packager to continually evolve an application as new needs and opportunities arise. Existing applications or critical parts can be permanently or temporarily retained and encapsulated within the new tier of which it becomes a component. The 3 different layers explained","title":"ELK"},{"location":"weekly/week2/elk/#elk-elastic-stack","text":"ELK Stack is the world\u2019s most popular log management platform. The ELK stack is an acronym used to describe a stack that comprises of three popular projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a distributed search and analytics engine built on Apache Lucene Logstash is used for both shipping as well as processing and storing logs Kibana is a visualization tool which is hosted through Nginx or Apache Some popular Use Cases for ELK are: Logging, Matrics, Security Analysis, Business Analysis - Logging - logging and analysing Data, like gaming data - Matrics - like NASA Rover can sent telematrics, sensor data - Security Analysis - Helps with securing comunication in Apps like Slack - Business Analysis - Helps with matching people","title":"ELK - Elastic Stack"},{"location":"weekly/week2/elk/#elasticsearch","text":"With ElasticSearch, User can sends search quary to your website and your server send those requests to ElasticSearch, Elasticsearch then processes this request and Responds back to the user, through the Server.","title":"ElasticSearch"},{"location":"weekly/week2/elk/#elasticsearch-architecture-key-components","text":"Elasticsearch Cluster is composed of a group of nodes that store data. Elasticsearch Node is an instance that runs in a cluster. Multiple nodes can be run on a single machine. When Elastic is running, you will have an instance of elastic known as Node. Each Node Has a unique ID and a node. Each Node belongs to a single cluster. Data is stored as documents in Elasticsearch. Indices are used to group documents that are related to each other. When an Index is created, it comes with a Shard, by default. There are the three main options to configure an Elasticsearch node: Elasticsearch master node Elasticsearch data node Elasticsearch client node Search movement - client --> servers --> ElasticSearch Shards is where data is stored. What you create an Index, you can create multiple shards that are distributed across nodes. Each shard has its replica to recover data in case of node failure. The primary shard and replica shard is always placed on different nodes. Horizontal scaling or scale-out is the main reason to shard a database. The number of shards depends heavily on the amount of data we have. Shard Example use Case 1 A BCBSNC Elastic engineer has 900k documents that they want to store. Each Shard can only Hold a maximum of 300k Documents Two more shards of 300GB each will be added bringing the total storage capacity to 900K Documents Shard Example use Case 2 Scenario 1: An Elastic engineer searches 500k documents that is stored in a single Shard The response time for searching 500k Documents in a single shard is 10 seconds. Scenario 2: An Elastic engineer runs a parallel searches of 500k documents that are distributed in 5 Shard. The response time for searching 100k Documents/Shard will be 2 seconds. So, the response time for searching all 500k Documents that are distributed amongst the 5 shard will still be 2 seconds.","title":"Elasticsearch Architecture: Key Components"},{"location":"weekly/week2/elk/#kibana","text":"Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. Kibana Aggregations There are two main types of aggregations Metric aggregations are used to calculate a value for each bucket based on the documents inside the bucket. Bucket aggregations are used groups documents together in one bucket according to your logic and requirements, while the Metric aggregations For More information on Kibana,: Click link","title":"Kibana"},{"location":"weekly/week2/elk/#adding-data-into-elasticsearch","text":"The main consideration for indexing or adding data into Elasticsearch largely depends on whether you are indexing general content or timestamped data. General content: Index content like HTML pages, catalogs and other files. Send data directly to Elasticseach from your application using an Elastic language client. Otherwise use Workplace Search content connectors or the Enterprise Search web crawler. Timestamped data: The preferred way to index timestamped data is to use Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host.","title":"Adding data into Elasticsearch"},{"location":"weekly/week2/elk/#how-to-ingest-data-into-elasticsearch-service","text":"There are 4 main ways in Adding Data to ElasticSearch Service: By using Beats and Logstash By using Kibana By programmatically indexing data Manually adding data Read more about data ingestion in ElasticSearch Click Link","title":"How to ingest data into Elasticsearch Service"},{"location":"weekly/week2/elk/#the-index","text":"The index is the basis of Elasticsearch, it is where you store your documents. You can compare it to a table in a database. An index has settings and a mapping which defines how to store and index the documents.","title":"The index"},{"location":"weekly/week2/elk/#mapping","text":"Mapping is the process of defining how a document, and the fields it contains, are stored and indexed. Each document is a collection of fields, which each have their own data type. When mapping your data, you create a mapping definition, which contains a list of fields that are pertinent to the document.","title":"Mapping"},{"location":"weekly/week2/elk/#what-is-elastic-integrations","text":"Elastic integrations are a streamlined way to connect your data to the Elastic Stack. Integrations are available for popular services and platforms, like Nginx, AWS, and MongoDB, as well as many generic input types like log files. For more on Data Integration Click link:","title":"What is Elastic integrations"},{"location":"weekly/week2/elk/#elastic-agent","text":"Elastic integrations are powered by Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. It can also protect hosts from security threats, query data from operating systems, forward data from remote services or hardware, and more.","title":"Elastic Agent"},{"location":"weekly/week2/elk/#sample-hands-on","text":"I will : Use curl commands to talk to the Elasticsearch REST API. Or if you choose to use Kibana Devtools, thats also ok, because Kibana\u2019s Devtools can automatically recognizes and converts curl commands . create an index Called middleware curl -X PUT \u201clocalhost:9200/middleware Delete the index Called middleware curl -X DELETE \u201clocalhost:9200/middleware\u201d Create an index Called middleware with just one shard and no replicas PUT /middleware?pretty { \"settings\" : { \"number_of_shards\" : 1, \"number_of_replicas\" : 0 } } loading data into that index Called middleware PUT /middleware/_mapping { \"properties\": { \"post_time\": { \"type\": \"date\" }, \"username\": { \"type\": \"keyword\" }, \"message\": { \"type\": \"text\" } } } Now Load some data Manually POST /middleware/_doc/?pretty { \"user\" : \"collins\", \"post_date\" : \"2023-16-03T14:10:10\", \"message\" : \"Welcome to Elasticsearch?\" } see this mediam link, its cool Click-Link","title":"Sample Hands-on"},{"location":"weekly/week2/elk/#shell-scripting","text":"A shell in a Linux operating system takes input from you in the form of commands, processes it, and then gives an output. ## Types of Shell There are four types, but two main shells in Linux: The Bourne Shell: The prompt for this shell is $ and its derivatives are listed below: POSIX shell also is known as sh Korn Shell also knew as sh Bourne Again SHell also knew as bash (most popular) The C shell: The prompt for this shell is %, and its subcategories are: C shell also is known as csh Tops C shell also is known as tcsh ### Hands-On Create script called: sososcript.sh $ nano sososcript.sh $ chmod +x sososcript.sh In the script, add the following content: #!/bin/bash echo \u201cWelcome to soso Devops master course sosoengineers\u201d sleep 2 echo \u201cyou\u2019re all welcome sosoengineers\u201d sleep 2 echo \u201chappy learning sosoengineers\u201d run the script: $ sh sososcript.sh EX 2: #!/bin/bash name=\"sosoengineers\" #this name is a variable echo \u201cWelcome to soso Devops master course $name\u201d sleep 2 echo \u201cyou\u2019re all welcome to $name\u201d sleep 2 echo \u201chappy learning at $name\u201d run the script: $ sh sososcript.sh ### VARIABLES A variable is a temporary store for a piece of information. There are two actions for variables: Setting a value for a variable. Reading the value for a variable. Variables are defined as follows \u2212 [variable_name=variable_value] EX: NAME=\"soso technologies\" Variable Types: When a shell is running, three main types of variables are present Local Variables are present within the current instance of the shell. They are set at the command prompt. Environment Variables are available to any child process of the shell. Shell Variables is a special variable that is set by the shell and is required by the shell to function correctly. EX 1: set the name soso engineers as a variable[read] ```sh #!/bin/bash echo \"what is the schools name\" read name echo \u201cWelcome to soso Devops master course $name\u201d sleep 2 echo \u201cyou\u2019re all welcome to $name\u201d sleep 2 echo \u201chappy learning at $name\u201d ***EX 2:*** set the name soso engineers as a variable[read] #!/bin/sh echo \"what is your study program?\" read name echo \"How is $name going so far?\" read remark echo \"It\u2019s also going $remark with me too!\" ***EX 3:*** set the name soso engineers as a variable[positional parameter-argument] #!/bin/bash name=$1 echo \u201cWelcome to soso Devops master course $name\u201d sleep 2 echo \u201cyou\u2019re all welcome to sosotech $name\u201d sleep 2 echo \u201chappy learning $name\u201d ***EX 4:*** set the name soso engineers multi variables #!/bin/bash sosouser=$(whoami) sosodate=$(date) serverconfig=$(ifconfig) sosodir=$(pwd) echo \"$sosouser $sosodate $serverconfig $sosodir\" ```","title":"Shell Scripting"},{"location":"weekly/week2/elk/#3-tier-application","text":"A 3-tier application will compose of 3 layers: a Presentation tier, an Application tier, and a Data tier.","title":"3 Tier Application"},{"location":"weekly/week2/elk/#benefits-of-a-3-tier-application","text":"The benefits of using a 3-tier architecture include improved horizontal scalability, performance, and availability. With three tiers, each part can be developed concurrently by a different team of programmers coding in different languages from the other tier developers. Because the programming for a tier can be changed or relocated without affecting the other tiers, the 3-tier model makes it easier for an enterprise or software packager to continually evolve an application as new needs and opportunities arise. Existing applications or critical parts can be permanently or temporarily retained and encapsulated within the new tier of which it becomes a component.","title":"Benefits of a 3 tier Application?"},{"location":"weekly/week2/elk/#the-3-different-layers-explained","text":"","title":"The 3 different layers explained"}]}