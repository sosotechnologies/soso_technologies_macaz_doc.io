{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])"},"docs":[{"location":"","text":"Welcome To SosoTech We specialize in IT training and Hands-on. What we do We train New and Working IT Professionals on the following fields: DevOps SRE Cloud Engineers AI Engineers","title":"About Us"},{"location":"#welcome-to-sosotech","text":"We specialize in IT training and Hands-on.","title":"Welcome To SosoTech"},{"location":"#what-we-do","text":"We train New and Working IT Professionals on the following fields: DevOps SRE Cloud Engineers AI Engineers","title":"What we do"},{"location":"Getting-Started/local-install/","text":"Locally System Requirements Windows OS systems will be the preferred OS that will be used throughout this program. Students with MaC OS can still use their MAC pcs, but may have some diffuculties with running some Windows-specific commands. Install WSL Requirments: windows 10 or greater In the windows search, Type: 'feat' Click -> Turning windows features off or on, and check select [x] virtual machine platform [x] windows Subsystem for Linux Some common WSL commands, use PowerShell $ winver $ wsl -l -v $ wsl --set-version Ubuntu-20.04 2 $ wsl --set-default-version 2 See Youtube link: Click link Install Linux Kernel Step 4 - Download the Linux kernel update package Now go to searchBar and search store Install Chocolatey Open Powershell as Administrative user Link Click-Link Run Command: Get-ExecutionPolicy If it returns Restricted, then run: Set-ExecutionPolicy AllSigned Now run the following command: Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1')) Install AWSCli Run command: msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Install Choco Packages choco install kubectl choco install k9s choco install terraform choco install kubens kubectx Install Docker Desktop/k8s use link: Click link to install Restart System","title":"Local-Installation"},{"location":"Getting-Started/local-install/#locally","text":"System Requirements Windows OS systems will be the preferred OS that will be used throughout this program. Students with MaC OS can still use their MAC pcs, but may have some diffuculties with running some Windows-specific commands.","title":"Locally"},{"location":"Getting-Started/local-install/#install-wsl","text":"Requirments: windows 10 or greater In the windows search, Type: 'feat' Click -> Turning windows features off or on, and check select [x] virtual machine platform [x] windows Subsystem for Linux Some common WSL commands, use PowerShell $ winver $ wsl -l -v $ wsl --set-version Ubuntu-20.04 2 $ wsl --set-default-version 2 See Youtube link: Click link Install Linux Kernel Step 4 - Download the Linux kernel update package Now go to searchBar and search store","title":"Install WSL"},{"location":"Getting-Started/local-install/#install-chocolatey","text":"Open Powershell as Administrative user Link Click-Link Run Command: Get-ExecutionPolicy If it returns Restricted, then run: Set-ExecutionPolicy AllSigned Now run the following command: Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))","title":"Install Chocolatey"},{"location":"Getting-Started/local-install/#install-awscli","text":"Run command: msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi","title":"Install AWSCli"},{"location":"Getting-Started/local-install/#install-choco-packages","text":"choco install kubectl choco install k9s choco install terraform choco install kubens kubectx","title":"Install Choco Packages"},{"location":"Getting-Started/local-install/#install-docker-desktopk8s","text":"use link: Click link to install Restart System","title":"Install Docker Desktop/k8s"},{"location":"Getting-Started/remote-install/","text":"Remote Server Installation links Install IAM EKS authenticator Right-Click to open Link in a New Tab curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator sudo mv ./aws-iam-authenticator /usr/local/bin aws-iam-authenticator help Install docker in ec2 Right-Click to open Link in a New Tab Install AWSCLI Right-Click to open Link in a New Tab Install Terraform Right-Click to open Link in a New Tab Install Kubens + kubectx Right-Click to open Link in a New Tab Install HelM Right-Click to open Link in a New Tab curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Install Kubectl Right-Click to open Link in a New Tab curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(<kubectl.sha256) kubectl\" | sha256sum --check sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --client --output=yaml Install MkDocs Right-Click to open Link in a New Tab pip install mkdocs mkdocs helm mkdocs new my-soso-package cd my-soso-package tree Install PiP on RHeL REDHAT sudo yum info python*-pip //get the pip version, then install the version sudo yum install python39-pip python3 --version curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user Install Trivy Right-Click to open Link in a New Tab sudo yum -y update sudo wget https://github.com/aquasecurity/trivy/releases/download/v0.18.3/trivy_0.18.3_Linux-64bit.deb sudo dpkg -i trivy_0.18.3_Linux-64bit.deb trivy i nginx //scanning nginx image trivy i nginx | grep -i critical trivy i nginx:alpine | grep -i critical","title":"Remote Installation"},{"location":"Getting-Started/remote-install/#remote-server","text":"","title":"Remote Server"},{"location":"Getting-Started/remote-install/#installation-links","text":"","title":"Installation links"},{"location":"Getting-Started/remote-install/#install-iam-eks-authenticator","text":"Right-Click to open Link in a New Tab curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator sudo mv ./aws-iam-authenticator /usr/local/bin aws-iam-authenticator help","title":"Install IAM EKS authenticator"},{"location":"Getting-Started/remote-install/#install-docker-in-ec2","text":"Right-Click to open Link in a New Tab","title":"Install docker in ec2"},{"location":"Getting-Started/remote-install/#install-awscli","text":"Right-Click to open Link in a New Tab","title":"Install AWSCLI"},{"location":"Getting-Started/remote-install/#install-terraform","text":"Right-Click to open Link in a New Tab","title":"Install Terraform"},{"location":"Getting-Started/remote-install/#install-kubens-kubectx","text":"Right-Click to open Link in a New Tab","title":"Install Kubens + kubectx"},{"location":"Getting-Started/remote-install/#install-helm","text":"Right-Click to open Link in a New Tab curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh","title":"Install HelM"},{"location":"Getting-Started/remote-install/#install-kubectl","text":"Right-Click to open Link in a New Tab curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" curl -LO \"https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\" echo \"$(<kubectl.sha256) kubectl\" | sha256sum --check sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubectl version --client --output=yaml","title":"Install Kubectl"},{"location":"Getting-Started/remote-install/#install-mkdocs","text":"Right-Click to open Link in a New Tab pip install mkdocs mkdocs helm mkdocs new my-soso-package cd my-soso-package tree","title":"Install MkDocs"},{"location":"Getting-Started/remote-install/#install-pip-on-rhel","text":"REDHAT sudo yum info python*-pip //get the pip version, then install the version sudo yum install python39-pip python3 --version curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user","title":"Install PiP on RHeL"},{"location":"Getting-Started/remote-install/#install-trivy","text":"Right-Click to open Link in a New Tab sudo yum -y update sudo wget https://github.com/aquasecurity/trivy/releases/download/v0.18.3/trivy_0.18.3_Linux-64bit.deb sudo dpkg -i trivy_0.18.3_Linux-64bit.deb trivy i nginx //scanning nginx image trivy i nginx | grep -i critical trivy i nginx:alpine | grep -i critical","title":"Install Trivy"},{"location":"weekly/AI/ai/","text":"Artificial Intelligence Eventhough AI Machines seem to be a threat to humanity, yet, they cannot beat the power of human intelligence, Machines will never beat the creative or imaginative ability of humans. Always remember, humans created the machine. What is AI? \"It is a branch of computer science by which we can create intelligent machines which can behave like a human, think like humans, and able to make decisions.\" setup google codeLab Open-Link Theres always been this lask of understanding between machine learning and artificial intelligence. Put in context, artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience. Artificial Neural Network (ANN) Artificial Neural Networks the most important part of the deep learning model, they are the brain inspired deep learning tool which replicate the way humans learn. The Neuron In the Human Body, the Neuron is a part of the brain and nervous system. The three broad functions of the CNS are: To take in sensory information process information, and To send out motor signals. But Now How do Neurons Connect? Neurons communicate with each other by sending chemicals, called neurotransmitters, across a tiny space called a synapse, between the axons and dendrites of nearby neurons. So, now that we have that understanding of a human neuron, and how the neurons connect, the goal of AI was to replicate the way the human brain learns. In another sence, create machines that would have the ability to learn, adapt, and make decisions. Neurons in AI As seen in the diagram below, the Input Values are independent Variables of [1, 2, and m]. These variables need to be standardized to make sure all variables contribute evenly to a scale when items are added together, or to make it easier to interpret results of a regression or other analysis. The output Value can be: continuous, binary or a catagorical variable. Neural network is made of interconnected neurons. Each of them is characterized by its weight, bias, and activation function. Weight Weight is the parameter within a neural network that transforms input data within the network's hidden layers. Bias Bias in AI is when the machine gives consistently different outputs for one group of people compared to another. Typically these bias outputs follow classical societal biases like race, gender, biological sex, nationality or age. Neural Network Activation Function? An Activation Function is what makes the decision, if a neuron should be activated or not. This means that an ctivation Function is a mathematical \u201cgate\u201d that decide whether the neuron\u2019s input to the network is important or not in the process of prediction using simpler mathematical operations. Types of Activation Functions Threshold Activation Function is a commonly used activation function.It gives 1 as output of the input is either 0 or positive. If the input is negative, it gives 0 as output. Sigmoid Activation Function Sigmoid Function is the most commonly used activation function in neural networks. It is the go-to model when it comes to predicting the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid has the ability to takes any real value as input and outputs values in the range of 0 to 1. Rectifier Activation Function is the most popularly used activation function in the areas of convolutional neural networks and deep learning. It is of the form: Hyperbolic Tangent Activation Function It is bipolar in nature. Similar to sigmoid, except a different output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0. How do Neural Networks Work? Example Case: Real Estate Price prediction using a trained NN. Looking at the Red arrows, this synapses connects Age, Area, bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Newer, That have at least 3 bedrooms or more, And which the homes have a larger area. Looking at the Blue arrows, this synapses connects Schools Ratings and bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Families with Kids in school Homes that have more bedrooms for the parents and kids. Looking at the Green arrows, this synapses connects Just the Age of the property. So, this neuron is paying attention to properties that maybe: Newly built, or Properties that are historic, valuable properties, like above 100years old. A rectifier function can be used here, stating that, the moment the property turns 100 years, the rectifier functions kicks from 0 to 1. Looking at the Yellow arrows, this synapses connects Area (SqFt) and Bedrooms of the property. So, this neuron is paying attention to properties that maybe: The homes here have a larger Area Sq and have many rooms. These may be homes that are mansions. Looking at the black arrows, this synapses connects all the input functions. So, this neuron is paying attention to properties because all the inputs match the criteria for the price of the property So, all these different neuron communications will predict the price of the property How do Neural Networks Learn? ANN can be categorized by how the data flows from the input node to the output node. B Some of the neural networks are some examples: Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Backpropagation algorithm Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. In simple terms, you can think of the data flowing from the input node to the output node through many different paths in the neural network. Only one path is the correct one that maps the input node to the correct output node. To find this path, the neural network uses a feedback loop, which works as follows: Each node makes a guess about the next node in the path. It checks if the guess was correct. Nodes assign higher weight values to paths that lead to more correct guesses and lower weight values to node paths that lead to incorrect guesses. For the next data point, the nodes make a new prediction using the higher weight paths and then repeat Step 1. Convolutional neural networks The hidden layers in convolutional neural networks perform specific mathematical functions, like summarizing or filtering, called convolutions. They are very useful for image classification because they can extract relevant features from images that are useful for image recognition and classification. The new form is easier to process without losing features that are critical for making a good prediction. Each hidden layer extracts and processes different image features, like edges, color, and depth. Machine Learning in ANNs These ML algorithms help to solve different business problems like Forecasting, Regression, Clustering, Classification, and Associations, etc. Machine learning is divided into mainly four types, which are: Supervised Machine Learning - Ex: Housing Price Prediction. A Supervised machine learning is a technique where we train the machines using the \"labelled\" dataset, and based on the training, the machine predicts the output. In this type of learning, the labelled data specifies that some of the inputs are already mapped to the output. The machine are First trained with the input and corresponding output, and then we ask the machine to predict the output using the test dataset. A classic example will be the example of training a machine with Unsupervised Machine Learning EX: Customer Segmentation and Market Basket Analysis. In unsupervised machine learning, the machine is trained using the unlabeled dataset, and the machine predicts the output without any supervision. In this type, the models are trained with the data that is neither labelled nor classified, and the model acts on that data without any supervision. Unsupervised learning is much similar as a human learns, which makes it closer to the real AI. Semi-Supervised Machine Learning Ex: Lane-Finding on GPS data. It represents the intermediate ground between Supervised and Unsupervised learning algorithms and uses the combination of labelled and unlabeled datasets during the training period. Reinforcement Learning Ex: Drivless Cars. Reinforcement learning works on a feedback-based process, in which an AI agent automatically explore its surrounding by hitting & trail, taking action, learning from experiences, and improving its performance. Agent gets rewarded for each good action and get punished for each bad action; hence the goal of reinforcement learning agent is to maximize the rewards. In reinforcement learning, there is no labelled data like supervised learning, and agents learn from their experiences only. For more on this section, see this javatpoint Link: Click-Link","title":"AI"},{"location":"weekly/AI/ai/#artificial-intelligence","text":"Eventhough AI Machines seem to be a threat to humanity, yet, they cannot beat the power of human intelligence, Machines will never beat the creative or imaginative ability of humans. Always remember, humans created the machine.","title":"Artificial Intelligence"},{"location":"weekly/AI/ai/#what-is-ai","text":"\"It is a branch of computer science by which we can create intelligent machines which can behave like a human, think like humans, and able to make decisions.\" setup google codeLab Open-Link Theres always been this lask of understanding between machine learning and artificial intelligence. Put in context, artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience.","title":"What is AI?"},{"location":"weekly/AI/ai/#artificial-neural-network-ann","text":"Artificial Neural Networks the most important part of the deep learning model, they are the brain inspired deep learning tool which replicate the way humans learn. The Neuron In the Human Body, the Neuron is a part of the brain and nervous system. The three broad functions of the CNS are: To take in sensory information process information, and To send out motor signals. But Now How do Neurons Connect? Neurons communicate with each other by sending chemicals, called neurotransmitters, across a tiny space called a synapse, between the axons and dendrites of nearby neurons. So, now that we have that understanding of a human neuron, and how the neurons connect, the goal of AI was to replicate the way the human brain learns. In another sence, create machines that would have the ability to learn, adapt, and make decisions.","title":"Artificial Neural Network (ANN)"},{"location":"weekly/AI/ai/#neurons-in-ai","text":"As seen in the diagram below, the Input Values are independent Variables of [1, 2, and m]. These variables need to be standardized to make sure all variables contribute evenly to a scale when items are added together, or to make it easier to interpret results of a regression or other analysis. The output Value can be: continuous, binary or a catagorical variable. Neural network is made of interconnected neurons. Each of them is characterized by its weight, bias, and activation function.","title":"Neurons in AI"},{"location":"weekly/AI/ai/#weight","text":"Weight is the parameter within a neural network that transforms input data within the network's hidden layers.","title":"Weight"},{"location":"weekly/AI/ai/#bias","text":"Bias in AI is when the machine gives consistently different outputs for one group of people compared to another. Typically these bias outputs follow classical societal biases like race, gender, biological sex, nationality or age.","title":"Bias"},{"location":"weekly/AI/ai/#neural-network-activation-function","text":"An Activation Function is what makes the decision, if a neuron should be activated or not. This means that an ctivation Function is a mathematical \u201cgate\u201d that decide whether the neuron\u2019s input to the network is important or not in the process of prediction using simpler mathematical operations. Types of Activation Functions Threshold Activation Function is a commonly used activation function.It gives 1 as output of the input is either 0 or positive. If the input is negative, it gives 0 as output. Sigmoid Activation Function Sigmoid Function is the most commonly used activation function in neural networks. It is the go-to model when it comes to predicting the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid has the ability to takes any real value as input and outputs values in the range of 0 to 1. Rectifier Activation Function is the most popularly used activation function in the areas of convolutional neural networks and deep learning. It is of the form: Hyperbolic Tangent Activation Function It is bipolar in nature. Similar to sigmoid, except a different output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.","title":"Neural Network Activation Function?"},{"location":"weekly/AI/ai/#how-do-neural-networks-work","text":"Example Case: Real Estate Price prediction using a trained NN. Looking at the Red arrows, this synapses connects Age, Area, bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Newer, That have at least 3 bedrooms or more, And which the homes have a larger area. Looking at the Blue arrows, this synapses connects Schools Ratings and bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Families with Kids in school Homes that have more bedrooms for the parents and kids. Looking at the Green arrows, this synapses connects Just the Age of the property. So, this neuron is paying attention to properties that maybe: Newly built, or Properties that are historic, valuable properties, like above 100years old. A rectifier function can be used here, stating that, the moment the property turns 100 years, the rectifier functions kicks from 0 to 1. Looking at the Yellow arrows, this synapses connects Area (SqFt) and Bedrooms of the property. So, this neuron is paying attention to properties that maybe: The homes here have a larger Area Sq and have many rooms. These may be homes that are mansions. Looking at the black arrows, this synapses connects all the input functions. So, this neuron is paying attention to properties because all the inputs match the criteria for the price of the property So, all these different neuron communications will predict the price of the property","title":"How do Neural Networks Work?"},{"location":"weekly/AI/ai/#how-do-neural-networks-learn","text":"ANN can be categorized by how the data flows from the input node to the output node. B Some of the neural networks are some examples: Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Backpropagation algorithm Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. In simple terms, you can think of the data flowing from the input node to the output node through many different paths in the neural network. Only one path is the correct one that maps the input node to the correct output node. To find this path, the neural network uses a feedback loop, which works as follows: Each node makes a guess about the next node in the path. It checks if the guess was correct. Nodes assign higher weight values to paths that lead to more correct guesses and lower weight values to node paths that lead to incorrect guesses. For the next data point, the nodes make a new prediction using the higher weight paths and then repeat Step 1. Convolutional neural networks The hidden layers in convolutional neural networks perform specific mathematical functions, like summarizing or filtering, called convolutions. They are very useful for image classification because they can extract relevant features from images that are useful for image recognition and classification. The new form is easier to process without losing features that are critical for making a good prediction. Each hidden layer extracts and processes different image features, like edges, color, and depth.","title":"How do Neural Networks Learn?"},{"location":"weekly/AI/ai/#machine-learning-in-anns","text":"These ML algorithms help to solve different business problems like Forecasting, Regression, Clustering, Classification, and Associations, etc. Machine learning is divided into mainly four types, which are: Supervised Machine Learning - Ex: Housing Price Prediction. A Supervised machine learning is a technique where we train the machines using the \"labelled\" dataset, and based on the training, the machine predicts the output. In this type of learning, the labelled data specifies that some of the inputs are already mapped to the output. The machine are First trained with the input and corresponding output, and then we ask the machine to predict the output using the test dataset. A classic example will be the example of training a machine with Unsupervised Machine Learning EX: Customer Segmentation and Market Basket Analysis. In unsupervised machine learning, the machine is trained using the unlabeled dataset, and the machine predicts the output without any supervision. In this type, the models are trained with the data that is neither labelled nor classified, and the model acts on that data without any supervision. Unsupervised learning is much similar as a human learns, which makes it closer to the real AI. Semi-Supervised Machine Learning Ex: Lane-Finding on GPS data. It represents the intermediate ground between Supervised and Unsupervised learning algorithms and uses the combination of labelled and unlabeled datasets during the training period. Reinforcement Learning Ex: Drivless Cars. Reinforcement learning works on a feedback-based process, in which an AI agent automatically explore its surrounding by hitting & trail, taking action, learning from experiences, and improving its performance. Agent gets rewarded for each good action and get punished for each bad action; hence the goal of reinforcement learning agent is to maximize the rewards. In reinforcement learning, there is no labelled data like supervised learning, and agents learn from their experiences only. For more on this section, see this javatpoint Link: Click-Link","title":"Machine Learning in ANNs"},{"location":"weekly/Azure/azure/","text":"Azure Free azure account and go to the postal link: Free account-Link Portal-Link Whats hierarchy Azure provides four levels of management: management groups -> Subscriptions -> Resource Groups -> Resources Management groups help you manage access, policy, and compliance for multiple subscriptions. All subscriptions in a management group automatically inherit the conditions that are applied to the management group. Subscriptions associate user accounts with the resources that they create. Resource groups are logical containers where you can deploy and manage Azure resources like web apps, databases, and storage accounts. Resources are instances of services that you can create, such as virtual machines, storage, and SQL databases. Set a budget Azure Storage types Azure Storage offers five core services: Blob Storage helps you create data lakes for your analytics needs, and provides storage to build powerful cloud-native and mobile apps. File Storage : Managed file shares for cloud or on-premises deployments. Queue Storage : A messaging store for reliable messaging between application components. Table Storage : A NoSQL store for schemaless storage of structured data. Managed Disk Storage : Block-level storage volumes for Azure VMs. Azure Elastic SAN (preview) : A fully integrated solution that simplifies deploying, scaling, managing, and configuring a SAN in Azure. Azure also offers a list of SQL Data and non-SQL Databases. For More on Azure Storage, see link: Azure Storage Link Azure Networking Service For More on Azure networking, see link: Azure networking Link Azure and Terraform Azure provder Terraform link: Terraform link Azure authentication from the terminal az login az account list az account list --query [*].[name,id] In azure terminal - bash, - Create an AD service principle for RBAC using below command, - Next, go to subscriptions and get the subscription ID. az ad sp create-for-rbac -n \"Terraform-Soso\" --role=\"Contributor\" --scopes=\"/subscriptions/b965a029-32b4-4671-b283-350344c89091\" Go to azure app registration and see that it was created. Copy the info created. In my case: { \"appId\": \"297ec591-b16e-42ce-9e03-1ebd725aa3f9\", \"displayName\": \"Terraform-Soso\", \"password\": \"FZG8Q~-uPKCxRNgIg8JVAKYO2xm4j..S1SpFoaxg\", \"tenant\": \"054da2e5-2fbf-483f-961e-a3b2839bd53c\" } Azure Active Directory Azure AD part of Microsoft Entra , is an identity and Access Management service that provides single sign-on, multifactor authentication, and conditional access to guard against 99.9 percent of cybersecurity attacks. Accounts and subscriptions Account: Subscription: Tenant: Resource Groups: users There are three types of user accounts that you can have in Azure AD: federated synchronized cloud Optional: Create a new tenant for your organization For more info see: To create a new tenant After creating tenant, it comes with nothin installed, so I will have to add subscriptions... Note : You can switch between tenants Storage Storage Access Control (IAM)","title":"Azure"},{"location":"weekly/Azure/azure/#azure","text":"Free azure account and go to the postal link: Free account-Link Portal-Link","title":"Azure"},{"location":"weekly/Azure/azure/#whats-hierarchy","text":"Azure provides four levels of management: management groups -> Subscriptions -> Resource Groups -> Resources Management groups help you manage access, policy, and compliance for multiple subscriptions. All subscriptions in a management group automatically inherit the conditions that are applied to the management group. Subscriptions associate user accounts with the resources that they create. Resource groups are logical containers where you can deploy and manage Azure resources like web apps, databases, and storage accounts. Resources are instances of services that you can create, such as virtual machines, storage, and SQL databases.","title":"Whats hierarchy"},{"location":"weekly/Azure/azure/#set-a-budget","text":"","title":"Set a budget"},{"location":"weekly/Azure/azure/#azure-storage-types","text":"Azure Storage offers five core services: Blob Storage helps you create data lakes for your analytics needs, and provides storage to build powerful cloud-native and mobile apps. File Storage : Managed file shares for cloud or on-premises deployments. Queue Storage : A messaging store for reliable messaging between application components. Table Storage : A NoSQL store for schemaless storage of structured data. Managed Disk Storage : Block-level storage volumes for Azure VMs. Azure Elastic SAN (preview) : A fully integrated solution that simplifies deploying, scaling, managing, and configuring a SAN in Azure. Azure also offers a list of SQL Data and non-SQL Databases. For More on Azure Storage, see link: Azure Storage Link","title":"Azure Storage types"},{"location":"weekly/Azure/azure/#azure-networking-service","text":"For More on Azure networking, see link: Azure networking Link","title":"Azure Networking Service"},{"location":"weekly/Azure/azure/#azure-and-terraform","text":"Azure provder Terraform link: Terraform link","title":"Azure and Terraform"},{"location":"weekly/Azure/azure/#azure-authentication-from-the-terminal","text":"az login az account list az account list --query [*].[name,id] In azure terminal - bash, - Create an AD service principle for RBAC using below command, - Next, go to subscriptions and get the subscription ID. az ad sp create-for-rbac -n \"Terraform-Soso\" --role=\"Contributor\" --scopes=\"/subscriptions/b965a029-32b4-4671-b283-350344c89091\" Go to azure app registration and see that it was created. Copy the info created. In my case: { \"appId\": \"297ec591-b16e-42ce-9e03-1ebd725aa3f9\", \"displayName\": \"Terraform-Soso\", \"password\": \"FZG8Q~-uPKCxRNgIg8JVAKYO2xm4j..S1SpFoaxg\", \"tenant\": \"054da2e5-2fbf-483f-961e-a3b2839bd53c\" }","title":"Azure authentication from the terminal"},{"location":"weekly/Azure/azure/#azure-active-directory","text":"Azure AD part of Microsoft Entra , is an identity and Access Management service that provides single sign-on, multifactor authentication, and conditional access to guard against 99.9 percent of cybersecurity attacks.","title":"Azure Active Directory"},{"location":"weekly/Azure/azure/#accounts-and-subscriptions","text":"Account: Subscription: Tenant: Resource Groups:","title":"Accounts and subscriptions"},{"location":"weekly/Azure/azure/#users","text":"There are three types of user accounts that you can have in Azure AD: federated synchronized cloud","title":"users"},{"location":"weekly/Azure/azure/#optional-create-a-new-tenant-for-your-organization","text":"For more info see: To create a new tenant After creating tenant, it comes with nothin installed, so I will have to add subscriptions... Note : You can switch between tenants","title":"Optional: Create a new tenant for your organization"},{"location":"weekly/Azure/azure/#storage","text":"","title":"Storage"},{"location":"weekly/Azure/azure/#storage-access-control-iam","text":"","title":"Storage Access Control (IAM)"},{"location":"weekly/Cloud-Technologies/intro-aws/","text":"Cloud Technologies Defining cloud computing Defining private, public, and hybrid cloud Exploring the components of cloud infrastructure Defining the different Service Models What is Cloud Computing? Cloud computing is the use of hosted services, such as servers, data storage, databases, software, and networking over the internet\u00ac\u200a\u2014\u200ainstead of directly accessing your computer hard drive. Types of cloud computing Cloud computing is offered in 3 major forms: private, public, and hybrid cloud. Private Cloud: The private cloud computing service is a cloud computing model where the infrastructure is dedicated only to select users instead of the general public, either over the Internet or a private internal network. Other names for the private cloud are internal or corporate cloud. Some private cloud providers are VMware-vRealize Suite Cloud Management Platform, AWS-Virtual Private Cloud, Microsoft-Azure Private Cloud\u2026 Public Cloud: The public cloud is defined as computing services offered by third-party providers over the public Internet, making them available to anyone who wants to use or purchase them. They may be free or sold on-demand, allowing customers to pay only per usage for the CPU cycles, storage, or bandwidth they consume. Some public cloud services are AWS, GCP, Azure, IBM Cloud, Alibaba Cloud\u2026 Hybrid Cloud: A hybrid cloud is a computing environment that combines an on-premises data center (also called a private cloud) with a public cloud, allowing data and applications to be shared between them. Some public cloud providers are VMware, Microsoft Azure\u2026 Components of cloud infrastructure Client-side Infrastructure: Here, users interact with the cloud through a front-end GUI (Graphical User Interface). Desktop, laptops, mobiles, tablets, and others are used on the client-side to access services. Application: The application may be any software or platform that a client wants to access. The operating system: The cloud operating system manages the operation, the virtual servers, execution and processes of virtual machines and virtual infrastructure, as well as the back-end hardware and software resources. Service: Cloud Services are offered in different models. These services are being provided based on the client\u2019s request. Some of the service models are seen below Service Models Cloud-Computing providers offer their \u201cservices\u201d according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). IAAS: Infrastructure As A Service (IAAS) is means of delivering computing infrastructure as on-demand services. It is one of the three fundamental cloud service model servers\u2019 storage network operating systems. In the user purchasing servers, software data center space, or network equipment and renting those resources as a fully outsourced service can demand model. It allows dynamic scaling, and the resources are distributed as a service. It generally includes multiple users on a single piece of hardware. PAAS: Platform as A Service (PAAS) is a cloud delivery model for applications composed of services managed by a third party. It provides elastic scaling of your application which allows developers to build applications and services over the internet and the deployment models include public, private and hybrid. SAAS: Software As A Service (SAAS) allows users to run existing online applications and it is a model software that is deployed as a hosting service and is accessed over Output Rephrased/Re-written Text the internet or software delivery model during which software and its associated data are hosted centrally and accessed using their client, usually an online browser over the web. SAAS services are used for the development and deployment of modern applications. The emergence of Cloud computing has given rise to more as-a-service offerings. Some of them include: - AIaaS: AI as a service - DaaS: Desktop as a service - ITaaS: IT as a service - RaaS: Ransomware as a service. Top benefits of cloud computing High Speed: the ability to spin up new instances in a matter of seconds. Efficiency and Cost Reduction Provides efficient Data Security Scalable: ease with quickly\u200a\u2014\u200ascale up/down infrastructures. Collaboration: Cloud environments enable better collaboration across teams. Provides unlimited Storage Capacity. Provides ease in Back-up and Restoring of Data Reliability: Cloud computing makes data backup, disaster recovery, and business continuity easier and less expensive because data can be mirrored at multiple redundant sites on the cloud provider\u2019s network. FOCUS: AWS CLOUD What Is Amazon Web Services(AWS): Amazon Web Services(AWS) is Amazon\u2019s cloud computing platform that offers a mix of packaged platform as a service (PaaS), software as a service (SaaS), and infrastructure as a service (IaaS). ### EC2 Amazon EC2 is AWS primary web service that provides resizable compute capacity in the cloud. Instance types: Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. Example instance Family: c4 Compute optimized\u2014For workloads requiring significant processing r3 Memory optimized\u2014For memory-intensive workloads i2 Storage optimized\u2014For workloads requiring high amounts of fast SSD storage g2 GPU-based instances\u2014Intended for graphics and general-purpose GPU compute workloads Amazon Machine Images (AMIs): Defines the initial software that will be on an instance when it is launched. An AMI defines; The Operating System (OS) and its configuration, The initial state of any patches, Application or system software. AMI\u2019s can be AWS published, from the AWS Marketplace, Generated from Existing Instances, or Uploaded Virtual Servers Security Groups: This is a VPC concept. If an instance is running in an Amazon VPC, you can change which security groups are associated with an instance while the instance is running. EBS: Is type of \u201cblock storage\u201d volume offering by AWS. EBS provides persistent block-level storage volumes for use with Amazon EC2 instances. Instance Stores: Is another type of \u201cblock storage\u201d volume offering by AWS for your instance. This storage is located on disks that are physically attached to the host computer. ### S3 Amazon Simple Storage Service is a secure, durable, and highly-scalable cloud storage. Some use cases for cloud storage include: - Backup and archive for on-premises or cloud data. - Content, media, and software storage and distribution. - Big data analytics. - Static website hosting. - Cloud-native mobile and Internet application hosting. ### VPC The Amazon Virtual Private Cloud (Amazon VPC) is a custom-defined virtual network within the AWS Cloud. ![cloud1](photos/cloud5.png) VPC COMPONENTS: An Amazon VPC consists of the following main components: Subnets, Route tables, Dynamic Host Configuration Protocol (DHCP) option sets Security groups, Network Access Control Lists (ACLs). Also, optional components: Internet Gateways (IGWs), Elastic IP (EIP) addresses, Elastic Network Interfaces (ENIs), Endpoints, Peering, Network Address Translation (NATs) instances and NAT gateways, Virtual Private Gateway (VPG), Customer Gateways (CGWs), and Virtual Private Networks (VPNs) ***Subnets:*** A subnet is a segment of an Amazon VPC\u2019s IP address range. Subnets reside within one Availability Zone and cannot span zones; You can, however, have multiple subnets in one Availability Zone. Each subnet must be associated with a route table, which controls the routing for the subnet. ***Route Tables:*** A route table is a logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where network traffic is directed. ***Internet Gateways:*** IGW allows communication between instances in your Amazon VPC and the Internet. ***Dynamic Host Configuration*** Protocol (DHCP) Option Sets: provides a standard for passing configuration information to hosts on a TCP/IP network. AWS automatically creates and associates a DHCP option set for your Amazon VPC upon creation and sets two options: domain-name-servers (defaulted to AmazonProvidedDNS) and domain-name (defaulted to the domain name for your region). ### IAM With AWS IAM Identity Center (successor to AWS Single Sign-On), you can manage sign-in security for your workforce identities, also known as workforce users. IAM Identity Center provides one place where you can create or connect workforce users and centrally manage their access across all their AWS accounts and applications. For more on IAM chack the link: IAM ### AWS ELASTIC LOAD BALANCER (ELB) AWS has an Elastic Load Balancer that is used to distribute incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing supports routing and load balancing of Hypertext. Transfer Protocol (HTTP), Hypertext Transfer Protocol Secure (HTTPS), Transmission Control Protocol (TCP), and Secure Sockets Layer (SSL) traffic to Amazon EC2 instances. Types of AWS ELB: The two major Elastic load balancers are: ***Application Load Balancer*** Application layer that describes the use of HTTP and HTTPS connections from clients to the load balancer and from the load balancer to your back-end instance. ***Network Load Balancer*** Are used to route traffic through layer 4, based IPs and TCP or UDP ports. ### EKS Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. For more on EKS chack the link: [EKS]https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html","title":"Cloud Technologies"},{"location":"weekly/Cloud-Technologies/intro-aws/#cloud-technologies","text":"Defining cloud computing Defining private, public, and hybrid cloud Exploring the components of cloud infrastructure Defining the different Service Models","title":"Cloud Technologies"},{"location":"weekly/Cloud-Technologies/intro-aws/#what-is-cloud-computing","text":"Cloud computing is the use of hosted services, such as servers, data storage, databases, software, and networking over the internet\u00ac\u200a\u2014\u200ainstead of directly accessing your computer hard drive.","title":"What is Cloud Computing?"},{"location":"weekly/Cloud-Technologies/intro-aws/#types-of-cloud-computing","text":"Cloud computing is offered in 3 major forms: private, public, and hybrid cloud. Private Cloud: The private cloud computing service is a cloud computing model where the infrastructure is dedicated only to select users instead of the general public, either over the Internet or a private internal network. Other names for the private cloud are internal or corporate cloud. Some private cloud providers are VMware-vRealize Suite Cloud Management Platform, AWS-Virtual Private Cloud, Microsoft-Azure Private Cloud\u2026 Public Cloud: The public cloud is defined as computing services offered by third-party providers over the public Internet, making them available to anyone who wants to use or purchase them. They may be free or sold on-demand, allowing customers to pay only per usage for the CPU cycles, storage, or bandwidth they consume. Some public cloud services are AWS, GCP, Azure, IBM Cloud, Alibaba Cloud\u2026 Hybrid Cloud: A hybrid cloud is a computing environment that combines an on-premises data center (also called a private cloud) with a public cloud, allowing data and applications to be shared between them. Some public cloud providers are VMware, Microsoft Azure\u2026","title":"Types of cloud computing"},{"location":"weekly/Cloud-Technologies/intro-aws/#components-of-cloud-infrastructure","text":"Client-side Infrastructure: Here, users interact with the cloud through a front-end GUI (Graphical User Interface). Desktop, laptops, mobiles, tablets, and others are used on the client-side to access services. Application: The application may be any software or platform that a client wants to access. The operating system: The cloud operating system manages the operation, the virtual servers, execution and processes of virtual machines and virtual infrastructure, as well as the back-end hardware and software resources. Service: Cloud Services are offered in different models. These services are being provided based on the client\u2019s request. Some of the service models are seen below","title":"Components of cloud infrastructure"},{"location":"weekly/Cloud-Technologies/intro-aws/#service-models","text":"Cloud-Computing providers offer their \u201cservices\u201d according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). IAAS: Infrastructure As A Service (IAAS) is means of delivering computing infrastructure as on-demand services. It is one of the three fundamental cloud service model servers\u2019 storage network operating systems. In the user purchasing servers, software data center space, or network equipment and renting those resources as a fully outsourced service can demand model. It allows dynamic scaling, and the resources are distributed as a service. It generally includes multiple users on a single piece of hardware. PAAS: Platform as A Service (PAAS) is a cloud delivery model for applications composed of services managed by a third party. It provides elastic scaling of your application which allows developers to build applications and services over the internet and the deployment models include public, private and hybrid. SAAS: Software As A Service (SAAS) allows users to run existing online applications and it is a model software that is deployed as a hosting service and is accessed over Output Rephrased/Re-written Text the internet or software delivery model during which software and its associated data are hosted centrally and accessed using their client, usually an online browser over the web. SAAS services are used for the development and deployment of modern applications. The emergence of Cloud computing has given rise to more as-a-service offerings. Some of them include: - AIaaS: AI as a service - DaaS: Desktop as a service - ITaaS: IT as a service - RaaS: Ransomware as a service.","title":"Service Models"},{"location":"weekly/Cloud-Technologies/intro-aws/#top-benefits-of-cloud-computing","text":"High Speed: the ability to spin up new instances in a matter of seconds. Efficiency and Cost Reduction Provides efficient Data Security Scalable: ease with quickly\u200a\u2014\u200ascale up/down infrastructures. Collaboration: Cloud environments enable better collaboration across teams. Provides unlimited Storage Capacity. Provides ease in Back-up and Restoring of Data Reliability: Cloud computing makes data backup, disaster recovery, and business continuity easier and less expensive because data can be mirrored at multiple redundant sites on the cloud provider\u2019s network.","title":"Top benefits of cloud computing"},{"location":"weekly/Cloud-Technologies/intro-aws/#focus-aws-cloud","text":"What Is Amazon Web Services(AWS): Amazon Web Services(AWS) is Amazon\u2019s cloud computing platform that offers a mix of packaged platform as a service (PaaS), software as a service (SaaS), and infrastructure as a service (IaaS). ### EC2 Amazon EC2 is AWS primary web service that provides resizable compute capacity in the cloud. Instance types: Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. Example instance Family: c4 Compute optimized\u2014For workloads requiring significant processing r3 Memory optimized\u2014For memory-intensive workloads i2 Storage optimized\u2014For workloads requiring high amounts of fast SSD storage g2 GPU-based instances\u2014Intended for graphics and general-purpose GPU compute workloads Amazon Machine Images (AMIs): Defines the initial software that will be on an instance when it is launched. An AMI defines; The Operating System (OS) and its configuration, The initial state of any patches, Application or system software. AMI\u2019s can be AWS published, from the AWS Marketplace, Generated from Existing Instances, or Uploaded Virtual Servers Security Groups: This is a VPC concept. If an instance is running in an Amazon VPC, you can change which security groups are associated with an instance while the instance is running. EBS: Is type of \u201cblock storage\u201d volume offering by AWS. EBS provides persistent block-level storage volumes for use with Amazon EC2 instances. Instance Stores: Is another type of \u201cblock storage\u201d volume offering by AWS for your instance. This storage is located on disks that are physically attached to the host computer. ### S3 Amazon Simple Storage Service is a secure, durable, and highly-scalable cloud storage. Some use cases for cloud storage include: - Backup and archive for on-premises or cloud data. - Content, media, and software storage and distribution. - Big data analytics. - Static website hosting. - Cloud-native mobile and Internet application hosting. ### VPC The Amazon Virtual Private Cloud (Amazon VPC) is a custom-defined virtual network within the AWS Cloud. ![cloud1](photos/cloud5.png) VPC COMPONENTS: An Amazon VPC consists of the following main components: Subnets, Route tables, Dynamic Host Configuration Protocol (DHCP) option sets Security groups, Network Access Control Lists (ACLs). Also, optional components: Internet Gateways (IGWs), Elastic IP (EIP) addresses, Elastic Network Interfaces (ENIs), Endpoints, Peering, Network Address Translation (NATs) instances and NAT gateways, Virtual Private Gateway (VPG), Customer Gateways (CGWs), and Virtual Private Networks (VPNs) ***Subnets:*** A subnet is a segment of an Amazon VPC\u2019s IP address range. Subnets reside within one Availability Zone and cannot span zones; You can, however, have multiple subnets in one Availability Zone. Each subnet must be associated with a route table, which controls the routing for the subnet. ***Route Tables:*** A route table is a logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where network traffic is directed. ***Internet Gateways:*** IGW allows communication between instances in your Amazon VPC and the Internet. ***Dynamic Host Configuration*** Protocol (DHCP) Option Sets: provides a standard for passing configuration information to hosts on a TCP/IP network. AWS automatically creates and associates a DHCP option set for your Amazon VPC upon creation and sets two options: domain-name-servers (defaulted to AmazonProvidedDNS) and domain-name (defaulted to the domain name for your region). ### IAM With AWS IAM Identity Center (successor to AWS Single Sign-On), you can manage sign-in security for your workforce identities, also known as workforce users. IAM Identity Center provides one place where you can create or connect workforce users and centrally manage their access across all their AWS accounts and applications. For more on IAM chack the link: IAM ### AWS ELASTIC LOAD BALANCER (ELB) AWS has an Elastic Load Balancer that is used to distribute incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing supports routing and load balancing of Hypertext. Transfer Protocol (HTTP), Hypertext Transfer Protocol Secure (HTTPS), Transmission Control Protocol (TCP), and Secure Sockets Layer (SSL) traffic to Amazon EC2 instances. Types of AWS ELB: The two major Elastic load balancers are: ***Application Load Balancer*** Application layer that describes the use of HTTP and HTTPS connections from clients to the load balancer and from the load balancer to your back-end instance. ***Network Load Balancer*** Are used to route traffic through layer 4, based IPs and TCP or UDP ports. ### EKS Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. For more on EKS chack the link: [EKS]https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html","title":"FOCUS: AWS CLOUD"},{"location":"weekly/Docker/docker/","text":"","title":"Docker"},{"location":"weekly/ELK/elk/","text":"ELK - Elastic Stack ELK Stack is the world\u2019s most popular log management platform. The ELK stack is an acronym used to describe a stack that comprises of three popular projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a distributed search and analytics engine built on Apache Lucene Logstash is used for both shipping as well as processing and storing logs Kibana is a visualization tool which is hosted through Nginx or Apache Some popular Use Cases for ELK are: Logging, Matrics, Security Analysis, Business Analysis - Logging - logging and analysing Data, like gaming data - Matrics - like NASA Rover can sent telematrics, sensor data - Security Analysis - Helps with securing comunication in Apps like Slack - Business Analysis - Helps with matching people ElasticSearch With ElasticSearch, User can sends search quary to your website and your server send those requests to ElasticSearch, Elasticsearch then processes this request and Responds back to the user, through the Server. Elasticsearch Architecture: Key Components Elasticsearch Cluster is composed of a group of nodes that store data. Elasticsearch Node is an instance that runs in a cluster. Multiple nodes can be run on a single machine. When Elastic is running, you will have an instance of elastic known as Node. Each Node Has a unique ID and a node. Each Node belongs to a single cluster. Data is stored as documents in Elasticsearch. Indices are used to group documents that are related to each other. When an Index is created, it comes with a Shard, by default. There are the three main options to configure an Elasticsearch node: Elasticsearch master node Elasticsearch data node Elasticsearch client node Search movement - client --> servers --> ElasticSearch Shards is where data is stored. What you create an Index, you can create multiple shards that are distributed across nodes. Each shard has its replica to recover data in case of node failure. The primary shard and replica shard is always placed on different nodes. Horizontal scaling or scale-out is the main reason to shard a database. The number of shards depends heavily on the amount of data we have. Shard Example use Case 1 A BCBSNC Elastic engineer has 900k documents that they want to store. Each Shard can only Hold a maximum of 300k Documents Two more shards of 300GB each will be added bringing the total storage capacity to 900K Documents Shard Example use Case 2 Scenario 1: An Elastic engineer searches 500k documents that is stored in a single Shard The response time for searching 500k Documents in a single shard is 10 seconds. Scenario 2: An Elastic engineer runs a parallel searches of 500k documents that are distributed in 5 Shard. The response time for searching 100k Documents/Shard will be 2 seconds. So, the response time for searching all 500k Documents that are distributed amongst the 5 shard will still be 2 seconds. Kibana Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. Kibana Aggregations There are two main types of aggregations Metric aggregations are used to calculate a value for each bucket based on the documents inside the bucket. Bucket aggregations are used groups documents together in one bucket according to your logic and requirements, while the Metric aggregations For More information on Kibana,: Click link Adding data into Elasticsearch The main consideration for indexing or adding data into Elasticsearch largely depends on whether you are indexing general content or timestamped data. General content: Index content like HTML pages, catalogs and other files. Send data directly to Elasticseach from your application using an Elastic language client. Otherwise use Workplace Search content connectors or the Enterprise Search web crawler. Timestamped data: The preferred way to index timestamped data is to use Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. How to ingest data into Elasticsearch Service There are 4 main ways in Adding Data to ElasticSearch Service: By using Beats and Logstash By using Kibana By programmatically indexing data Manually adding data Read more about data ingestion in ElasticSearch Click Link The index The index is the basis of Elasticsearch, it is where you store your documents. You can compare it to a table in a database. An index has settings and a mapping which defines how to store and index the documents. Mapping Mapping is the process of defining how a document, and the fields it contains, are stored and indexed. Each document is a collection of fields, which each have their own data type. When mapping your data, you create a mapping definition, which contains a list of fields that are pertinent to the document. What is Elastic integrations Elastic integrations are a streamlined way to connect your data to the Elastic Stack. Integrations are available for popular services and platforms, like Nginx, AWS, and MongoDB, as well as many generic input types like log files. For more on Data Integration Click link: Elastic Agent Elastic integrations are powered by Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. It can also protect hosts from security threats, query data from operating systems, forward data from remote services or hardware, and more. Sample Hands-on I will : Use curl commands to talk to the Elasticsearch REST API. Or if you choose to use Kibana Devtools, thats also ok, because Kibana\u2019s Devtools can automatically recognizes and converts curl commands . create an index Called middleware curl -X PUT \u201clocalhost:9200/middleware Delete the index Called middleware curl -X DELETE \u201clocalhost:9200/middleware\u201d Create an index Called middleware with just one shard and no replicas PUT /middleware?pretty { \"settings\" : { \"number_of_shards\" : 1, \"number_of_replicas\" : 0 } } loading data into that index Called middleware PUT /middleware/_mapping { \"properties\": { \"post_time\": { \"type\": \"date\" }, \"username\": { \"type\": \"keyword\" }, \"message\": { \"type\": \"text\" } } } Now Load some data Manually POST /middleware/_doc/?pretty { \"user\" : \"collins\", \"post_date\" : \"2023-16-03T14:10:10\", \"message\" : \"Welcome to Elasticsearch?\" } see this mediam link, its cool Click-Link","title":"ELK"},{"location":"weekly/ELK/elk/#elk-elastic-stack","text":"ELK Stack is the world\u2019s most popular log management platform. The ELK stack is an acronym used to describe a stack that comprises of three popular projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a distributed search and analytics engine built on Apache Lucene Logstash is used for both shipping as well as processing and storing logs Kibana is a visualization tool which is hosted through Nginx or Apache Some popular Use Cases for ELK are: Logging, Matrics, Security Analysis, Business Analysis - Logging - logging and analysing Data, like gaming data - Matrics - like NASA Rover can sent telematrics, sensor data - Security Analysis - Helps with securing comunication in Apps like Slack - Business Analysis - Helps with matching people","title":"ELK - Elastic Stack"},{"location":"weekly/ELK/elk/#elasticsearch","text":"With ElasticSearch, User can sends search quary to your website and your server send those requests to ElasticSearch, Elasticsearch then processes this request and Responds back to the user, through the Server.","title":"ElasticSearch"},{"location":"weekly/ELK/elk/#elasticsearch-architecture-key-components","text":"Elasticsearch Cluster is composed of a group of nodes that store data. Elasticsearch Node is an instance that runs in a cluster. Multiple nodes can be run on a single machine. When Elastic is running, you will have an instance of elastic known as Node. Each Node Has a unique ID and a node. Each Node belongs to a single cluster. Data is stored as documents in Elasticsearch. Indices are used to group documents that are related to each other. When an Index is created, it comes with a Shard, by default. There are the three main options to configure an Elasticsearch node: Elasticsearch master node Elasticsearch data node Elasticsearch client node Search movement - client --> servers --> ElasticSearch Shards is where data is stored. What you create an Index, you can create multiple shards that are distributed across nodes. Each shard has its replica to recover data in case of node failure. The primary shard and replica shard is always placed on different nodes. Horizontal scaling or scale-out is the main reason to shard a database. The number of shards depends heavily on the amount of data we have. Shard Example use Case 1 A BCBSNC Elastic engineer has 900k documents that they want to store. Each Shard can only Hold a maximum of 300k Documents Two more shards of 300GB each will be added bringing the total storage capacity to 900K Documents Shard Example use Case 2 Scenario 1: An Elastic engineer searches 500k documents that is stored in a single Shard The response time for searching 500k Documents in a single shard is 10 seconds. Scenario 2: An Elastic engineer runs a parallel searches of 500k documents that are distributed in 5 Shard. The response time for searching 100k Documents/Shard will be 2 seconds. So, the response time for searching all 500k Documents that are distributed amongst the 5 shard will still be 2 seconds.","title":"Elasticsearch Architecture: Key Components"},{"location":"weekly/ELK/elk/#kibana","text":"Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. Kibana Aggregations There are two main types of aggregations Metric aggregations are used to calculate a value for each bucket based on the documents inside the bucket. Bucket aggregations are used groups documents together in one bucket according to your logic and requirements, while the Metric aggregations For More information on Kibana,: Click link","title":"Kibana"},{"location":"weekly/ELK/elk/#adding-data-into-elasticsearch","text":"The main consideration for indexing or adding data into Elasticsearch largely depends on whether you are indexing general content or timestamped data. General content: Index content like HTML pages, catalogs and other files. Send data directly to Elasticseach from your application using an Elastic language client. Otherwise use Workplace Search content connectors or the Enterprise Search web crawler. Timestamped data: The preferred way to index timestamped data is to use Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host.","title":"Adding data into Elasticsearch"},{"location":"weekly/ELK/elk/#how-to-ingest-data-into-elasticsearch-service","text":"There are 4 main ways in Adding Data to ElasticSearch Service: By using Beats and Logstash By using Kibana By programmatically indexing data Manually adding data Read more about data ingestion in ElasticSearch Click Link","title":"How to ingest data into Elasticsearch Service"},{"location":"weekly/ELK/elk/#the-index","text":"The index is the basis of Elasticsearch, it is where you store your documents. You can compare it to a table in a database. An index has settings and a mapping which defines how to store and index the documents.","title":"The index"},{"location":"weekly/ELK/elk/#mapping","text":"Mapping is the process of defining how a document, and the fields it contains, are stored and indexed. Each document is a collection of fields, which each have their own data type. When mapping your data, you create a mapping definition, which contains a list of fields that are pertinent to the document.","title":"Mapping"},{"location":"weekly/ELK/elk/#what-is-elastic-integrations","text":"Elastic integrations are a streamlined way to connect your data to the Elastic Stack. Integrations are available for popular services and platforms, like Nginx, AWS, and MongoDB, as well as many generic input types like log files. For more on Data Integration Click link:","title":"What is Elastic integrations"},{"location":"weekly/ELK/elk/#elastic-agent","text":"Elastic integrations are powered by Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. It can also protect hosts from security threats, query data from operating systems, forward data from remote services or hardware, and more.","title":"Elastic Agent"},{"location":"weekly/ELK/elk/#sample-hands-on","text":"I will : Use curl commands to talk to the Elasticsearch REST API. Or if you choose to use Kibana Devtools, thats also ok, because Kibana\u2019s Devtools can automatically recognizes and converts curl commands . create an index Called middleware curl -X PUT \u201clocalhost:9200/middleware Delete the index Called middleware curl -X DELETE \u201clocalhost:9200/middleware\u201d Create an index Called middleware with just one shard and no replicas PUT /middleware?pretty { \"settings\" : { \"number_of_shards\" : 1, \"number_of_replicas\" : 0 } } loading data into that index Called middleware PUT /middleware/_mapping { \"properties\": { \"post_time\": { \"type\": \"date\" }, \"username\": { \"type\": \"keyword\" }, \"message\": { \"type\": \"text\" } } } Now Load some data Manually POST /middleware/_doc/?pretty { \"user\" : \"collins\", \"post_date\" : \"2023-16-03T14:10:10\", \"message\" : \"Welcome to Elasticsearch?\" } see this mediam link, its cool Click-Link","title":"Sample Hands-on"},{"location":"weekly/GitHub/github/","text":"","title":"GitHub"},{"location":"weekly/Gitaction/gitaction/","text":"Getting Started GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production. Github actions has three main components: Events -> Workflows -> Actions Workflow triggers are events that cause a workflow to run. Events provide status updates from GitHub repos from code commits or other event of a project. For more information on workflows. see site-link: For more information about how to use workflow triggers: See workflow triggers Description of the below yaml file name: is straightforward, you can name what ever name you choose The events is the line thet starts with On. In the yaml file, we have an event to push. actions/checkout@v3: is already defined in Github. You can see more Here: github actions create a personal access token in your HitHub account and name: PERSONAL_TOKEN name: Publish docs via GitHub Pages on: push: branches: - main permissions: contents: write jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - uses: actions/cache@v2 with: key: ${{ github.ref }} path: .cache - run: pip install mkdocs-material - run: mkdocs gh-deploy --force env: PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }} CONFIG_FILE: folder/mkdocs.yml #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}","title":"GitHub Action"},{"location":"weekly/Gitaction/gitaction/#getting-started","text":"GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production. Github actions has three main components: Events -> Workflows -> Actions Workflow triggers are events that cause a workflow to run. Events provide status updates from GitHub repos from code commits or other event of a project. For more information on workflows. see site-link: For more information about how to use workflow triggers: See workflow triggers","title":"Getting Started"},{"location":"weekly/Gitaction/gitaction/#description-of-the-below-yaml-file","text":"name: is straightforward, you can name what ever name you choose The events is the line thet starts with On. In the yaml file, we have an event to push. actions/checkout@v3: is already defined in Github. You can see more Here: github actions create a personal access token in your HitHub account and name: PERSONAL_TOKEN name: Publish docs via GitHub Pages on: push: branches: - main permissions: contents: write jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - uses: actions/cache@v2 with: key: ${{ github.ref }} path: .cache - run: pip install mkdocs-material - run: mkdocs gh-deploy --force env: PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }} CONFIG_FILE: folder/mkdocs.yml #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}","title":"Description of the below yaml file"},{"location":"weekly/Jenkins/jenkins/","text":"","title":"Jenkins"},{"location":"weekly/Kubernetes/kubernetes/","text":"","title":"Kubernetes"},{"location":"weekly/Openshift/openshift/","text":"What's Red Hat OpenShift Red Hat OpenShift, the industry's leading hybrid cloud application platform powered by Kubernetes, brings together tested and trusted services to reduce the friction of developing, modernizing, deploying, running, and managing applications. See official link: Openshift Getting Started - Steps Signup/signin to Openshift account: See-Link Navigate to: Clusters -> Cluster Type -> Amazon Web Services -> Installer-provisioned infrastructure Install OpenShift on AWS AWS Openshift installation Link: See-Link There are Two Cluster-Setup options to choose: a customized cluster or quickly install. Install the Openshift installer and client on your local computer. Move that file to your remote linux machine. You're see the client and installer. Extract the installer and client: tar xvf openshift-install-linux.tar.gz tar xvf openshift-client-linux.tar.gz Move the oc, kubectl and installer to usr/bin directoory sudo mv oc /usr/local/bin/ sudo mv kubectl /usr/local/bin/ sudo cp openshift-install /usr/local/bin/ which openshift-install oc help Copy the secret content and paste in a file. This secret is from the redhad account, used to Map the AWS Cluster with the redHat account. In the AWS Console, setup the following: Route53 public Hoster Zone Add OpenShift port 6443 to instance security groups Create Create Access and Secret Keys for the Root-user and configure aws aws configure Create an SSH Key in the root directory [root@ip-172-31-12-23 ~]# ssh-keygen -t rsa -b 4096 -N '' -f id_rsa Evaluate and add to root [root@ip-172-31-12-23 ~]# eval \"$(ssh-agent -s)\" [root@ip-172-31-12-23 ~]# ssh-add /root/id_rsa - make a new directory for the installation, mine is soso-dir mkdir soso-dir/ Now install Openshift ./openshift-install create cluster --dir /root/soso-dir/ --log-level debug After installation, you should have your results as seen in the below image: Cat and export the konfig file cat soso-dir/auth/kubeconfig export KUBECONFIG=/root/soso-dir/auth/kubeconfig oc whoami ### Destroy the cluster ./openshift-install destroy cluster --dir /root/soso-dir/ --log-level debug OR OR OR OR OR install Openshift using the config, so you can customize . A prompt will begin. The last prompt will be the secret Copy and paste the secret characters that we had saved earlier. openshift-install create install-config Copy the install-config.yaml to install-config.yaml.bak cp install-config.yaml install-config.yaml.bak vi install-config.yaml.bak paste the below content in the file. edit the file to suite ur options. apiVersion: v1 baseDomain: marcollogistics.com controlPlane: hyperthreading: Enabled name: master platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 3 compute: - hyperthreading: Enabled name: worker platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 1 metadata: name: openshift networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.0.0/20 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: aws: region: us-east-1 userTags: adminContact: Collins costCenter: 123 email: cafanwi@sosotechnologies.com publish: External pullSecret: '' sshKey: SEE MY workflow process [ec2-user@ip-172-31-12-23 s]$ ls openshift-install-linux.tar.gz README.md [ec2-user@ip-172-31-12-23 s]$ tar xvf openshift-install-linux.tar.gz README.md openshift-install [ec2-user@ip-172-31-12-23 s]$ ls openshift-install openshift-install-linux.tar.gz README.md [ec2-user@ip-172-31-12-23 s]$ ls openshift-client-linux.tar.gz openshift-install openshift-install-linux.tar.gz README.md [ec2-user@ip-172-31-12-23 s]$ tar xvf openshift-client-linux.tar.gz README.md oc kubectl [ec2-user@ip-172-31-12-23 s]$ ls kubectl oc openshift-client-linux.tar.gz openshift-install openshift-install-linux.tar.gz README.md [ec2-user@ip-172-31-12-23 s]$ AWS Openshift installation Link: See-Link","title":"OpenShift"},{"location":"weekly/Openshift/openshift/#whats-red-hat-openshift","text":"Red Hat OpenShift, the industry's leading hybrid cloud application platform powered by Kubernetes, brings together tested and trusted services to reduce the friction of developing, modernizing, deploying, running, and managing applications. See official link: Openshift","title":"What's Red Hat OpenShift"},{"location":"weekly/Openshift/openshift/#getting-started-steps","text":"Signup/signin to Openshift account: See-Link Navigate to: Clusters -> Cluster Type -> Amazon Web Services -> Installer-provisioned infrastructure","title":"Getting Started - Steps"},{"location":"weekly/Openshift/openshift/#install-openshift-on-aws","text":"AWS Openshift installation Link: See-Link There are Two Cluster-Setup options to choose: a customized cluster or quickly install. Install the Openshift installer and client on your local computer. Move that file to your remote linux machine. You're see the client and installer. Extract the installer and client: tar xvf openshift-install-linux.tar.gz tar xvf openshift-client-linux.tar.gz Move the oc, kubectl and installer to usr/bin directoory sudo mv oc /usr/local/bin/ sudo mv kubectl /usr/local/bin/ sudo cp openshift-install /usr/local/bin/ which openshift-install oc help Copy the secret content and paste in a file. This secret is from the redhad account, used to Map the AWS Cluster with the redHat account. In the AWS Console, setup the following: Route53 public Hoster Zone Add OpenShift port 6443 to instance security groups Create Create Access and Secret Keys for the Root-user and configure aws aws configure Create an SSH Key in the root directory [root@ip-172-31-12-23 ~]# ssh-keygen -t rsa -b 4096 -N '' -f id_rsa Evaluate and add to root [root@ip-172-31-12-23 ~]# eval \"$(ssh-agent -s)\" [root@ip-172-31-12-23 ~]# ssh-add /root/id_rsa - make a new directory for the installation, mine is soso-dir mkdir soso-dir/ Now install Openshift ./openshift-install create cluster --dir /root/soso-dir/ --log-level debug After installation, you should have your results as seen in the below image: Cat and export the konfig file cat soso-dir/auth/kubeconfig export KUBECONFIG=/root/soso-dir/auth/kubeconfig oc whoami ### Destroy the cluster ./openshift-install destroy cluster --dir /root/soso-dir/ --log-level debug OR OR OR OR OR install Openshift using the config, so you can customize . A prompt will begin. The last prompt will be the secret Copy and paste the secret characters that we had saved earlier. openshift-install create install-config Copy the install-config.yaml to install-config.yaml.bak cp install-config.yaml install-config.yaml.bak vi install-config.yaml.bak paste the below content in the file. edit the file to suite ur options. apiVersion: v1 baseDomain: marcollogistics.com controlPlane: hyperthreading: Enabled name: master platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 3 compute: - hyperthreading: Enabled name: worker platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 1 metadata: name: openshift networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.0.0/20 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: aws: region: us-east-1 userTags: adminContact: Collins costCenter: 123 email: cafanwi@sosotechnologies.com publish: External pullSecret: '' sshKey: SEE MY workflow process [ec2-user@ip-172-31-12-23 s]$ ls openshift-install-linux.tar.gz README.md [ec2-user@ip-172-31-12-23 s]$ tar xvf openshift-install-linux.tar.gz README.md openshift-install [ec2-user@ip-172-31-12-23 s]$ ls openshift-install openshift-install-linux.tar.gz README.md [ec2-user@ip-172-31-12-23 s]$ ls openshift-client-linux.tar.gz openshift-install openshift-install-linux.tar.gz README.md [ec2-user@ip-172-31-12-23 s]$ tar xvf openshift-client-linux.tar.gz README.md oc kubectl [ec2-user@ip-172-31-12-23 s]$ ls kubectl oc openshift-client-linux.tar.gz openshift-install openshift-install-linux.tar.gz README.md [ec2-user@ip-172-31-12-23 s]$ AWS Openshift installation Link: See-Link","title":"Install OpenShift on AWS"},{"location":"weekly/Random/random/","text":"Tables Are Cool col 1 is b sdvvrbbbbbbbbrggg $1600","title":"Random Topics"},{"location":"weekly/Terraform/terraform/","text":"","title":"Terraform"}]}