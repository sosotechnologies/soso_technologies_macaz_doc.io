{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])"},"docs":[{"location":"","text":"Welcome To SosoTech We specialize in IT training and Hands-on. What we do We train New and Working IT Professionals on the following fields: DevOps SRE Cloud Engineers AI Engineers","title":"About Us"},{"location":"#welcome-to-sosotech","text":"We specialize in IT training and Hands-on.","title":"Welcome To SosoTech"},{"location":"#what-we-do","text":"We train New and Working IT Professionals on the following fields: DevOps SRE Cloud Engineers AI Engineers","title":"What we do"},{"location":"Getting-Started/local-install/","text":"Locally System Requirements Windows OS systems will be the preferred OS that will be used throughout this program. Students with MaC OS can still use their MAC pcs, but may have some diffuculties with running some Windows-specific commands. Install WSL Requirments: windows 10 or greater In the windows search, Type: 'feat' Click -> Turning windows features off or on, and check select [x] virtual machine platform [x] windows Subsystem for Linux Some common WSL commands, use PowerShell $ winver $ wsl -l -v $ wsl --set-version Ubuntu-20.04 2 $ wsl --set-default-version 2 See Youtube link: Click link Install Linux Kernel Step 4 - Download the Linux kernel update package Now go to searchBar and search store Install Chocolatey Open Powershell as Administrative user Link Click-Link Run Command: Get-ExecutionPolicy If it returns Restricted, then run: Set-ExecutionPolicy AllSigned Now run the following command: Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1')) Install AWSCli Run command: msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi Install Choco Packages choco install kubectl choco install k9s choco install terraform choco install kubens kubectx Install Docker Desktop/k8s use link: Click link to install Restart System","title":"Local-Installation"},{"location":"Getting-Started/local-install/#locally","text":"System Requirements Windows OS systems will be the preferred OS that will be used throughout this program. Students with MaC OS can still use their MAC pcs, but may have some diffuculties with running some Windows-specific commands.","title":"Locally"},{"location":"Getting-Started/local-install/#install-wsl","text":"Requirments: windows 10 or greater In the windows search, Type: 'feat' Click -> Turning windows features off or on, and check select [x] virtual machine platform [x] windows Subsystem for Linux Some common WSL commands, use PowerShell $ winver $ wsl -l -v $ wsl --set-version Ubuntu-20.04 2 $ wsl --set-default-version 2 See Youtube link: Click link Install Linux Kernel Step 4 - Download the Linux kernel update package Now go to searchBar and search store","title":"Install WSL"},{"location":"Getting-Started/local-install/#install-chocolatey","text":"Open Powershell as Administrative user Link Click-Link Run Command: Get-ExecutionPolicy If it returns Restricted, then run: Set-ExecutionPolicy AllSigned Now run the following command: Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))","title":"Install Chocolatey"},{"location":"Getting-Started/local-install/#install-awscli","text":"Run command: msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi","title":"Install AWSCli"},{"location":"Getting-Started/local-install/#install-choco-packages","text":"choco install kubectl choco install k9s choco install terraform choco install kubens kubectx","title":"Install Choco Packages"},{"location":"Getting-Started/local-install/#install-docker-desktopk8s","text":"use link: Click link to install Restart System","title":"Install Docker Desktop/k8s"},{"location":"Getting-Started/remote-install/","text":"Remote Server Installation links Install IAM EKS authenticator Right-Click to open Link in a New Tab curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator sudo mv ./aws-iam-authenticator /usr/local/bin aws-iam-authenticator help Install docker in ec2 Right-Click to open Link in a New Tab Install AWSCLI Right-Click to open Link in a New Tab Install AWCLI on Ubuntu sudo apt update -y sudo apt install awscli -y Install Terraform Right-Click to open Link in a New Tab Install Kubens + kubectx Right-Click to open Link in a New Tab Install HelM Right-Click to open Link in a New Tab curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Install Kubectl Right-Click to open Link in a New Tab curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.11/2023-03-17/bin/linux/amd64/kubectl chmod +x ./kubectl mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin ***More Troubleshooting Options*** sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl [OR] sudo install -o root -g root -m 0755 kubectl /home/ec2-user/bin/kubectl kubectl version --client --output=yaml Install MkDocs Right-Click to open Link in a New Tab pip install mkdocs mkdocs helm mkdocs new my-soso-package cd my-soso-package tree Install PiP on RHeL REDHAT sudo yum info python*-pip //get the pip version, then install the version sudo yum install python3-pip python3 --version curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user Install python-pip in ubuntu sudo apt install python3-pip Install Trivy Right-Click to open Link in a New Tab sudo yum -y update sudo wget https://github.com/aquasecurity/trivy/releases/download/v0.18.3/trivy_0.18.3_Linux-64bit.deb sudo dpkg -i trivy_0.18.3_Linux-64bit.deb trivy i nginx //scanning nginx image trivy i nginx | grep -i critical trivy i nginx:alpine | grep -i critical Install KOPs on Ubuntu Right-Click to open Link in a New Tab Add KOPS User sudo adduser kops sudo echo \"kops ALL=(ALL) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/kops sudo su - kops Installing kOps curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64 chmod +x ./kops sudo mv ./kops /usr/local/bin/ Install Python PiP sudo apt -y update sudo apt install python3-pip Install AWSCli sudo apt install awscli -y Install Kubectl curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl Create S3 Bucket aws s3 ls aws s3api create-bucket \\ --bucket soso-kops-bucket \\ --region us-east-1 \\ --acl public-read Add env variables in bashrc vi .bashrc export NAME=soso-kops-bucket.k8s.local export KOPS_STATE_STORE=s3://soso-kops-bucket.local source .bashrc Generate an sshkeys before creating cluster ssh-keygen Install Cluster preconfig file kops create cluster \\ --name=${NAME} \\ --cloud=aws \\ --zones=us-west-2a \\ --discovery-store=s3://soso-kops-bucket/${NAME}/discovery Edit the cluster name- OPTIONAL kops edit cluster --name ${NAME} Install Cluster kops update cluster --name ${NAME} --yes --admin NOTE: WAIT 10 Mins before Checking Nodes and Validating cluster * Get nodes kubectl get nodes Validate Cluster kops validate cluster --wait 10m Delete cluster kops delete cluster --name ${NAME} [OR] kops delete cluster --name ${NAME} --yes Install pyton and pup on libux AWS python --version sudo yum install python37 python3 --version To install pip and the EB CLI curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user echo $SHELL ls -a ~ export PATH=/bin/bash:$PATH source .bashrc","title":"Remote Installation"},{"location":"Getting-Started/remote-install/#remote-server","text":"","title":"Remote Server"},{"location":"Getting-Started/remote-install/#installation-links","text":"","title":"Installation links"},{"location":"Getting-Started/remote-install/#install-iam-eks-authenticator","text":"Right-Click to open Link in a New Tab curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator sudo mv ./aws-iam-authenticator /usr/local/bin aws-iam-authenticator help","title":"Install IAM EKS authenticator"},{"location":"Getting-Started/remote-install/#install-docker-in-ec2","text":"Right-Click to open Link in a New Tab","title":"Install docker in ec2"},{"location":"Getting-Started/remote-install/#install-awscli","text":"Right-Click to open Link in a New Tab Install AWCLI on Ubuntu sudo apt update -y sudo apt install awscli -y","title":"Install AWSCLI"},{"location":"Getting-Started/remote-install/#install-terraform","text":"Right-Click to open Link in a New Tab","title":"Install Terraform"},{"location":"Getting-Started/remote-install/#install-kubens-kubectx","text":"Right-Click to open Link in a New Tab","title":"Install Kubens + kubectx"},{"location":"Getting-Started/remote-install/#install-helm","text":"Right-Click to open Link in a New Tab curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh","title":"Install HelM"},{"location":"Getting-Started/remote-install/#install-kubectl","text":"Right-Click to open Link in a New Tab curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.11/2023-03-17/bin/linux/amd64/kubectl chmod +x ./kubectl mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin ***More Troubleshooting Options*** sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl [OR] sudo install -o root -g root -m 0755 kubectl /home/ec2-user/bin/kubectl kubectl version --client --output=yaml","title":"Install Kubectl"},{"location":"Getting-Started/remote-install/#install-mkdocs","text":"Right-Click to open Link in a New Tab pip install mkdocs mkdocs helm mkdocs new my-soso-package cd my-soso-package tree","title":"Install MkDocs"},{"location":"Getting-Started/remote-install/#install-pip-on-rhel","text":"REDHAT sudo yum info python*-pip //get the pip version, then install the version sudo yum install python3-pip python3 --version curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user","title":"Install PiP on RHeL"},{"location":"Getting-Started/remote-install/#install-python-pip-in-ubuntu","text":"sudo apt install python3-pip","title":"Install python-pip in ubuntu"},{"location":"Getting-Started/remote-install/#install-trivy","text":"Right-Click to open Link in a New Tab sudo yum -y update sudo wget https://github.com/aquasecurity/trivy/releases/download/v0.18.3/trivy_0.18.3_Linux-64bit.deb sudo dpkg -i trivy_0.18.3_Linux-64bit.deb trivy i nginx //scanning nginx image trivy i nginx | grep -i critical trivy i nginx:alpine | grep -i critical","title":"Install Trivy"},{"location":"Getting-Started/remote-install/#install-kops-on-ubuntu","text":"Right-Click to open Link in a New Tab Add KOPS User sudo adduser kops sudo echo \"kops ALL=(ALL) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/kops sudo su - kops Installing kOps curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64 chmod +x ./kops sudo mv ./kops /usr/local/bin/ Install Python PiP sudo apt -y update sudo apt install python3-pip Install AWSCli sudo apt install awscli -y Install Kubectl curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl Create S3 Bucket aws s3 ls aws s3api create-bucket \\ --bucket soso-kops-bucket \\ --region us-east-1 \\ --acl public-read Add env variables in bashrc vi .bashrc export NAME=soso-kops-bucket.k8s.local export KOPS_STATE_STORE=s3://soso-kops-bucket.local source .bashrc Generate an sshkeys before creating cluster ssh-keygen Install Cluster preconfig file kops create cluster \\ --name=${NAME} \\ --cloud=aws \\ --zones=us-west-2a \\ --discovery-store=s3://soso-kops-bucket/${NAME}/discovery Edit the cluster name- OPTIONAL kops edit cluster --name ${NAME} Install Cluster kops update cluster --name ${NAME} --yes --admin NOTE: WAIT 10 Mins before Checking Nodes and Validating cluster * Get nodes kubectl get nodes Validate Cluster kops validate cluster --wait 10m Delete cluster kops delete cluster --name ${NAME} [OR] kops delete cluster --name ${NAME} --yes Install pyton and pup on libux AWS python --version sudo yum install python37 python3 --version To install pip and the EB CLI curl -O https://bootstrap.pypa.io/get-pip.py python3 get-pip.py --user echo $SHELL ls -a ~ export PATH=/bin/bash:$PATH source .bashrc","title":"Install KOPs on Ubuntu"},{"location":"weekly/AI/ai/","text":"Artificial Intelligence Eventhough AI Machines seem to be a threat to humanity, yet, they cannot beat the power of human intelligence, Machines will never beat the creative or imaginative ability of humans. Always remember, humans created the machine. What is AI? \"It is a branch of computer science by which we can create intelligent machines which can behave like a human, think like humans, and able to make decisions.\" setup google codeLab Open-Link Theres always been this lask of understanding between machine learning and artificial intelligence. Put in context, artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience. Artificial Neural Network (ANN) Artificial Neural Networks the most important part of the deep learning model, they are the brain inspired deep learning tool which replicate the way humans learn. The Neuron In the Human Body, the Neuron is a part of the brain and nervous system. The three broad functions of the CNS are: To take in sensory information process information, and To send out motor signals. But Now How do Neurons Connect? Neurons communicate with each other by sending chemicals, called neurotransmitters, across a tiny space called a synapse, between the axons and dendrites of nearby neurons. So, now that we have that understanding of a human neuron, and how the neurons connect, the goal of AI was to replicate the way the human brain learns. In another sence, create machines that would have the ability to learn, adapt, and make decisions. Neurons in AI As seen in the diagram below, the Input Values are independent Variables of [1, 2, and m]. These variables need to be standardized to make sure all variables contribute evenly to a scale when items are added together, or to make it easier to interpret results of a regression or other analysis. The output Value can be: continuous, binary or a catagorical variable. Neural network is made of interconnected neurons. Each of them is characterized by its weight, bias, and activation function. Weight Weight is the parameter within a neural network that transforms input data within the network's hidden layers. Bias Bias in AI is when the machine gives consistently different outputs for one group of people compared to another. Typically these bias outputs follow classical societal biases like race, gender, biological sex, nationality or age. Neural Network Activation Function? An Activation Function is what makes the decision, if a neuron should be activated or not. This means that an ctivation Function is a mathematical \u201cgate\u201d that decide whether the neuron\u2019s input to the network is important or not in the process of prediction using simpler mathematical operations. Types of Activation Functions Threshold Activation Function is a commonly used activation function.It gives 1 as output of the input is either 0 or positive. If the input is negative, it gives 0 as output. Sigmoid Activation Function Sigmoid Function is the most commonly used activation function in neural networks. It is the go-to model when it comes to predicting the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid has the ability to takes any real value as input and outputs values in the range of 0 to 1. Rectifier Activation Function is the most popularly used activation function in the areas of convolutional neural networks and deep learning. It is of the form: Hyperbolic Tangent Activation Function It is bipolar in nature. Similar to sigmoid, except a different output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0. How do Neural Networks Work? Example Case: Real Estate Price prediction using a trained NN. Looking at the Red arrows, this synapses connects Age, Area, bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Newer, That have at least 3 bedrooms or more, And which the homes have a larger area. Looking at the Blue arrows, this synapses connects Schools Ratings and bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Families with Kids in school Homes that have more bedrooms for the parents and kids. Looking at the Green arrows, this synapses connects Just the Age of the property. So, this neuron is paying attention to properties that maybe: Newly built, or Properties that are historic, valuable properties, like above 100years old. A rectifier function can be used here, stating that, the moment the property turns 100 years, the rectifier functions kicks from 0 to 1. Looking at the Yellow arrows, this synapses connects Area (SqFt) and Bedrooms of the property. So, this neuron is paying attention to properties that maybe: The homes here have a larger Area Sq and have many rooms. These may be homes that are mansions. Looking at the black arrows, this synapses connects all the input functions. So, this neuron is paying attention to properties because all the inputs match the criteria for the price of the property So, all these different neuron communications will predict the price of the property How do Neural Networks Learn? ANN can be categorized by how the data flows from the input node to the output node. B Some of the neural networks are some examples: Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Backpropagation algorithm Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. In simple terms, you can think of the data flowing from the input node to the output node through many different paths in the neural network. Only one path is the correct one that maps the input node to the correct output node. To find this path, the neural network uses a feedback loop, which works as follows: Each node makes a guess about the next node in the path. It checks if the guess was correct. Nodes assign higher weight values to paths that lead to more correct guesses and lower weight values to node paths that lead to incorrect guesses. For the next data point, the nodes make a new prediction using the higher weight paths and then repeat Step 1. Convolutional neural networks The hidden layers in convolutional neural networks perform specific mathematical functions, like summarizing or filtering, called convolutions. They are very useful for image classification because they can extract relevant features from images that are useful for image recognition and classification. The new form is easier to process without losing features that are critical for making a good prediction. Each hidden layer extracts and processes different image features, like edges, color, and depth. Machine Learning in ANNs These ML algorithms help to solve different business problems like Forecasting, Regression, Clustering, Classification, and Associations, etc. Machine learning is divided into mainly four types, which are: Supervised Machine Learning - Ex: Housing Price Prediction. A Supervised machine learning is a technique where we train the machines using the \"labelled\" dataset, and based on the training, the machine predicts the output. In this type of learning, the labelled data specifies that some of the inputs are already mapped to the output. The machine are First trained with the input and corresponding output, and then we ask the machine to predict the output using the test dataset. A classic example will be the example of training a machine with Unsupervised Machine Learning EX: Customer Segmentation and Market Basket Analysis. In unsupervised machine learning, the machine is trained using the unlabeled dataset, and the machine predicts the output without any supervision. In this type, the models are trained with the data that is neither labelled nor classified, and the model acts on that data without any supervision. Unsupervised learning is much similar as a human learns, which makes it closer to the real AI. Semi-Supervised Machine Learning Ex: Lane-Finding on GPS data. It represents the intermediate ground between Supervised and Unsupervised learning algorithms and uses the combination of labelled and unlabeled datasets during the training period. Reinforcement Learning Ex: Drivless Cars. Reinforcement learning works on a feedback-based process, in which an AI agent automatically explore its surrounding by hitting & trail, taking action, learning from experiences, and improving its performance. Agent gets rewarded for each good action and get punished for each bad action; hence the goal of reinforcement learning agent is to maximize the rewards. In reinforcement learning, there is no labelled data like supervised learning, and agents learn from their experiences only. For more on this section, see this javatpoint Link: Click-Link","title":"AI"},{"location":"weekly/AI/ai/#artificial-intelligence","text":"Eventhough AI Machines seem to be a threat to humanity, yet, they cannot beat the power of human intelligence, Machines will never beat the creative or imaginative ability of humans. Always remember, humans created the machine.","title":"Artificial Intelligence"},{"location":"weekly/AI/ai/#what-is-ai","text":"\"It is a branch of computer science by which we can create intelligent machines which can behave like a human, think like humans, and able to make decisions.\" setup google codeLab Open-Link Theres always been this lask of understanding between machine learning and artificial intelligence. Put in context, artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience.","title":"What is AI?"},{"location":"weekly/AI/ai/#artificial-neural-network-ann","text":"Artificial Neural Networks the most important part of the deep learning model, they are the brain inspired deep learning tool which replicate the way humans learn. The Neuron In the Human Body, the Neuron is a part of the brain and nervous system. The three broad functions of the CNS are: To take in sensory information process information, and To send out motor signals. But Now How do Neurons Connect? Neurons communicate with each other by sending chemicals, called neurotransmitters, across a tiny space called a synapse, between the axons and dendrites of nearby neurons. So, now that we have that understanding of a human neuron, and how the neurons connect, the goal of AI was to replicate the way the human brain learns. In another sence, create machines that would have the ability to learn, adapt, and make decisions.","title":"Artificial Neural Network (ANN)"},{"location":"weekly/AI/ai/#neurons-in-ai","text":"As seen in the diagram below, the Input Values are independent Variables of [1, 2, and m]. These variables need to be standardized to make sure all variables contribute evenly to a scale when items are added together, or to make it easier to interpret results of a regression or other analysis. The output Value can be: continuous, binary or a catagorical variable. Neural network is made of interconnected neurons. Each of them is characterized by its weight, bias, and activation function.","title":"Neurons in AI"},{"location":"weekly/AI/ai/#weight","text":"Weight is the parameter within a neural network that transforms input data within the network's hidden layers.","title":"Weight"},{"location":"weekly/AI/ai/#bias","text":"Bias in AI is when the machine gives consistently different outputs for one group of people compared to another. Typically these bias outputs follow classical societal biases like race, gender, biological sex, nationality or age.","title":"Bias"},{"location":"weekly/AI/ai/#neural-network-activation-function","text":"An Activation Function is what makes the decision, if a neuron should be activated or not. This means that an ctivation Function is a mathematical \u201cgate\u201d that decide whether the neuron\u2019s input to the network is important or not in the process of prediction using simpler mathematical operations. Types of Activation Functions Threshold Activation Function is a commonly used activation function.It gives 1 as output of the input is either 0 or positive. If the input is negative, it gives 0 as output. Sigmoid Activation Function Sigmoid Function is the most commonly used activation function in neural networks. It is the go-to model when it comes to predicting the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid has the ability to takes any real value as input and outputs values in the range of 0 to 1. Rectifier Activation Function is the most popularly used activation function in the areas of convolutional neural networks and deep learning. It is of the form: Hyperbolic Tangent Activation Function It is bipolar in nature. Similar to sigmoid, except a different output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.","title":"Neural Network Activation Function?"},{"location":"weekly/AI/ai/#how-do-neural-networks-work","text":"Example Case: Real Estate Price prediction using a trained NN. Looking at the Red arrows, this synapses connects Age, Area, bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Newer, That have at least 3 bedrooms or more, And which the homes have a larger area. Looking at the Blue arrows, this synapses connects Schools Ratings and bedroom to a neuron. So, this neuron is paying attention to properties that maybe: Families with Kids in school Homes that have more bedrooms for the parents and kids. Looking at the Green arrows, this synapses connects Just the Age of the property. So, this neuron is paying attention to properties that maybe: Newly built, or Properties that are historic, valuable properties, like above 100years old. A rectifier function can be used here, stating that, the moment the property turns 100 years, the rectifier functions kicks from 0 to 1. Looking at the Yellow arrows, this synapses connects Area (SqFt) and Bedrooms of the property. So, this neuron is paying attention to properties that maybe: The homes here have a larger Area Sq and have many rooms. These may be homes that are mansions. Looking at the black arrows, this synapses connects all the input functions. So, this neuron is paying attention to properties because all the inputs match the criteria for the price of the property So, all these different neuron communications will predict the price of the property","title":"How do Neural Networks Work?"},{"location":"weekly/AI/ai/#how-do-neural-networks-learn","text":"ANN can be categorized by how the data flows from the input node to the output node. B Some of the neural networks are some examples: Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Backpropagation algorithm Artificial neural networks learn continuously by using corrective feedback loops to improve their predictive analytics. In simple terms, you can think of the data flowing from the input node to the output node through many different paths in the neural network. Only one path is the correct one that maps the input node to the correct output node. To find this path, the neural network uses a feedback loop, which works as follows: Each node makes a guess about the next node in the path. It checks if the guess was correct. Nodes assign higher weight values to paths that lead to more correct guesses and lower weight values to node paths that lead to incorrect guesses. For the next data point, the nodes make a new prediction using the higher weight paths and then repeat Step 1. Convolutional neural networks The hidden layers in convolutional neural networks perform specific mathematical functions, like summarizing or filtering, called convolutions. They are very useful for image classification because they can extract relevant features from images that are useful for image recognition and classification. The new form is easier to process without losing features that are critical for making a good prediction. Each hidden layer extracts and processes different image features, like edges, color, and depth.","title":"How do Neural Networks Learn?"},{"location":"weekly/AI/ai/#machine-learning-in-anns","text":"These ML algorithms help to solve different business problems like Forecasting, Regression, Clustering, Classification, and Associations, etc. Machine learning is divided into mainly four types, which are: Supervised Machine Learning - Ex: Housing Price Prediction. A Supervised machine learning is a technique where we train the machines using the \"labelled\" dataset, and based on the training, the machine predicts the output. In this type of learning, the labelled data specifies that some of the inputs are already mapped to the output. The machine are First trained with the input and corresponding output, and then we ask the machine to predict the output using the test dataset. A classic example will be the example of training a machine with Unsupervised Machine Learning EX: Customer Segmentation and Market Basket Analysis. In unsupervised machine learning, the machine is trained using the unlabeled dataset, and the machine predicts the output without any supervision. In this type, the models are trained with the data that is neither labelled nor classified, and the model acts on that data without any supervision. Unsupervised learning is much similar as a human learns, which makes it closer to the real AI. Semi-Supervised Machine Learning Ex: Lane-Finding on GPS data. It represents the intermediate ground between Supervised and Unsupervised learning algorithms and uses the combination of labelled and unlabeled datasets during the training period. Reinforcement Learning Ex: Drivless Cars. Reinforcement learning works on a feedback-based process, in which an AI agent automatically explore its surrounding by hitting & trail, taking action, learning from experiences, and improving its performance. Agent gets rewarded for each good action and get punished for each bad action; hence the goal of reinforcement learning agent is to maximize the rewards. In reinforcement learning, there is no labelled data like supervised learning, and agents learn from their experiences only. For more on this section, see this javatpoint Link: Click-Link","title":"Machine Learning in ANNs"},{"location":"weekly/Azure/azure/","text":"Azure Free azure account and go to the postal link: Free account-Link Portal-Link Whats hierarchy Azure provides four levels of management: management groups -> Subscriptions -> Resource Groups -> Resources Management groups help you manage access, policy, and compliance for multiple subscriptions. All subscriptions in a management group automatically inherit the conditions that are applied to the management group. Subscriptions associate user accounts with the resources that they create. Resource groups are logical containers where you can deploy and manage Azure resources like web apps, databases, and storage accounts. Resources are instances of services that you can create, such as virtual machines, storage, and SQL databases. Set a budget Azure Storage types Azure Storage offers five core services: Blob Storage helps you create data lakes for your analytics needs, and provides storage to build powerful cloud-native and mobile apps. File Storage : Managed file shares for cloud or on-premises deployments. Queue Storage : A messaging store for reliable messaging between application components. Table Storage : A NoSQL store for schemaless storage of structured data. Managed Disk Storage : Block-level storage volumes for Azure VMs. Azure Elastic SAN (preview) : A fully integrated solution that simplifies deploying, scaling, managing, and configuring a SAN in Azure. Azure also offers a list of SQL Data and non-SQL Databases. For More on Azure Storage, see link: Azure Storage Link Azure Networking Service For More on Azure networking, see link: Azure networking Link Azure and Terraform Azure provder Terraform link: Terraform link Azure authentication from the terminal az login az account list az account list --query [*].[name,id] In azure terminal - bash, - Create an AD service principle for RBAC using below command, - Next, go to subscriptions and get the subscription ID. az ad sp create-for-rbac -n \"Terraform-Soso\" --role=\"Contributor\" --scopes=\"/subscriptions/b965a029-32b4-4671-b283-350344c89091\" Go to azure app registration and see that it was created. Copy the info created. In my case: { \"appId\": \"297ec591-b16e-42ce-9e03-1ebd725aa3f9\", \"displayName\": \"Terraform-Soso\", \"password\": \"FZG8Q~-uPKCxRNgIg8JVAKYO2xm4j..S1SpFoaxg\", \"tenant\": \"054da2e5-2fbf-483f-961e-a3b2839bd53c\" } Azure Active Directory Azure AD part of Microsoft Entra , is an identity and Access Management service that provides single sign-on, multifactor authentication, and conditional access to guard against 99.9 percent of cybersecurity attacks. Accounts and subscriptions Account: Subscription: Tenant: Resource Groups: users There are three types of user accounts that you can have in Azure AD: federated synchronized cloud Optional: Create a new tenant for your organization For more info see: To create a new tenant After creating tenant, it comes with nothin installed, so I will have to add subscriptions... Note : You can switch between tenants Storage Storage Access Control (IAM)","title":"Azure"},{"location":"weekly/Azure/azure/#azure","text":"Free azure account and go to the postal link: Free account-Link Portal-Link","title":"Azure"},{"location":"weekly/Azure/azure/#whats-hierarchy","text":"Azure provides four levels of management: management groups -> Subscriptions -> Resource Groups -> Resources Management groups help you manage access, policy, and compliance for multiple subscriptions. All subscriptions in a management group automatically inherit the conditions that are applied to the management group. Subscriptions associate user accounts with the resources that they create. Resource groups are logical containers where you can deploy and manage Azure resources like web apps, databases, and storage accounts. Resources are instances of services that you can create, such as virtual machines, storage, and SQL databases.","title":"Whats hierarchy"},{"location":"weekly/Azure/azure/#set-a-budget","text":"","title":"Set a budget"},{"location":"weekly/Azure/azure/#azure-storage-types","text":"Azure Storage offers five core services: Blob Storage helps you create data lakes for your analytics needs, and provides storage to build powerful cloud-native and mobile apps. File Storage : Managed file shares for cloud or on-premises deployments. Queue Storage : A messaging store for reliable messaging between application components. Table Storage : A NoSQL store for schemaless storage of structured data. Managed Disk Storage : Block-level storage volumes for Azure VMs. Azure Elastic SAN (preview) : A fully integrated solution that simplifies deploying, scaling, managing, and configuring a SAN in Azure. Azure also offers a list of SQL Data and non-SQL Databases. For More on Azure Storage, see link: Azure Storage Link","title":"Azure Storage types"},{"location":"weekly/Azure/azure/#azure-networking-service","text":"For More on Azure networking, see link: Azure networking Link","title":"Azure Networking Service"},{"location":"weekly/Azure/azure/#azure-and-terraform","text":"Azure provder Terraform link: Terraform link","title":"Azure and Terraform"},{"location":"weekly/Azure/azure/#azure-authentication-from-the-terminal","text":"az login az account list az account list --query [*].[name,id] In azure terminal - bash, - Create an AD service principle for RBAC using below command, - Next, go to subscriptions and get the subscription ID. az ad sp create-for-rbac -n \"Terraform-Soso\" --role=\"Contributor\" --scopes=\"/subscriptions/b965a029-32b4-4671-b283-350344c89091\" Go to azure app registration and see that it was created. Copy the info created. In my case: { \"appId\": \"297ec591-b16e-42ce-9e03-1ebd725aa3f9\", \"displayName\": \"Terraform-Soso\", \"password\": \"FZG8Q~-uPKCxRNgIg8JVAKYO2xm4j..S1SpFoaxg\", \"tenant\": \"054da2e5-2fbf-483f-961e-a3b2839bd53c\" }","title":"Azure authentication from the terminal"},{"location":"weekly/Azure/azure/#azure-active-directory","text":"Azure AD part of Microsoft Entra , is an identity and Access Management service that provides single sign-on, multifactor authentication, and conditional access to guard against 99.9 percent of cybersecurity attacks.","title":"Azure Active Directory"},{"location":"weekly/Azure/azure/#accounts-and-subscriptions","text":"Account: Subscription: Tenant: Resource Groups:","title":"Accounts and subscriptions"},{"location":"weekly/Azure/azure/#users","text":"There are three types of user accounts that you can have in Azure AD: federated synchronized cloud","title":"users"},{"location":"weekly/Azure/azure/#optional-create-a-new-tenant-for-your-organization","text":"For more info see: To create a new tenant After creating tenant, it comes with nothin installed, so I will have to add subscriptions... Note : You can switch between tenants","title":"Optional: Create a new tenant for your organization"},{"location":"weekly/Azure/azure/#storage","text":"","title":"Storage"},{"location":"weekly/Azure/azure/#storage-access-control-iam","text":"","title":"Storage Access Control (IAM)"},{"location":"weekly/CICD/cicd/","text":"Install servers Install Individuals servers for: - Nexus - Sonarqube - Jenkins After Installing Servers Add Sonar security group (All Traffic) in the Jenkins server security Group in AWS Console. Also, Jenkins security group (All Traffic) in the Sonar server security Group in AWS Console. Nexus Centos 7 (Amazon Market place) TCP Port 8081 from MyIP and Jenkins-SG #!/bin/bash yum install java-1.8.0-openjdk.x86_64 wget -y mkdir -p /opt/nexus/ mkdir -p /tmp/nexus/ cd /tmp/nexus/ NEXUSURL=\"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\" wget $NEXUSURL -O nexus.tar.gz EXTOUT=`tar xzvf nexus.tar.gz` NEXUSDIR=`echo $EXTOUT | cut -d '/' -f1` rm -rf /tmp/nexus/nexus.tar.gz rsync -avzh /tmp/nexus/ /opt/nexus/ useradd nexus chown -R nexus.nexus /opt/nexus cat <<EOT>> /etc/systemd/system/nexus.service [Unit] Description=nexus service After=network.target [Service] Type=forking LimitNOFILE=65536 ExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start ExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop User=nexus Restart=on-abort [Install] WantedBy=multi-user.target EOT echo 'run_as_user=\"nexus\"' > /opt/nexus/$NEXUSDIR/bin/nexus.rc systemctl daemon-reload systemctl start nexus systemctl enable nexus Configure Nexus Login: - username: admin - Get Password: cat /opt/nexus/sonatype-work/nexus3/admin.password Check and start the nexus service sudo systemctl status nexus SonarQube Ubuntu VERSION=\"18.04\" TCP Port 9000 TCP Port 80 from MyIP and Jenkins-SG Sonar Installation Script #!/bin/bash cp /etc/sysctl.conf /root/sysctl.conf_backup cat <<EOT> /etc/sysctl.conf vm.max_map_count=262144 fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 EOT cp /etc/security/limits.conf /root/sec_limit.conf_backup cat <<EOT> /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 409 EOT sudo apt-get update -y sudo apt-get install openjdk-11-jdk -y sudo update-alternatives --config java java -version sudo apt update wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' sudo apt install postgresql postgresql-contrib -y #sudo -u postgres psql -c \"SELECT version();\" sudo systemctl enable postgresql.service sudo systemctl start postgresql.service sudo echo \"postgres:admin123\" | chpasswd runuser -l postgres -c \"createuser sonar\" sudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\" sudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\" sudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\" systemctl restart postgresql #systemctl status -l postgresql netstat -tulpena | grep postgres sudo mkdir -p /sonarqube/ cd /sonarqube/ sudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip sudo apt-get install zip -y sudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/ sudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube sudo groupadd sonar sudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar sudo chown sonar:sonar /opt/sonarqube/ -R cp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup cat <<EOT> /opt/sonarqube/conf/sonar.properties sonar.jdbc.username=sonar sonar.jdbc.password=admin123 sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.web.host=0.0.0.0 sonar.web.port=9000 sonar.web.javaAdditionalOpts=-server sonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError sonar.log.level=INFO sonar.path.logs=logs EOT cat <<EOT> /etc/systemd/system/sonarqube.service [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target EOT systemctl daemon-reload systemctl enable sonarqube.service #systemctl start sonarqube.service #systemctl status -l sonarqube.service apt-get install nginx -y rm -rf /etc/nginx/sites-enabled/default rm -rf /etc/nginx/sites-available/default cat <<EOT> /etc/nginx/sites-available/sonarqube server{ listen 80; server_name sonarqube.groophy.in; access_log /var/log/nginx/sonar.access.log; error_log /var/log/nginx/sonar.error.log; proxy_buffers 16 64k; proxy_buffer_size 128k; location / { proxy_pass http://127.0.0.1:9000; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_set_header Host \\$host; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto http; } } EOT ln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube systemctl enable nginx.service #systemctl restart nginx.service sudo ufw allow 80,9000,9001/tcp echo \"System reboot in 30 sec\" sleep 30 reboot Configure Sonar Check and start the sonarqube service sudo systemctl status sonarqube Login: - username: admin - Password: admin Slack SetUp Slack Steps: - A workspace: sosotech - Create a Channel(s): sosochannel1 - Add teammates to the channel - Add Jenkins credentials: sososlacktoken Get the Jenkins app from the : Slack App Directory Add to Slack and select the channel Add the CI Jenkins Integration: Im using #sosochannel1 Copy the Token in Step 3 and go create a Slack Credential In Jenkins credentials Scroll docn and save. No Go to jenkins and configure credentials called: sososlacktoken Jenkins Ubuntu VERSION=\"20.04.6 LTS TCP Port 8080 from Anywhere - IPv4 and IPv6 Install If you have any issues, then: curl the IP address if you had any issues. curl http://[your-put-IP]/latest/user-data LIKE SO: --> curl http://56.22.1.2/latest/user-data Also refer to site to update your code: Optional-Link Ubuntu installation script for VERSION=\"20.04.6 LTS #!/bin/bash sudo apt update sudo apt install openjdk-11-jdk -y sudo apt install maven -y curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc > /dev/null echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list > /dev/null sudo apt-get update sudo apt-get install jenkins -y Check and start the jenkins service sudo systemctl status jenkins sudo systemctl status jenkins java -version whereis git Get Jenkins Password sudo cat /var/lib/jenkins/secrets/initialAdminPassword INSTALL JDK8 On the Jenkins Server In Server terminal, Install Maven and JDK8 sudo apt update sudo apt install openjdk-8-jdk -y Get JDK8 Path from the Jenkins Server CD to ROOT and Copy the java path. Copy the path in a node Path for use in Jenkins Global Tool Configuration. /usr/lib/jvm/java-1.8.0-openjdk-amd64 . See below Photo sudo su - ls /usr/lib/jvm INSTALL MAVEN On the Jenkins Server Go to the Maven site and get latest version: Right-click and copy .tar link sudo su - cd /opt apt install wget wget https://dlcdn.apache.org/maven/maven-3/3.9.1/binaries/apache-maven-3.9.1-bin.tar.gz tar -xvzf apache-maven-3.9.1-bin.tar.gz mv apache-maven-3.9.1 maven rm -rf apache-maven-3.9.1-bin.tar.gz cd .. Install Docker on the Jenkins Server see link: Reference link sudo su - sudo apt update -y sudo apt-get install \\ ca-certificates \\ curl \\ gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg echo \\ \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update -y sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin sudo systemctl status docker docker -v Add Jenkins User to the docker group id jenkins usermod -a -G docker jenkins Install AWSCLI in Jenkins Server sudo apt install awscli -y NOW RESTART YOUR JENKINS SERVER 1. Install Jenkins plugins Dashboard --> Manage Jenkins --> Plugin Manager Pipeline Maven Integration Pipeline Utility Steps Github Integration Plugin SonarQube Scanner Slack Notification Build Timestamp docker pipeline docker Amazon ECR CloudBees Docker Build and Publish Amazon Web Services SDK :: All 2. Global Tool Configuration Configure CI [Git, Maven, JVM, SonarQube Scanner ] on Jenkins GUI . In the Jenkins UI --> manage Jenkins --> Global Tool Configuration [save] Services Configured Names JDK SosoJDK8 git Git MAVEN SOSOMAVEN3 SonarQube Scanner sososonar4.7 See the Maven, Git and JDK configuration images See the SonarQube Configuration image 3. Configure Credential Navigate to: Jenkins UI --> manage Jenkins --> Manage Credentials --> System --> Global credentials Services Credential ID UserName/Password/secret-text DockerHub sosodockertoken Username-Password AWS - ECR User sosoawstoken UserName/Password MAVEN SonarQube sososonartoken secret-text Slack sososlacktoken secret-text build-trigger sososshtrigger SSH Username with Private Key Jenkins-Slave(KOPS) sosokopstoken SSH Username with Private Key In the Jenkins UI: Configure the following credentials AWS DockerHub --> (generate Token) My account --> security --> secret text k8s Config sonarqube --> (generate Token) My account --> security --> secret text Configure AWS USER Credentials for ECR Create credentials for the username and password for the saved Jenkins user. Configure Dockerhub Credential(Token) Log into your dockerhub account and create username-password --> security: LINK Configure SonarQube Credential(Token) Login to the sonarQube UI, go to Myaccount --> security create a Token Add a Token Add the Token as Credential To jenkins Global credentials Configure SLACK Credential Configure ssh-trigger for Build trigger Configure Jenkins-Slave Kops Ubuntu user use the KOPS instance private IP as the Host use the Remote directory /opt/jenkins-slave; will create this later. Configure AWS Credential 4. Configure Systems Navigate to: Jenkins UI --> manage Jenkins --> Configure System Services Configured Names xxx xxx Slack sosotech SonarQube Servers sososonar Configure SonarQube Server Configure the sonar server in Jenkins uring the SonarQube Public IP and the sonar credentials. For quality gate and analysis, see the sonarQube section NOTE : Don't forget to add webhooks Configure TimeStamp change the timestamp pattern yy-MM-dd_HH-mm as seen in the image: Configure Slack Notification configure the folloring : - A workspace: sosotech - A Channel(s): sosochannel1 Jenkins Jobs There are some Jenkins Jobs Demo'd here, like Pipeline, Freestyle: Freestyle Project Use this repo: https://github.com/sosotechnologies/cicd-maven.git It's a public repo, so credentials are optional See the image to guide you during setup. Pipeline There are 2 Options to use here: - Pipeline script - Pipeline script from SCM - after build check path: /var/lib/jenkins/workspace Some Sample Pipeline Scripts: Pipeline1: Jenkins, Maven simple pipeline*** pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages { stage('Fetch code') { steps { git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build'){ steps{ sh 'mvn install -DskipTests' } post { success { echo 'I think the archieve is All Good...' archiveArtifacts artifacts: '**/target/*.war' } } } stage('UNIT TEST') { steps{ sh 'mvn test' } } } } Popeline 2: Jenkins, Maven, Checkstyle, Sonar-Analysis and Quality Gate - pipeline Add Sonar security gtoup in the Jenkins server security Group in AWS Console. Also, Jenkins security gtoup in the Sonar server security Group in AWS Console. Note: In this step os the sonar pipeline you will see this Line of code: steps { withSonarQubeEnv('sososonar') { The sososonar represents the name of the SonarQube Servers in Configure Syatem. pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages{ stage('Fetch code') { steps{ git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build') { steps { sh 'mvn clean install -DskipTests' } post { success { echo \"I think the archieve is All Good\" archiveArtifacts artifacts: '**/*.war' } } } stage('Test'){ steps { sh 'mvn test' } } stage('Checkstyle Analysis'){ steps { sh 'mvn checkstyle:checkstyle' } } stage('Sonar Analysis') { environment { scannerHome = tool 'sososonar4.7' } steps { withSonarQubeEnv('sososonar') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=sosotech \\ -Dsonar.projectName=sosotech \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } } } stage(\"Quality Gate\") { steps { timeout(time: 1, unit: 'HOURS') { waitForQualityGate abortPipeline: true } } } } } Pipeline 3: Implementing DOCKER ECR Create an AWS-ECR repo called: soso-repository Build image of webapp and puch to ECR pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } environment { JenkinsECRCredential = 'ecr:us-east-1:sosoawstoken' sosoappRegistry = \"088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository\" sosotechRegistry = \"https://088789840359.dkr.ecr.us-east-1.amazonaws.com\" } stages { stage('Fetch code'){ steps { git branch: 'master', url: 'https://github.com/sosotechnologies/cicd-maven-jenkins-ecr.git' } } stage('Test'){ steps { sh 'mvn test' } } stage('Build App Image') { steps { script { dockerImage = docker.build( sosoappRegistry + \":$BUILD_NUMBER\", \"./sosotech-Dockerfiles/sosoapp/multistagebuild/\") } } } stage('Upload App Image') { steps{ script { docker.withRegistry( sosotechRegistry, JenkinsECRCredential ) { dockerImage.push(\"$BUILD_NUMBER\") dockerImage.push('latest') } } } } } } ``` ##### Popeline 4: Jenkins, Maven, Checkstyle, Docker, Sonar-Analysis and Quality Gate - pipeline ```Jenkinsfile pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } environment { JenkinsECRCredential = 'ecr:us-east-1:sosoawstoken' sosoappRegistry = \"088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository\" sosotechRegistry = \"https://088789840359.dkr.ecr.us-east-1.amazonaws.com\" } stages { stage('Fetch code'){ steps { git branch: 'master', url: 'https://github.com/sosotechnologies/cicd-maven-jenkins-ecr.git' } } stage('Test'){ steps { sh 'mvn test' } } stage ('CODE ANALYSIS WITH CHECKSTYLE'){ steps { sh 'mvn checkstyle:checkstyle' } post { success { echo 'Generated Analysis Result' } } } stage('build && SonarQube analysis') { environment { scannerHome = tool 'sososonar4.7' } steps { withSonarQubeEnv('sososonar') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=sosotech \\ -Dsonar.projectName=sosotech \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } } } stage(\"Quality Gate\") { steps { timeout(time: 1, unit: 'HOURS') { waitForQualityGate abortPipeline: true } } } stage('Build App Image') { steps { script { dockerImage = docker.build( sosoappRegistry + \":$BUILD_NUMBER\", \"./sosotech-Dockerfiles/sosoapp/multistagebuild/\") } } } stage('Upload App Image') { steps{ script { docker.withRegistry( sosotechRegistry, JenkinsECRCredential ) { dockerImage.push(\"$BUILD_NUMBER\") dockerImage.push('latest') } } } } } } Pipeline 5: FULL Pipeline*** def COLOR_MAP = [ 'SUCCESS': 'good', 'FAILURE': 'danger', ] pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages{ stage('Fetch code') { steps{ git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build') { steps { sh 'mvn clean install -DskipTests' } post { success { echo \"Now Archiving.\" archiveArtifacts artifacts: '**/*.war' } } } stage('Test'){ steps { sh 'mvn test' } } stage('Checkstyle Analysis'){ steps { sh 'mvn checkstyle:checkstyle' } } stage('Sonar Analysis') { environment { scannerHome = tool 'sososonar4.7' } steps { withSonarQubeEnv('sososonar') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=sosotech \\ -Dsonar.projectName=sosotech \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } } } stage(\"Quality Gate\") { steps { timeout(time: 1, unit: 'HOURS') { waitForQualityGate abortPipeline: true } } } } post { always { echo 'Slack Notifications.' slackSend channel: '#sosochannel1', color: COLOR_MAP[currentBuild.currentResult], message: \"*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \\n More info at: ${env.BUILD_URL}\" } } } Building sosotech documentation site pipeline { agent any environment { JenkinsECRCredential = 'ecr:us-east-1:sosoawstoken' sosoappRegistry = \"088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository\" sosotechRegistry = \"https://088789840359.dkr.ecr.us-east-1.amazonaws.com\" } stages { stage('Fetch code'){ steps { git branch: 'main', url: 'https://github.com/sosotechnologies/docs_docker_io.git' } } stage('Build App Image') { steps { script { dockerImage = docker.build( sosoappRegistry + \":$BUILD_NUMBER\", \"./\") } } } stage('Upload App Image') { steps{ script { docker.withRegistry( sosotechRegistry, JenkinsECRCredential ) { dockerImage.push(\"$BUILD_NUMBER\") dockerImage.push('latest') } } } } } } Building Sosotech Node App pipeline { agent any environment { JenkinsECRCredential = 'ecr:us-east-1:sosoawstoken' sosoappRegistry = \"088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository\" sosotechRegistry = \"https://088789840359.dkr.ecr.us-east-1.amazonaws.com\" } stages { stage('Fetch code'){ steps { git branch: 'master', url: 'https://github.com/sosotechnologies/sosojenkins.git' } } stage('Build App Image') { steps { script { dockerImage = docker.build( sosoappRegistry + \":$BUILD_NUMBER\", \"./dockerhub-nodejs\") } } } stage('Upload App Image') { steps{ script { docker.withRegistry( sosotechRegistry, JenkinsECRCredential ) { dockerImage.push(\"$BUILD_NUMBER\") dockerImage.push('mkdocss') } } } } } } Build Triggers Requirements: - Set a New private Git Repo - Set a new ssh Key: ssh-keygen - Get the content of your id_rsa.pub key: cat ~/.ssh/id_rsa.pub - Register the id_rsa.pub in Github SSH and GPC Keys - Get the content of your id_rsa key: cat ~/.ssh/id_rsa - Register the id_rsa key in Jenkins configure credentials. My repo is : git@github.com:sosotechnologies/sosojenkinstriggers.git Add SSH Key to Github Settings Make a dir mysosotriggers : mkdir mysosotriggers hello-world/webapp/src/main/webapp/WEB-INF/ Create a Jenkinsfile in mysosotriggers: cd mysosotriggers git clone git@github.com:sosotechnologies/sosojenkinstriggers.git cd sosojenkinstriggers touch Jenkinsfile vi Jenkinsfile Add this into the Jenkinsfile pipeline { agent any stages { stage('Build') { steps{ sh 'echo \"Looks like the build is Done!\"' } } } } Create a Pipeline Job like in below photo: GitHub webHook Jobs Go to video 163 CICD MicroServices Docker server Ubuntu VERSION=\"20.04.6 LTS Create a New AWS instance. T3 medium During instance creation select option [Allow HTTP traffic from the internet] Storage should be 20Gb Add below script in the user data #!/bin/bash # Install Docker sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release -y curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null # Install Docker-Compose sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io -y sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Add ubuntu user into Docker Group sudo usermod -a -G docker ubuntu Clone the repo : Soso-repo docker-compose build docker-compose up -d For docker, see the official docker section in thus doc If you ever encounter NO SPACE issue when building your image sudo docker image prune -f && sudo docker container prune -f Kubernetes KOPs EKS OpenShift KOPS Ubuntu VERSION=\"20.04.6 LTS Installing kOps on Ubuntu Install Helm on the Ubuntu server install JDK-8 on server Create a directory called jenkins-slave in /opt Create a New(public) GitHub repo: Mine is: My repo INSTALL JDK8 On the Jenkins Server In Server terminal, Install Maven and JDK8 sudo apt update sudo apt install openjdk-8-jdk -y curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64 chmod +x ./kops sudo mv ./kops /usr/local/bin/ Install Python PiP sudo apt -y update sudo apt install python3-pip Install AWSCli sudo apt install awscli -y Install Kubectl curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl Create S3 Bucket aws s3 ls aws s3 mb s3://soso-kops-bucket.local OPTIONAL aws s3api create-bucket \\ --bucket soso-kops-bucket \\ --region us-east-1 Add env variables in bashrc vi .bashrc export NAME=soso-kops-bucket.k8s.local export KOPS_STATE_STORE=s3://soso-kops-bucket.local source .bashrc Generate an sshkeys before creating cluster ssh-keygen Install Cluster preconfig file kops create cluster \\ --name=${NAME} \\ --cloud=aws \\ --zones=us-west-2a \\ --node-size t2.medium \\ --master-size t2.medium \\ --master-count 1 --node-size t2.medium --node-count=1 Edit the cluster name- OPTIONAL kops edit cluster --name ${NAME} Install Cluster kops update cluster --name ${NAME} --yes --admin NOTE: WAIT 10 Mins before Checking Nodes and Validating cluster * Get nodes kubectl get nodes Validate Cluster kops validate cluster --wait 10m Delete cluster - OPTIONAL kops delete cluster --name ${NAME} [OR] kops delete cluster --name ${NAME} --yes Make directory /opt/jenkins-slave and give ownership to ubuntu Add Jenkins SG to Kops and Vice-Versa mkdir jenkins-slave sudo mkdir /opt/jenkins-slave sudo chown ubuntu.ubuntu /opt/jenkins-slave -R Create a new Slave Node in Jenkins CICD Work Flow - Remove my helm and make your own helm directory in same path mkdir cicd-k8s git clone https://github.com/sosotechnologies/cicd-kubernetes-jenkins-pipeline.git cd mkdir helm && cd helm helm create sosotechecharts replace the yaml files in the template with our yaml files in: cici-kubernetes-jenkins-pipeline/kubernetes/soso-app/templates cd cici-kubernetes-jenkins-pipeline/helm/sosotechecharts/templates rm -rf * NOTE : Set the deployment Image as a variable. Name to what ever but take note of the Name, as this name will be passed as a tag when building the Helm Chart' Now install the Chart - cd back into the helm directory and run below command - create a namespace called cicd - helm install --namespace cicd [nameof the chart] --set [the deployment variable]=[built image from dockerhub] kubectl create ns cicd helm install --namespace cicd soso-helm-name --set sosodeploymentvariable=sosotech/sosowebapp:2 helm list --namerpace cicd Delete the Chart helm delete soso-helm-name -n cicd pipeline { agent any /* tools { maven \"maven3\" } */ environment { registry = \"sosotech/docs-repo\" registryCredential = 'sosodockertoken' } stages{ stage('BUILD'){ steps { sh 'mvn clean install -DskipTests' } post { success { echo 'Now Archiving...' archiveArtifacts artifacts: '**/target/*.war' } } } stage('UNIT TEST'){ steps { sh 'mvn test' } } stage('INTEGRATION TEST'){ steps { sh 'mvn verify -DskipUnitTests' } } stage ('CODE ANALYSIS WITH CHECKSTYLE'){ steps { sh 'mvn checkstyle:checkstyle' } post { success { echo 'Generated Analysis Result' } } } stage('Building image') { steps{ script { dockerImage = docker.build registry + \":$BUILD_NUMBER\" } } } stage('Deploy Image') { steps{ script { docker.withRegistry( '', registryCredential ) { dockerImage.push(\"$BUILD_NUMBER\") dockerImage.push('latest') } } } } stage('Remove Unused docker image') { steps{ sh \"docker rmi $registry:$BUILD_NUMBER\" } } stage('CODE ANALYSIS with SONARQUBE') { environment { scannerHome = tool 'mysonarscanner4' } steps { withSonarQubeEnv('sonar-pro') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \\ -Dsonar.projectName=vprofile-repo \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } timeout(time: 10, unit: 'MINUTES') { waitForQualityGate abortPipeline: true } } } stage('Kubernetes Deploy') { agent { label 'KOPS' } steps { sh \"helm upgrade --install --force vproifle-stack helm/sosotechecharts --set appimage=${registry}:${BUILD_NUMBER} --namespace prod\" } } } }","title":"CICD"},{"location":"weekly/CICD/cicd/#install-servers","text":"Install Individuals servers for: - Nexus - Sonarqube - Jenkins After Installing Servers Add Sonar security group (All Traffic) in the Jenkins server security Group in AWS Console. Also, Jenkins security group (All Traffic) in the Sonar server security Group in AWS Console.","title":"Install servers"},{"location":"weekly/CICD/cicd/#nexus","text":"Centos 7 (Amazon Market place) TCP Port 8081 from MyIP and Jenkins-SG #!/bin/bash yum install java-1.8.0-openjdk.x86_64 wget -y mkdir -p /opt/nexus/ mkdir -p /tmp/nexus/ cd /tmp/nexus/ NEXUSURL=\"https://download.sonatype.com/nexus/3/latest-unix.tar.gz\" wget $NEXUSURL -O nexus.tar.gz EXTOUT=`tar xzvf nexus.tar.gz` NEXUSDIR=`echo $EXTOUT | cut -d '/' -f1` rm -rf /tmp/nexus/nexus.tar.gz rsync -avzh /tmp/nexus/ /opt/nexus/ useradd nexus chown -R nexus.nexus /opt/nexus cat <<EOT>> /etc/systemd/system/nexus.service [Unit] Description=nexus service After=network.target [Service] Type=forking LimitNOFILE=65536 ExecStart=/opt/nexus/$NEXUSDIR/bin/nexus start ExecStop=/opt/nexus/$NEXUSDIR/bin/nexus stop User=nexus Restart=on-abort [Install] WantedBy=multi-user.target EOT echo 'run_as_user=\"nexus\"' > /opt/nexus/$NEXUSDIR/bin/nexus.rc systemctl daemon-reload systemctl start nexus systemctl enable nexus","title":"Nexus"},{"location":"weekly/CICD/cicd/#configure-nexus","text":"Login: - username: admin - Get Password: cat /opt/nexus/sonatype-work/nexus3/admin.password Check and start the nexus service sudo systemctl status nexus","title":"Configure Nexus"},{"location":"weekly/CICD/cicd/#sonarqube","text":"Ubuntu VERSION=\"18.04\" TCP Port 9000 TCP Port 80 from MyIP and Jenkins-SG","title":"SonarQube"},{"location":"weekly/CICD/cicd/#sonar-installation","text":"Script #!/bin/bash cp /etc/sysctl.conf /root/sysctl.conf_backup cat <<EOT> /etc/sysctl.conf vm.max_map_count=262144 fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 EOT cp /etc/security/limits.conf /root/sec_limit.conf_backup cat <<EOT> /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 409 EOT sudo apt-get update -y sudo apt-get install openjdk-11-jdk -y sudo update-alternatives --config java java -version sudo apt update wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' sudo apt install postgresql postgresql-contrib -y #sudo -u postgres psql -c \"SELECT version();\" sudo systemctl enable postgresql.service sudo systemctl start postgresql.service sudo echo \"postgres:admin123\" | chpasswd runuser -l postgres -c \"createuser sonar\" sudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\" sudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\" sudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\" systemctl restart postgresql #systemctl status -l postgresql netstat -tulpena | grep postgres sudo mkdir -p /sonarqube/ cd /sonarqube/ sudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip sudo apt-get install zip -y sudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/ sudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube sudo groupadd sonar sudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar sudo chown sonar:sonar /opt/sonarqube/ -R cp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup cat <<EOT> /opt/sonarqube/conf/sonar.properties sonar.jdbc.username=sonar sonar.jdbc.password=admin123 sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.web.host=0.0.0.0 sonar.web.port=9000 sonar.web.javaAdditionalOpts=-server sonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError sonar.log.level=INFO sonar.path.logs=logs EOT cat <<EOT> /etc/systemd/system/sonarqube.service [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target EOT systemctl daemon-reload systemctl enable sonarqube.service #systemctl start sonarqube.service #systemctl status -l sonarqube.service apt-get install nginx -y rm -rf /etc/nginx/sites-enabled/default rm -rf /etc/nginx/sites-available/default cat <<EOT> /etc/nginx/sites-available/sonarqube server{ listen 80; server_name sonarqube.groophy.in; access_log /var/log/nginx/sonar.access.log; error_log /var/log/nginx/sonar.error.log; proxy_buffers 16 64k; proxy_buffer_size 128k; location / { proxy_pass http://127.0.0.1:9000; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_set_header Host \\$host; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto http; } } EOT ln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube systemctl enable nginx.service #systemctl restart nginx.service sudo ufw allow 80,9000,9001/tcp echo \"System reboot in 30 sec\" sleep 30 reboot","title":"Sonar Installation"},{"location":"weekly/CICD/cicd/#configure-sonar","text":"Check and start the sonarqube service sudo systemctl status sonarqube Login: - username: admin - Password: admin","title":"Configure Sonar"},{"location":"weekly/CICD/cicd/#slack","text":"SetUp Slack Steps: - A workspace: sosotech - Create a Channel(s): sosochannel1 - Add teammates to the channel - Add Jenkins credentials: sososlacktoken Get the Jenkins app from the : Slack App Directory Add to Slack and select the channel Add the CI Jenkins Integration: Im using #sosochannel1 Copy the Token in Step 3 and go create a Slack Credential In Jenkins credentials Scroll docn and save. No Go to jenkins and configure credentials called: sososlacktoken","title":"Slack"},{"location":"weekly/CICD/cicd/#jenkins","text":"Ubuntu VERSION=\"20.04.6 LTS TCP Port 8080 from Anywhere - IPv4 and IPv6","title":"Jenkins"},{"location":"weekly/CICD/cicd/#install","text":"If you have any issues, then: curl the IP address if you had any issues. curl http://[your-put-IP]/latest/user-data LIKE SO: --> curl http://56.22.1.2/latest/user-data Also refer to site to update your code: Optional-Link Ubuntu installation script for VERSION=\"20.04.6 LTS #!/bin/bash sudo apt update sudo apt install openjdk-11-jdk -y sudo apt install maven -y curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc > /dev/null echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list > /dev/null sudo apt-get update sudo apt-get install jenkins -y Check and start the jenkins service sudo systemctl status jenkins sudo systemctl status jenkins java -version whereis git Get Jenkins Password sudo cat /var/lib/jenkins/secrets/initialAdminPassword INSTALL JDK8 On the Jenkins Server In Server terminal, Install Maven and JDK8 sudo apt update sudo apt install openjdk-8-jdk -y Get JDK8 Path from the Jenkins Server CD to ROOT and Copy the java path. Copy the path in a node Path for use in Jenkins Global Tool Configuration. /usr/lib/jvm/java-1.8.0-openjdk-amd64 . See below Photo sudo su - ls /usr/lib/jvm INSTALL MAVEN On the Jenkins Server Go to the Maven site and get latest version: Right-click and copy .tar link sudo su - cd /opt apt install wget wget https://dlcdn.apache.org/maven/maven-3/3.9.1/binaries/apache-maven-3.9.1-bin.tar.gz tar -xvzf apache-maven-3.9.1-bin.tar.gz mv apache-maven-3.9.1 maven rm -rf apache-maven-3.9.1-bin.tar.gz cd .. Install Docker on the Jenkins Server see link: Reference link sudo su - sudo apt update -y sudo apt-get install \\ ca-certificates \\ curl \\ gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg echo \\ \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update -y sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin sudo systemctl status docker docker -v Add Jenkins User to the docker group id jenkins usermod -a -G docker jenkins Install AWSCLI in Jenkins Server sudo apt install awscli -y NOW RESTART YOUR JENKINS SERVER","title":"Install"},{"location":"weekly/CICD/cicd/#1-install-jenkins-plugins","text":"Dashboard --> Manage Jenkins --> Plugin Manager Pipeline Maven Integration Pipeline Utility Steps Github Integration Plugin SonarQube Scanner Slack Notification Build Timestamp docker pipeline docker Amazon ECR CloudBees Docker Build and Publish Amazon Web Services SDK :: All","title":"1. Install Jenkins plugins"},{"location":"weekly/CICD/cicd/#2-global-tool-configuration","text":"Configure CI [Git, Maven, JVM, SonarQube Scanner ] on Jenkins GUI . In the Jenkins UI --> manage Jenkins --> Global Tool Configuration [save] Services Configured Names JDK SosoJDK8 git Git MAVEN SOSOMAVEN3 SonarQube Scanner sososonar4.7 See the Maven, Git and JDK configuration images See the SonarQube Configuration image","title":"2. Global Tool Configuration"},{"location":"weekly/CICD/cicd/#3-configure-credential","text":"Navigate to: Jenkins UI --> manage Jenkins --> Manage Credentials --> System --> Global credentials Services Credential ID UserName/Password/secret-text DockerHub sosodockertoken Username-Password AWS - ECR User sosoawstoken UserName/Password MAVEN SonarQube sososonartoken secret-text Slack sososlacktoken secret-text build-trigger sososshtrigger SSH Username with Private Key Jenkins-Slave(KOPS) sosokopstoken SSH Username with Private Key In the Jenkins UI: Configure the following credentials AWS DockerHub --> (generate Token) My account --> security --> secret text k8s Config sonarqube --> (generate Token) My account --> security --> secret text","title":"3. Configure Credential"},{"location":"weekly/CICD/cicd/#configure-aws-user-credentials-for-ecr","text":"Create credentials for the username and password for the saved Jenkins user.","title":"Configure AWS USER Credentials for ECR"},{"location":"weekly/CICD/cicd/#configure-dockerhub-credentialtoken","text":"Log into your dockerhub account and create username-password --> security: LINK","title":"Configure Dockerhub Credential(Token)"},{"location":"weekly/CICD/cicd/#configure-sonarqube-credentialtoken","text":"Login to the sonarQube UI, go to Myaccount --> security create a Token Add a Token Add the Token as Credential To jenkins Global credentials","title":"Configure SonarQube Credential(Token)"},{"location":"weekly/CICD/cicd/#configure-slack-credential","text":"","title":"Configure SLACK Credential"},{"location":"weekly/CICD/cicd/#configure-ssh-trigger-for-build-trigger","text":"","title":"Configure ssh-trigger for Build trigger"},{"location":"weekly/CICD/cicd/#configure-jenkins-slave-kops-ubuntu-user","text":"use the KOPS instance private IP as the Host use the Remote directory /opt/jenkins-slave; will create this later.","title":"Configure Jenkins-Slave Kops Ubuntu user"},{"location":"weekly/CICD/cicd/#configure-aws-credential","text":"","title":"Configure AWS Credential"},{"location":"weekly/CICD/cicd/#4-configure-systems","text":"Navigate to: Jenkins UI --> manage Jenkins --> Configure System Services Configured Names xxx xxx Slack sosotech SonarQube Servers sososonar","title":"4. Configure Systems"},{"location":"weekly/CICD/cicd/#configure-sonarqube-server","text":"Configure the sonar server in Jenkins uring the SonarQube Public IP and the sonar credentials. For quality gate and analysis, see the sonarQube section NOTE : Don't forget to add webhooks","title":"Configure SonarQube Server"},{"location":"weekly/CICD/cicd/#configure-timestamp","text":"change the timestamp pattern yy-MM-dd_HH-mm as seen in the image:","title":"Configure TimeStamp"},{"location":"weekly/CICD/cicd/#configure-slack-notification","text":"configure the folloring : - A workspace: sosotech - A Channel(s): sosochannel1","title":"Configure Slack Notification"},{"location":"weekly/CICD/cicd/#jenkins-jobs","text":"There are some Jenkins Jobs Demo'd here, like Pipeline, Freestyle:","title":"Jenkins Jobs"},{"location":"weekly/CICD/cicd/#freestyle-project","text":"Use this repo: https://github.com/sosotechnologies/cicd-maven.git It's a public repo, so credentials are optional See the image to guide you during setup.","title":"Freestyle Project"},{"location":"weekly/CICD/cicd/#pipeline","text":"There are 2 Options to use here: - Pipeline script - Pipeline script from SCM - after build check path: /var/lib/jenkins/workspace Some Sample Pipeline Scripts:","title":"Pipeline"},{"location":"weekly/CICD/cicd/#pipeline1-jenkins-maven-simple-pipeline","text":"pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages { stage('Fetch code') { steps { git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build'){ steps{ sh 'mvn install -DskipTests' } post { success { echo 'I think the archieve is All Good...' archiveArtifacts artifacts: '**/target/*.war' } } } stage('UNIT TEST') { steps{ sh 'mvn test' } } } }","title":"Pipeline1: Jenkins, Maven simple pipeline***"},{"location":"weekly/CICD/cicd/#popeline-2-jenkins-maven-checkstyle-sonar-analysis-and-quality-gate-pipeline","text":"Add Sonar security gtoup in the Jenkins server security Group in AWS Console. Also, Jenkins security gtoup in the Sonar server security Group in AWS Console. Note: In this step os the sonar pipeline you will see this Line of code: steps { withSonarQubeEnv('sososonar') { The sososonar represents the name of the SonarQube Servers in Configure Syatem. pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages{ stage('Fetch code') { steps{ git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build') { steps { sh 'mvn clean install -DskipTests' } post { success { echo \"I think the archieve is All Good\" archiveArtifacts artifacts: '**/*.war' } } } stage('Test'){ steps { sh 'mvn test' } } stage('Checkstyle Analysis'){ steps { sh 'mvn checkstyle:checkstyle' } } stage('Sonar Analysis') { environment { scannerHome = tool 'sososonar4.7' } steps { withSonarQubeEnv('sososonar') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=sosotech \\ -Dsonar.projectName=sosotech \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } } } stage(\"Quality Gate\") { steps { timeout(time: 1, unit: 'HOURS') { waitForQualityGate abortPipeline: true } } } } }","title":"Popeline 2: Jenkins, Maven, Checkstyle, Sonar-Analysis and Quality Gate - pipeline"},{"location":"weekly/CICD/cicd/#pipeline-3-implementing-docker-ecr","text":"Create an AWS-ECR repo called: soso-repository Build image of webapp and puch to ECR pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } environment { JenkinsECRCredential = 'ecr:us-east-1:sosoawstoken' sosoappRegistry = \"088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository\" sosotechRegistry = \"https://088789840359.dkr.ecr.us-east-1.amazonaws.com\" } stages { stage('Fetch code'){ steps { git branch: 'master', url: 'https://github.com/sosotechnologies/cicd-maven-jenkins-ecr.git' } } stage('Test'){ steps { sh 'mvn test' } } stage('Build App Image') { steps { script { dockerImage = docker.build( sosoappRegistry + \":$BUILD_NUMBER\", \"./sosotech-Dockerfiles/sosoapp/multistagebuild/\") } } } stage('Upload App Image') { steps{ script { docker.withRegistry( sosotechRegistry, JenkinsECRCredential ) { dockerImage.push(\"$BUILD_NUMBER\") dockerImage.push('latest') } } } } } } ``` ##### Popeline 4: Jenkins, Maven, Checkstyle, Docker, Sonar-Analysis and Quality Gate - pipeline ```Jenkinsfile pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } environment { JenkinsECRCredential = 'ecr:us-east-1:sosoawstoken' sosoappRegistry = \"088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository\" sosotechRegistry = \"https://088789840359.dkr.ecr.us-east-1.amazonaws.com\" } stages { stage('Fetch code'){ steps { git branch: 'master', url: 'https://github.com/sosotechnologies/cicd-maven-jenkins-ecr.git' } } stage('Test'){ steps { sh 'mvn test' } } stage ('CODE ANALYSIS WITH CHECKSTYLE'){ steps { sh 'mvn checkstyle:checkstyle' } post { success { echo 'Generated Analysis Result' } } } stage('build && SonarQube analysis') { environment { scannerHome = tool 'sososonar4.7' } steps { withSonarQubeEnv('sososonar') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=sosotech \\ -Dsonar.projectName=sosotech \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } } } stage(\"Quality Gate\") { steps { timeout(time: 1, unit: 'HOURS') { waitForQualityGate abortPipeline: true } } } stage('Build App Image') { steps { script { dockerImage = docker.build( sosoappRegistry + \":$BUILD_NUMBER\", \"./sosotech-Dockerfiles/sosoapp/multistagebuild/\") } } } stage('Upload App Image') { steps{ script { docker.withRegistry( sosotechRegistry, JenkinsECRCredential ) { dockerImage.push(\"$BUILD_NUMBER\") dockerImage.push('latest') } } } } } }","title":"Pipeline 3: Implementing DOCKER ECR"},{"location":"weekly/CICD/cicd/#pipeline-5-full-pipeline","text":"def COLOR_MAP = [ 'SUCCESS': 'good', 'FAILURE': 'danger', ] pipeline { agent any tools { maven \"SOSOMAVEN3\" jdk \"SosoJDK8\" } stages{ stage('Fetch code') { steps{ git branch: 'master', url:'https://github.com/sosotechnologies/cicd-maven.git' } } stage('Build') { steps { sh 'mvn clean install -DskipTests' } post { success { echo \"Now Archiving.\" archiveArtifacts artifacts: '**/*.war' } } } stage('Test'){ steps { sh 'mvn test' } } stage('Checkstyle Analysis'){ steps { sh 'mvn checkstyle:checkstyle' } } stage('Sonar Analysis') { environment { scannerHome = tool 'sososonar4.7' } steps { withSonarQubeEnv('sososonar') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=sosotech \\ -Dsonar.projectName=sosotech \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } } } stage(\"Quality Gate\") { steps { timeout(time: 1, unit: 'HOURS') { waitForQualityGate abortPipeline: true } } } } post { always { echo 'Slack Notifications.' slackSend channel: '#sosochannel1', color: COLOR_MAP[currentBuild.currentResult], message: \"*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \\n More info at: ${env.BUILD_URL}\" } } } Building sosotech documentation site pipeline { agent any environment { JenkinsECRCredential = 'ecr:us-east-1:sosoawstoken' sosoappRegistry = \"088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository\" sosotechRegistry = \"https://088789840359.dkr.ecr.us-east-1.amazonaws.com\" } stages { stage('Fetch code'){ steps { git branch: 'main', url: 'https://github.com/sosotechnologies/docs_docker_io.git' } } stage('Build App Image') { steps { script { dockerImage = docker.build( sosoappRegistry + \":$BUILD_NUMBER\", \"./\") } } } stage('Upload App Image') { steps{ script { docker.withRegistry( sosotechRegistry, JenkinsECRCredential ) { dockerImage.push(\"$BUILD_NUMBER\") dockerImage.push('latest') } } } } } } Building Sosotech Node App pipeline { agent any environment { JenkinsECRCredential = 'ecr:us-east-1:sosoawstoken' sosoappRegistry = \"088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository\" sosotechRegistry = \"https://088789840359.dkr.ecr.us-east-1.amazonaws.com\" } stages { stage('Fetch code'){ steps { git branch: 'master', url: 'https://github.com/sosotechnologies/sosojenkins.git' } } stage('Build App Image') { steps { script { dockerImage = docker.build( sosoappRegistry + \":$BUILD_NUMBER\", \"./dockerhub-nodejs\") } } } stage('Upload App Image') { steps{ script { docker.withRegistry( sosotechRegistry, JenkinsECRCredential ) { dockerImage.push(\"$BUILD_NUMBER\") dockerImage.push('mkdocss') } } } } } }","title":"Pipeline 5: FULL Pipeline***"},{"location":"weekly/CICD/cicd/#build-triggers","text":"Requirements: - Set a New private Git Repo - Set a new ssh Key: ssh-keygen - Get the content of your id_rsa.pub key: cat ~/.ssh/id_rsa.pub - Register the id_rsa.pub in Github SSH and GPC Keys - Get the content of your id_rsa key: cat ~/.ssh/id_rsa - Register the id_rsa key in Jenkins configure credentials. My repo is : git@github.com:sosotechnologies/sosojenkinstriggers.git Add SSH Key to Github Settings Make a dir mysosotriggers : mkdir mysosotriggers hello-world/webapp/src/main/webapp/WEB-INF/ Create a Jenkinsfile in mysosotriggers: cd mysosotriggers git clone git@github.com:sosotechnologies/sosojenkinstriggers.git cd sosojenkinstriggers touch Jenkinsfile vi Jenkinsfile Add this into the Jenkinsfile pipeline { agent any stages { stage('Build') { steps{ sh 'echo \"Looks like the build is Done!\"' } } } } Create a Pipeline Job like in below photo:","title":"Build Triggers"},{"location":"weekly/CICD/cicd/#github-webhook-jobs","text":"Go to video 163","title":"GitHub webHook Jobs"},{"location":"weekly/CICD/cicd/#cicd-microservices","text":"","title":"CICD MicroServices"},{"location":"weekly/CICD/cicd/#docker-server","text":"Ubuntu VERSION=\"20.04.6 LTS Create a New AWS instance. T3 medium During instance creation select option [Allow HTTP traffic from the internet] Storage should be 20Gb Add below script in the user data #!/bin/bash # Install Docker sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release -y curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null # Install Docker-Compose sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io -y sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose # Add ubuntu user into Docker Group sudo usermod -a -G docker ubuntu Clone the repo : Soso-repo docker-compose build docker-compose up -d For docker, see the official docker section in thus doc If you ever encounter NO SPACE issue when building your image sudo docker image prune -f && sudo docker container prune -f","title":"Docker server"},{"location":"weekly/CICD/cicd/#kubernetes","text":"KOPs EKS OpenShift","title":"Kubernetes"},{"location":"weekly/CICD/cicd/#kops","text":"Ubuntu VERSION=\"20.04.6 LTS Installing kOps on Ubuntu Install Helm on the Ubuntu server install JDK-8 on server Create a directory called jenkins-slave in /opt Create a New(public) GitHub repo: Mine is: My repo INSTALL JDK8 On the Jenkins Server In Server terminal, Install Maven and JDK8 sudo apt update sudo apt install openjdk-8-jdk -y curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64 chmod +x ./kops sudo mv ./kops /usr/local/bin/ Install Python PiP sudo apt -y update sudo apt install python3-pip Install AWSCli sudo apt install awscli -y Install Kubectl curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl Create S3 Bucket aws s3 ls aws s3 mb s3://soso-kops-bucket.local OPTIONAL aws s3api create-bucket \\ --bucket soso-kops-bucket \\ --region us-east-1 Add env variables in bashrc vi .bashrc export NAME=soso-kops-bucket.k8s.local export KOPS_STATE_STORE=s3://soso-kops-bucket.local source .bashrc Generate an sshkeys before creating cluster ssh-keygen Install Cluster preconfig file kops create cluster \\ --name=${NAME} \\ --cloud=aws \\ --zones=us-west-2a \\ --node-size t2.medium \\ --master-size t2.medium \\ --master-count 1 --node-size t2.medium --node-count=1 Edit the cluster name- OPTIONAL kops edit cluster --name ${NAME} Install Cluster kops update cluster --name ${NAME} --yes --admin NOTE: WAIT 10 Mins before Checking Nodes and Validating cluster * Get nodes kubectl get nodes Validate Cluster kops validate cluster --wait 10m Delete cluster - OPTIONAL kops delete cluster --name ${NAME} [OR] kops delete cluster --name ${NAME} --yes Make directory /opt/jenkins-slave and give ownership to ubuntu Add Jenkins SG to Kops and Vice-Versa mkdir jenkins-slave sudo mkdir /opt/jenkins-slave sudo chown ubuntu.ubuntu /opt/jenkins-slave -R","title":"KOPS"},{"location":"weekly/CICD/cicd/#create-a-new-slave-node-in-jenkins","text":"CICD Work Flow - Remove my helm and make your own helm directory in same path mkdir cicd-k8s git clone https://github.com/sosotechnologies/cicd-kubernetes-jenkins-pipeline.git cd mkdir helm && cd helm helm create sosotechecharts replace the yaml files in the template with our yaml files in: cici-kubernetes-jenkins-pipeline/kubernetes/soso-app/templates cd cici-kubernetes-jenkins-pipeline/helm/sosotechecharts/templates rm -rf * NOTE : Set the deployment Image as a variable. Name to what ever but take note of the Name, as this name will be passed as a tag when building the Helm Chart' Now install the Chart - cd back into the helm directory and run below command - create a namespace called cicd - helm install --namespace cicd [nameof the chart] --set [the deployment variable]=[built image from dockerhub] kubectl create ns cicd helm install --namespace cicd soso-helm-name --set sosodeploymentvariable=sosotech/sosowebapp:2 helm list --namerpace cicd Delete the Chart helm delete soso-helm-name -n cicd pipeline { agent any /* tools { maven \"maven3\" } */ environment { registry = \"sosotech/docs-repo\" registryCredential = 'sosodockertoken' } stages{ stage('BUILD'){ steps { sh 'mvn clean install -DskipTests' } post { success { echo 'Now Archiving...' archiveArtifacts artifacts: '**/target/*.war' } } } stage('UNIT TEST'){ steps { sh 'mvn test' } } stage('INTEGRATION TEST'){ steps { sh 'mvn verify -DskipUnitTests' } } stage ('CODE ANALYSIS WITH CHECKSTYLE'){ steps { sh 'mvn checkstyle:checkstyle' } post { success { echo 'Generated Analysis Result' } } } stage('Building image') { steps{ script { dockerImage = docker.build registry + \":$BUILD_NUMBER\" } } } stage('Deploy Image') { steps{ script { docker.withRegistry( '', registryCredential ) { dockerImage.push(\"$BUILD_NUMBER\") dockerImage.push('latest') } } } } stage('Remove Unused docker image') { steps{ sh \"docker rmi $registry:$BUILD_NUMBER\" } } stage('CODE ANALYSIS with SONARQUBE') { environment { scannerHome = tool 'mysonarscanner4' } steps { withSonarQubeEnv('sonar-pro') { sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \\ -Dsonar.projectName=vprofile-repo \\ -Dsonar.projectVersion=1.0 \\ -Dsonar.sources=src/ \\ -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \\ -Dsonar.junit.reportsPath=target/surefire-reports/ \\ -Dsonar.jacoco.reportsPath=target/jacoco.exec \\ -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml''' } timeout(time: 10, unit: 'MINUTES') { waitForQualityGate abortPipeline: true } } } stage('Kubernetes Deploy') { agent { label 'KOPS' } steps { sh \"helm upgrade --install --force vproifle-stack helm/sosotechecharts --set appimage=${registry}:${BUILD_NUMBER} --namespace prod\" } } } }","title":"Create a new Slave Node in Jenkins"},{"location":"weekly/Cloud-Technologies/intro-aws/","text":"Cloud Technologies Defining cloud computing Defining private, public, and hybrid cloud Exploring the components of cloud infrastructure Defining the different Service Models What is Cloud Computing? Cloud computing is the use of hosted services, such as servers, data storage, databases, software, and networking over the internet\u00ac\u200a\u2014\u200ainstead of directly accessing your computer hard drive. Types of cloud computing Cloud computing is offered in 3 major forms: private, public, and hybrid cloud. Private Cloud: The private cloud computing service is a cloud computing model where the infrastructure is dedicated only to select users instead of the general public, either over the Internet or a private internal network. Other names for the private cloud are internal or corporate cloud. Some private cloud providers are VMware-vRealize Suite Cloud Management Platform, AWS-Virtual Private Cloud, Microsoft-Azure Private Cloud\u2026 Public Cloud: The public cloud is defined as computing services offered by third-party providers over the public Internet, making them available to anyone who wants to use or purchase them. They may be free or sold on-demand, allowing customers to pay only per usage for the CPU cycles, storage, or bandwidth they consume. Some public cloud services are AWS, GCP, Azure, IBM Cloud, Alibaba Cloud\u2026 Hybrid Cloud: A hybrid cloud is a computing environment that combines an on-premises data center (also called a private cloud) with a public cloud, allowing data and applications to be shared between them. Some public cloud providers are VMware, Microsoft Azure\u2026 Components of cloud infrastructure Client-side Infrastructure: Here, users interact with the cloud through a front-end GUI (Graphical User Interface). Desktop, laptops, mobiles, tablets, and others are used on the client-side to access services. Application: The application may be any software or platform that a client wants to access. The operating system: The cloud operating system manages the operation, the virtual servers, execution and processes of virtual machines and virtual infrastructure, as well as the back-end hardware and software resources. Service: Cloud Services are offered in different models. These services are being provided based on the client\u2019s request. Some of the service models are seen below Service Models Cloud-Computing providers offer their \u201cservices\u201d according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). IAAS: Infrastructure As A Service (IAAS) is means of delivering computing infrastructure as on-demand services. It is one of the three fundamental cloud service model servers\u2019 storage network operating systems. In the user purchasing servers, software data center space, or network equipment and renting those resources as a fully outsourced service can demand model. It allows dynamic scaling, and the resources are distributed as a service. It generally includes multiple users on a single piece of hardware. PAAS: Platform as A Service (PAAS) is a cloud delivery model for applications composed of services managed by a third party. It provides elastic scaling of your application which allows developers to build applications and services over the internet and the deployment models include public, private and hybrid. SAAS: Software As A Service (SAAS) allows users to run existing online applications and it is a model software that is deployed as a hosting service and is accessed over Output Rephrased/Re-written Text the internet or software delivery model during which software and its associated data are hosted centrally and accessed using their client, usually an online browser over the web. SAAS services are used for the development and deployment of modern applications. The emergence of Cloud computing has given rise to more as-a-service offerings. Some of them include: - AIaaS: AI as a service - DaaS: Desktop as a service - ITaaS: IT as a service - RaaS: Ransomware as a service. Top benefits of cloud computing High Speed: the ability to spin up new instances in a matter of seconds. Efficiency and Cost Reduction Provides efficient Data Security Scalable: ease with quickly\u200a\u2014\u200ascale up/down infrastructures. Collaboration: Cloud environments enable better collaboration across teams. Provides unlimited Storage Capacity. Provides ease in Back-up and Restoring of Data Reliability: Cloud computing makes data backup, disaster recovery, and business continuity easier and less expensive because data can be mirrored at multiple redundant sites on the cloud provider\u2019s network. FOCUS: AWS CLOUD What Is Amazon Web Services(AWS): Amazon Web Services(AWS) is Amazon\u2019s cloud computing platform that offers a mix of packaged platform as a service (PaaS), software as a service (SaaS), and infrastructure as a service (IaaS). ### EC2 Amazon EC2 is AWS primary web service that provides resizable compute capacity in the cloud. Instance types: Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. Example instance Family: c4 Compute optimized\u2014For workloads requiring significant processing r3 Memory optimized\u2014For memory-intensive workloads i2 Storage optimized\u2014For workloads requiring high amounts of fast SSD storage g2 GPU-based instances\u2014Intended for graphics and general-purpose GPU compute workloads Amazon Machine Images (AMIs): Defines the initial software that will be on an instance when it is launched. An AMI defines; The Operating System (OS) and its configuration, The initial state of any patches, Application or system software. AMI\u2019s can be AWS published, from the AWS Marketplace, Generated from Existing Instances, or Uploaded Virtual Servers Security Groups: This is a VPC concept. If an instance is running in an Amazon VPC, you can change which security groups are associated with an instance while the instance is running. EBS: Is type of \u201cblock storage\u201d volume offering by AWS. EBS provides persistent block-level storage volumes for use with Amazon EC2 instances. Instance Stores: Is another type of \u201cblock storage\u201d volume offering by AWS for your instance. This storage is located on disks that are physically attached to the host computer. ### S3 Amazon Simple Storage Service is a secure, durable, and highly-scalable cloud storage. Some use cases for cloud storage include: - Backup and archive for on-premises or cloud data. - Content, media, and software storage and distribution. - Big data analytics. - Static website hosting. - Cloud-native mobile and Internet application hosting. ### VPC The Amazon Virtual Private Cloud (Amazon VPC) is a custom-defined virtual network within the AWS Cloud. ![cloud1](photos/cloud5.png) VPC COMPONENTS: An Amazon VPC consists of the following main components: Subnets, Route tables, Dynamic Host Configuration Protocol (DHCP) option sets Security groups, Network Access Control Lists (ACLs). Also, optional components: Internet Gateways (IGWs), Elastic IP (EIP) addresses, Elastic Network Interfaces (ENIs), Endpoints, Peering, Network Address Translation (NATs) instances and NAT gateways, Virtual Private Gateway (VPG), Customer Gateways (CGWs), and Virtual Private Networks (VPNs) ***Subnets:*** A subnet is a segment of an Amazon VPC\u2019s IP address range. Subnets reside within one Availability Zone and cannot span zones; You can, however, have multiple subnets in one Availability Zone. Each subnet must be associated with a route table, which controls the routing for the subnet. ***Route Tables:*** A route table is a logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where network traffic is directed. ***Internet Gateways:*** IGW allows communication between instances in your Amazon VPC and the Internet. ***Dynamic Host Configuration*** Protocol (DHCP) Option Sets: provides a standard for passing configuration information to hosts on a TCP/IP network. AWS automatically creates and associates a DHCP option set for your Amazon VPC upon creation and sets two options: domain-name-servers (defaulted to AmazonProvidedDNS) and domain-name (defaulted to the domain name for your region). ### IAM With AWS IAM Identity Center (successor to AWS Single Sign-On), you can manage sign-in security for your workforce identities, also known as workforce users. IAM Identity Center provides one place where you can create or connect workforce users and centrally manage their access across all their AWS accounts and applications. For more on IAM chack the link: IAM ### AWS ELASTIC LOAD BALANCER (ELB) AWS has an Elastic Load Balancer that is used to distribute incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing supports routing and load balancing of Hypertext. Transfer Protocol (HTTP), Hypertext Transfer Protocol Secure (HTTPS), Transmission Control Protocol (TCP), and Secure Sockets Layer (SSL) traffic to Amazon EC2 instances. Types of AWS ELB: The two major Elastic load balancers are: ***Application Load Balancer*** Application layer that describes the use of HTTP and HTTPS connections from clients to the load balancer and from the load balancer to your back-end instance. ***Network Load Balancer*** Are used to route traffic through layer 4, based IPs and TCP or UDP ports. ### EKS Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. For more on EKS chack the link: [EKS]https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html","title":"Cloud Technologies"},{"location":"weekly/Cloud-Technologies/intro-aws/#cloud-technologies","text":"Defining cloud computing Defining private, public, and hybrid cloud Exploring the components of cloud infrastructure Defining the different Service Models","title":"Cloud Technologies"},{"location":"weekly/Cloud-Technologies/intro-aws/#what-is-cloud-computing","text":"Cloud computing is the use of hosted services, such as servers, data storage, databases, software, and networking over the internet\u00ac\u200a\u2014\u200ainstead of directly accessing your computer hard drive.","title":"What is Cloud Computing?"},{"location":"weekly/Cloud-Technologies/intro-aws/#types-of-cloud-computing","text":"Cloud computing is offered in 3 major forms: private, public, and hybrid cloud. Private Cloud: The private cloud computing service is a cloud computing model where the infrastructure is dedicated only to select users instead of the general public, either over the Internet or a private internal network. Other names for the private cloud are internal or corporate cloud. Some private cloud providers are VMware-vRealize Suite Cloud Management Platform, AWS-Virtual Private Cloud, Microsoft-Azure Private Cloud\u2026 Public Cloud: The public cloud is defined as computing services offered by third-party providers over the public Internet, making them available to anyone who wants to use or purchase them. They may be free or sold on-demand, allowing customers to pay only per usage for the CPU cycles, storage, or bandwidth they consume. Some public cloud services are AWS, GCP, Azure, IBM Cloud, Alibaba Cloud\u2026 Hybrid Cloud: A hybrid cloud is a computing environment that combines an on-premises data center (also called a private cloud) with a public cloud, allowing data and applications to be shared between them. Some public cloud providers are VMware, Microsoft Azure\u2026","title":"Types of cloud computing"},{"location":"weekly/Cloud-Technologies/intro-aws/#components-of-cloud-infrastructure","text":"Client-side Infrastructure: Here, users interact with the cloud through a front-end GUI (Graphical User Interface). Desktop, laptops, mobiles, tablets, and others are used on the client-side to access services. Application: The application may be any software or platform that a client wants to access. The operating system: The cloud operating system manages the operation, the virtual servers, execution and processes of virtual machines and virtual infrastructure, as well as the back-end hardware and software resources. Service: Cloud Services are offered in different models. These services are being provided based on the client\u2019s request. Some of the service models are seen below","title":"Components of cloud infrastructure"},{"location":"weekly/Cloud-Technologies/intro-aws/#service-models","text":"Cloud-Computing providers offer their \u201cservices\u201d according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). IAAS: Infrastructure As A Service (IAAS) is means of delivering computing infrastructure as on-demand services. It is one of the three fundamental cloud service model servers\u2019 storage network operating systems. In the user purchasing servers, software data center space, or network equipment and renting those resources as a fully outsourced service can demand model. It allows dynamic scaling, and the resources are distributed as a service. It generally includes multiple users on a single piece of hardware. PAAS: Platform as A Service (PAAS) is a cloud delivery model for applications composed of services managed by a third party. It provides elastic scaling of your application which allows developers to build applications and services over the internet and the deployment models include public, private and hybrid. SAAS: Software As A Service (SAAS) allows users to run existing online applications and it is a model software that is deployed as a hosting service and is accessed over Output Rephrased/Re-written Text the internet or software delivery model during which software and its associated data are hosted centrally and accessed using their client, usually an online browser over the web. SAAS services are used for the development and deployment of modern applications. The emergence of Cloud computing has given rise to more as-a-service offerings. Some of them include: - AIaaS: AI as a service - DaaS: Desktop as a service - ITaaS: IT as a service - RaaS: Ransomware as a service.","title":"Service Models"},{"location":"weekly/Cloud-Technologies/intro-aws/#top-benefits-of-cloud-computing","text":"High Speed: the ability to spin up new instances in a matter of seconds. Efficiency and Cost Reduction Provides efficient Data Security Scalable: ease with quickly\u200a\u2014\u200ascale up/down infrastructures. Collaboration: Cloud environments enable better collaboration across teams. Provides unlimited Storage Capacity. Provides ease in Back-up and Restoring of Data Reliability: Cloud computing makes data backup, disaster recovery, and business continuity easier and less expensive because data can be mirrored at multiple redundant sites on the cloud provider\u2019s network.","title":"Top benefits of cloud computing"},{"location":"weekly/Cloud-Technologies/intro-aws/#focus-aws-cloud","text":"What Is Amazon Web Services(AWS): Amazon Web Services(AWS) is Amazon\u2019s cloud computing platform that offers a mix of packaged platform as a service (PaaS), software as a service (SaaS), and infrastructure as a service (IaaS). ### EC2 Amazon EC2 is AWS primary web service that provides resizable compute capacity in the cloud. Instance types: Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. Example instance Family: c4 Compute optimized\u2014For workloads requiring significant processing r3 Memory optimized\u2014For memory-intensive workloads i2 Storage optimized\u2014For workloads requiring high amounts of fast SSD storage g2 GPU-based instances\u2014Intended for graphics and general-purpose GPU compute workloads Amazon Machine Images (AMIs): Defines the initial software that will be on an instance when it is launched. An AMI defines; The Operating System (OS) and its configuration, The initial state of any patches, Application or system software. AMI\u2019s can be AWS published, from the AWS Marketplace, Generated from Existing Instances, or Uploaded Virtual Servers Security Groups: This is a VPC concept. If an instance is running in an Amazon VPC, you can change which security groups are associated with an instance while the instance is running. EBS: Is type of \u201cblock storage\u201d volume offering by AWS. EBS provides persistent block-level storage volumes for use with Amazon EC2 instances. Instance Stores: Is another type of \u201cblock storage\u201d volume offering by AWS for your instance. This storage is located on disks that are physically attached to the host computer. ### S3 Amazon Simple Storage Service is a secure, durable, and highly-scalable cloud storage. Some use cases for cloud storage include: - Backup and archive for on-premises or cloud data. - Content, media, and software storage and distribution. - Big data analytics. - Static website hosting. - Cloud-native mobile and Internet application hosting. ### VPC The Amazon Virtual Private Cloud (Amazon VPC) is a custom-defined virtual network within the AWS Cloud. ![cloud1](photos/cloud5.png) VPC COMPONENTS: An Amazon VPC consists of the following main components: Subnets, Route tables, Dynamic Host Configuration Protocol (DHCP) option sets Security groups, Network Access Control Lists (ACLs). Also, optional components: Internet Gateways (IGWs), Elastic IP (EIP) addresses, Elastic Network Interfaces (ENIs), Endpoints, Peering, Network Address Translation (NATs) instances and NAT gateways, Virtual Private Gateway (VPG), Customer Gateways (CGWs), and Virtual Private Networks (VPNs) ***Subnets:*** A subnet is a segment of an Amazon VPC\u2019s IP address range. Subnets reside within one Availability Zone and cannot span zones; You can, however, have multiple subnets in one Availability Zone. Each subnet must be associated with a route table, which controls the routing for the subnet. ***Route Tables:*** A route table is a logical construct within an Amazon VPC that contains a set of rules (called routes) that are applied to the subnet and used to determine where network traffic is directed. ***Internet Gateways:*** IGW allows communication between instances in your Amazon VPC and the Internet. ***Dynamic Host Configuration*** Protocol (DHCP) Option Sets: provides a standard for passing configuration information to hosts on a TCP/IP network. AWS automatically creates and associates a DHCP option set for your Amazon VPC upon creation and sets two options: domain-name-servers (defaulted to AmazonProvidedDNS) and domain-name (defaulted to the domain name for your region). ### IAM With AWS IAM Identity Center (successor to AWS Single Sign-On), you can manage sign-in security for your workforce identities, also known as workforce users. IAM Identity Center provides one place where you can create or connect workforce users and centrally manage their access across all their AWS accounts and applications. For more on IAM chack the link: IAM ### AWS ELASTIC LOAD BALANCER (ELB) AWS has an Elastic Load Balancer that is used to distribute incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing supports routing and load balancing of Hypertext. Transfer Protocol (HTTP), Hypertext Transfer Protocol Secure (HTTPS), Transmission Control Protocol (TCP), and Secure Sockets Layer (SSL) traffic to Amazon EC2 instances. Types of AWS ELB: The two major Elastic load balancers are: ***Application Load Balancer*** Application layer that describes the use of HTTP and HTTPS connections from clients to the load balancer and from the load balancer to your back-end instance. ***Network Load Balancer*** Are used to route traffic through layer 4, based IPs and TCP or UDP ports. ### EKS Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. For more on EKS chack the link: [EKS]https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html","title":"FOCUS: AWS CLOUD"},{"location":"weekly/Docker/docker/","text":"History of Containarizaion Traditional deployment era: Early on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a physical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can be instances where one application would take up most of the resources, and as a result, the other applications would underperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources were underutilized, and it was expensive for organizations to maintain many physical servers. Virtualized deployment era: As a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization allows applications to be isolated between VMs and provides a level of security as the information of one application cannot be freely accessed by another application. Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware. Container deployment era: Containers are like VMs, but they have relaxed isolation properties to share the operating system (OS) the applications. Containers are an abstraction at the app layer that packages code and dependencies together. Multiple containers can run on the same machine and share the OS kernel with other containers, each running as isolated processes in user space. Docker architecture Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. What is Docker? Docker is a software platform that allows you to build, test, and deploy applications quickly. Docker packages software into standardized units called containers that have everything the software needs to run including libraries, system tools, code, and runtime. Using Docker, you can quickly deploy and scale applications into any environment and know your code will run. What\u2019s a docker container? A container is a Runnable instance of an image. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. What is an image An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run. What\u2019s the difference between an Image and a container? Simply put, containers are dependent on images. Containers need images to construct a run-time environment and run an application. Dockerfile A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Breaking down the components of a dockerfile FROM node WORKDIR /app COPY . /app RUN npm install EXPOSE 80 CMD [\"node\", \"server.js\"] FROM node build your image from another base image WORKDIR /app this workdir is what i defined in #2, I am telling Docker that when I install npm, I want you to do in this directory. Optionally, i can still define my WORKDIR as \" /app \" COPY . /app COPY . ./ COPY . . tell docker the files on the left it should use. In our case i'm using [. .] meaning select every file The first [.] represent the files in the same directory as the dockerfile The second [.] represents the path inside the image where those files will be stored. Ex: you can copy like so: \" COPY . /app \" IN this example we are copying the files to /app in the container I can also set as a relative path as just [ . ./] because I have defined a workdir with a defined value RUN npm install if you recall, we did this process as a command earlier EXPOSE 80 remember we have defined a port # in our server.js, and we have to let docker know that it should listen to this port CMD [\"node\", \"server.js\"] to define a CMD, we open an array [], and also pass 2 strings \"\", \"\" to separate our commands CMD command specifies the instruction that is to be executed when a Docker container starts. We are telling docker to use the node command inside the container to run our server.js file. Docker Network One of the reasons Docker containers and services are so powerful is that you can connect them together, or connect them to non-Docker workloads. Docker containers can communicate an HTTP request in ways: - From the container to an external application via an API communication or - From the container to a local host machine - From the container to another container application Docker Network Drivers Docker Networks actually support different kinds of \"Drivers\" which influence the behavior of the Network. The default driver is the \"bridge\" driver - it provides the behavior shown in this module (i.e. Containers can find each other by name if they are in the same Network). The driver can be set when a Network is created, simply by adding the --driver option. EX: docker network create --driver bridge soso-net Of course, if you want to use the \"bridge\" driver, you can simply omit the entire option since \"bridge\" is the default anyways. Docker also supports these alternative drivers - though you will use the \"bridge\" driver in most cases: host: For standalone containers, isolation between container and host system is removed (i.e. they share localhost as a network) overlay: Multiple Docker daemons (i.e. Docker running on different machines) are able to connect with each other. Only works in \"Swarm\" mode which is a dated / almost deprecated way of connecting multiple containers macvlan: You can set a custom MAC address to a container - this address can then be used for communication with that container none: All networking is disabled. Third-party plugins: You can install third-party plugins which then may add all kinds of behaviors and functionalities. As mentioned, the \"bridge\" driver makes most sense in the vast majority of scenarios. Docker Storage There are 2 kinds of storage that docker can use (Anonymous Volume/named volume and Bind Mount) Anonymous Volume/named volume are managed by docker, while Bind Mount is managed by us, cause we setup (Anonymous Volume): A type of volume managed by docker. Anonymous volumes have to be created inside a dockerfile. Named Volume: For Data that can be viewed and stored but can\u2019t be accessed directly For Named volume, I don\u2019t have to add the volume in the dockerfile. I will add a volume line in the command I will type in the terminal. Ex: Creating a sample Names Volume: docker run -d -p 3000:80 --name mysoso-container sosonode docker run -d -p 3000:80 --name mysoso-container [-v for volume] [the path inside the container we wanna save] [volume name] docker run -d -p 3000:80 --name mysoso-container -v sosofeedback:/app/feedback mysosofeedback-node:volumes Ex: Creating a sample Bind Mounts Volume: docker run -d -p 3000:80 --name mysoso-container -v sosofeedback:/app/feedback [add another volume] mysosofeedback-node:volumes docker run -d -p 3000:80 --name mysoso-container -v sosofeedback:/app/feedback -v \u201c/Users/macfe/OneDrive/Desktop/nodejs-a-z/nodejs-02/data-volume-dockerfile/data-volumes-02-added-dockerfile:/app\u201d mysosofeedback-node:volumes Use Cases 1. Docker and Mkdocs Build and deploy push Mkdocs to ECR/DockerHub Clone this repo: docs_docker_io Install Docker sudo yum install docker -y sudo systemctl status docker sudo systemctl start docker sudo systemctl enable docker Build and Push Docker sudo docker build -t sosodocs . Tag the sosodoc image sudo docker tag sosodocs 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Push image to repo sudo docker push 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Run the image with any one of the 2 commands, set yout ip:80 docker run -itd -p 80:80 --rm sosodocs docker run -t -i -p 80:80 sosodocs And that is all, you should be able to navigate to http://127.0.0.1:80 and see the documentation website running. Optional BONUS!!!: DEPLOY TO AN EXISTING EKS CLUSTER k expose deploy mkdocs --name=mkdocs-svc --port=80 --type=LoadBalancer --targetPort=80 --dry-run=client -o yaml > deploy.yaml k expose deploy mkdocs --name=mkdocs-svc --port=80 --type=LoadBalancer --target-port=80 --dry-run=client -o yaml > service.yaml Optional Tag Your docker image with a version sudo docker tag sosodocs sosodocs:v1 Delete the existing running container, 26b43376e040 is mine, get yours. sudo docker rm 26b43376e040 OR sudo docker rm 26b43376e040 --force Re-run the new versioned-image sudo docker run -itd -p 80:80 --rm sosodocs:v1 To remove container or image: docker rm [container] docker rmi [image]","title":"Docker"},{"location":"weekly/Docker/docker/#history-of-containarizaion","text":"Traditional deployment era: Early on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a physical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can be instances where one application would take up most of the resources, and as a result, the other applications would underperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources were underutilized, and it was expensive for organizations to maintain many physical servers. Virtualized deployment era: As a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization allows applications to be isolated between VMs and provides a level of security as the information of one application cannot be freely accessed by another application. Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware. Container deployment era: Containers are like VMs, but they have relaxed isolation properties to share the operating system (OS) the applications. Containers are an abstraction at the app layer that packages code and dependencies together. Multiple containers can run on the same machine and share the OS kernel with other containers, each running as isolated processes in user space.","title":"History of Containarizaion"},{"location":"weekly/Docker/docker/#docker-architecture","text":"Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. What is Docker? Docker is a software platform that allows you to build, test, and deploy applications quickly. Docker packages software into standardized units called containers that have everything the software needs to run including libraries, system tools, code, and runtime. Using Docker, you can quickly deploy and scale applications into any environment and know your code will run. What\u2019s a docker container? A container is a Runnable instance of an image. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings. What is an image An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run. What\u2019s the difference between an Image and a container? Simply put, containers are dependent on images. Containers need images to construct a run-time environment and run an application.","title":"Docker architecture"},{"location":"weekly/Docker/docker/#dockerfile","text":"A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Breaking down the components of a dockerfile FROM node WORKDIR /app COPY . /app RUN npm install EXPOSE 80 CMD [\"node\", \"server.js\"] FROM node build your image from another base image WORKDIR /app this workdir is what i defined in #2, I am telling Docker that when I install npm, I want you to do in this directory. Optionally, i can still define my WORKDIR as \" /app \" COPY . /app COPY . ./ COPY . . tell docker the files on the left it should use. In our case i'm using [. .] meaning select every file The first [.] represent the files in the same directory as the dockerfile The second [.] represents the path inside the image where those files will be stored. Ex: you can copy like so: \" COPY . /app \" IN this example we are copying the files to /app in the container I can also set as a relative path as just [ . ./] because I have defined a workdir with a defined value RUN npm install if you recall, we did this process as a command earlier EXPOSE 80 remember we have defined a port # in our server.js, and we have to let docker know that it should listen to this port CMD [\"node\", \"server.js\"] to define a CMD, we open an array [], and also pass 2 strings \"\", \"\" to separate our commands CMD command specifies the instruction that is to be executed when a Docker container starts. We are telling docker to use the node command inside the container to run our server.js file.","title":"Dockerfile"},{"location":"weekly/Docker/docker/#docker-network","text":"One of the reasons Docker containers and services are so powerful is that you can connect them together, or connect them to non-Docker workloads. Docker containers can communicate an HTTP request in ways: - From the container to an external application via an API communication or - From the container to a local host machine - From the container to another container application","title":"Docker Network"},{"location":"weekly/Docker/docker/#docker-network-drivers","text":"Docker Networks actually support different kinds of \"Drivers\" which influence the behavior of the Network. The default driver is the \"bridge\" driver - it provides the behavior shown in this module (i.e. Containers can find each other by name if they are in the same Network). The driver can be set when a Network is created, simply by adding the --driver option. EX: docker network create --driver bridge soso-net Of course, if you want to use the \"bridge\" driver, you can simply omit the entire option since \"bridge\" is the default anyways. Docker also supports these alternative drivers - though you will use the \"bridge\" driver in most cases: host: For standalone containers, isolation between container and host system is removed (i.e. they share localhost as a network) overlay: Multiple Docker daemons (i.e. Docker running on different machines) are able to connect with each other. Only works in \"Swarm\" mode which is a dated / almost deprecated way of connecting multiple containers macvlan: You can set a custom MAC address to a container - this address can then be used for communication with that container none: All networking is disabled. Third-party plugins: You can install third-party plugins which then may add all kinds of behaviors and functionalities. As mentioned, the \"bridge\" driver makes most sense in the vast majority of scenarios.","title":"Docker Network Drivers"},{"location":"weekly/Docker/docker/#docker-storage","text":"There are 2 kinds of storage that docker can use (Anonymous Volume/named volume and Bind Mount) Anonymous Volume/named volume are managed by docker, while Bind Mount is managed by us, cause we setup (Anonymous Volume): A type of volume managed by docker. Anonymous volumes have to be created inside a dockerfile. Named Volume: For Data that can be viewed and stored but can\u2019t be accessed directly For Named volume, I don\u2019t have to add the volume in the dockerfile. I will add a volume line in the command I will type in the terminal. Ex: Creating a sample Names Volume: docker run -d -p 3000:80 --name mysoso-container sosonode docker run -d -p 3000:80 --name mysoso-container [-v for volume] [the path inside the container we wanna save] [volume name] docker run -d -p 3000:80 --name mysoso-container -v sosofeedback:/app/feedback mysosofeedback-node:volumes Ex: Creating a sample Bind Mounts Volume: docker run -d -p 3000:80 --name mysoso-container -v sosofeedback:/app/feedback [add another volume] mysosofeedback-node:volumes docker run -d -p 3000:80 --name mysoso-container -v sosofeedback:/app/feedback -v \u201c/Users/macfe/OneDrive/Desktop/nodejs-a-z/nodejs-02/data-volume-dockerfile/data-volumes-02-added-dockerfile:/app\u201d mysosofeedback-node:volumes","title":"Docker Storage"},{"location":"weekly/Docker/docker/#use-cases","text":"","title":"Use Cases"},{"location":"weekly/Docker/docker/#1-docker-and-mkdocs","text":"Build and deploy push Mkdocs to ECR/DockerHub Clone this repo: docs_docker_io Install Docker sudo yum install docker -y sudo systemctl status docker sudo systemctl start docker sudo systemctl enable docker","title":"1. Docker and Mkdocs"},{"location":"weekly/Docker/docker/#build-and-push-docker","text":"sudo docker build -t sosodocs . Tag the sosodoc image sudo docker tag sosodocs 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Push image to repo sudo docker push 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Run the image with any one of the 2 commands, set yout ip:80 docker run -itd -p 80:80 --rm sosodocs docker run -t -i -p 80:80 sosodocs And that is all, you should be able to navigate to http://127.0.0.1:80 and see the documentation website running. Optional BONUS!!!: DEPLOY TO AN EXISTING EKS CLUSTER k expose deploy mkdocs --name=mkdocs-svc --port=80 --type=LoadBalancer --targetPort=80 --dry-run=client -o yaml > deploy.yaml k expose deploy mkdocs --name=mkdocs-svc --port=80 --type=LoadBalancer --target-port=80 --dry-run=client -o yaml > service.yaml Optional Tag Your docker image with a version sudo docker tag sosodocs sosodocs:v1 Delete the existing running container, 26b43376e040 is mine, get yours. sudo docker rm 26b43376e040 OR sudo docker rm 26b43376e040 --force Re-run the new versioned-image sudo docker run -itd -p 80:80 --rm sosodocs:v1 To remove container or image: docker rm [container] docker rmi [image]","title":"Build and Push Docker"},{"location":"weekly/ELK/elk/","text":"ELK - Elastic Stack ELK Stack is the world\u2019s most popular log management platform. The ELK stack is an acronym used to describe a stack that comprises of three popular projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a distributed search and analytics engine built on Apache Lucene Logstash is used for both shipping as well as processing and storing logs Kibana is a visualization tool which is hosted through Nginx or Apache Some popular Use Cases for ELK are: Logging, Matrics, Security Analysis, Business Analysis - Logging - logging and analysing Data, like gaming data - Matrics - like NASA Rover can sent telematrics, sensor data - Security Analysis - Helps with securing comunication in Apps like Slack - Business Analysis - Helps with matching people ElasticSearch With ElasticSearch, User can sends search quary to your website and your server send those requests to ElasticSearch, Elasticsearch then processes this request and Responds back to the user, through the Server. Elasticsearch Architecture: Key Components Elasticsearch Cluster is composed of a group of nodes that store data. Elasticsearch Node is an instance that runs in a cluster. Multiple nodes can be run on a single machine. When Elastic is running, you will have an instance of elastic known as Node. Each Node Has a unique ID and a node. Each Node belongs to a single cluster. Data is stored as documents in Elasticsearch. Indices are used to group documents that are related to each other. When an Index is created, it comes with a Shard, by default. There are the three main options to configure an Elasticsearch node: Elasticsearch master node Elasticsearch data node Elasticsearch client node Search movement - client --> servers --> ElasticSearch Shards is where data is stored. What you create an Index, you can create multiple shards that are distributed across nodes. Each shard has its replica to recover data in case of node failure. The primary shard and replica shard is always placed on different nodes. Horizontal scaling or scale-out is the main reason to shard a database. The number of shards depends heavily on the amount of data we have. Shard Example use Case 1 A BCBSNC Elastic engineer has 900k documents that they want to store. Each Shard can only Hold a maximum of 300k Documents Two more shards of 300GB each will be added bringing the total storage capacity to 900K Documents Shard Example use Case 2 Scenario 1: An Elastic engineer searches 500k documents that is stored in a single Shard The response time for searching 500k Documents in a single shard is 10 seconds. Scenario 2: An Elastic engineer runs a parallel searches of 500k documents that are distributed in 5 Shard. The response time for searching 100k Documents/Shard will be 2 seconds. So, the response time for searching all 500k Documents that are distributed amongst the 5 shard will still be 2 seconds. Kibana Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. Kibana Aggregations There are two main types of aggregations Metric aggregations are used to calculate a value for each bucket based on the documents inside the bucket. Bucket aggregations are used groups documents together in one bucket according to your logic and requirements, while the Metric aggregations For More information on Kibana,: Click link Adding data into Elasticsearch The main consideration for indexing or adding data into Elasticsearch largely depends on whether you are indexing general content or timestamped data. General content: Index content like HTML pages, catalogs and other files. Send data directly to Elasticseach from your application using an Elastic language client. Otherwise use Workplace Search content connectors or the Enterprise Search web crawler. Timestamped data: The preferred way to index timestamped data is to use Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. How to ingest data into Elasticsearch Service There are 4 main ways in Adding Data to ElasticSearch Service: By using Beats and Logstash By using Kibana By programmatically indexing data Manually adding data Read more about data ingestion in ElasticSearch Click Link The index The index is the basis of Elasticsearch, it is where you store your documents. You can compare it to a table in a database. An index has settings and a mapping which defines how to store and index the documents. Mapping Mapping is the process of defining how a document, and the fields it contains, are stored and indexed. Each document is a collection of fields, which each have their own data type. When mapping your data, you create a mapping definition, which contains a list of fields that are pertinent to the document. What is Elastic integrations Elastic integrations are a streamlined way to connect your data to the Elastic Stack. Integrations are available for popular services and platforms, like Nginx, AWS, and MongoDB, as well as many generic input types like log files. For more on Data Integration Click link: Elastic Agent Elastic integrations are powered by Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. It can also protect hosts from security threats, query data from operating systems, forward data from remote services or hardware, and more. Sample Hands-on I will : Use curl commands to talk to the Elasticsearch REST API. Or if you choose to use Kibana Devtools, thats also ok, because Kibana\u2019s Devtools can automatically recognizes and converts curl commands . create an index Called middleware curl -X PUT \u201clocalhost:9200/middleware Delete the index Called middleware curl -X DELETE \u201clocalhost:9200/middleware\u201d Create an index Called middleware with just one shard and no replicas PUT /middleware?pretty { \"settings\" : { \"number_of_shards\" : 1, \"number_of_replicas\" : 0 } } loading data into that index Called middleware PUT /middleware/_mapping { \"properties\": { \"post_time\": { \"type\": \"date\" }, \"username\": { \"type\": \"keyword\" }, \"message\": { \"type\": \"text\" } } } Now Load some data Manually POST /middleware/_doc/?pretty { \"user\" : \"collins\", \"post_date\" : \"2023-16-03T14:10:10\", \"message\" : \"Welcome to Elasticsearch?\" } see this mediam link, its cool Click-Link","title":"ELK"},{"location":"weekly/ELK/elk/#elk-elastic-stack","text":"ELK Stack is the world\u2019s most popular log management platform. The ELK stack is an acronym used to describe a stack that comprises of three popular projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a distributed search and analytics engine built on Apache Lucene Logstash is used for both shipping as well as processing and storing logs Kibana is a visualization tool which is hosted through Nginx or Apache Some popular Use Cases for ELK are: Logging, Matrics, Security Analysis, Business Analysis - Logging - logging and analysing Data, like gaming data - Matrics - like NASA Rover can sent telematrics, sensor data - Security Analysis - Helps with securing comunication in Apps like Slack - Business Analysis - Helps with matching people","title":"ELK - Elastic Stack"},{"location":"weekly/ELK/elk/#elasticsearch","text":"With ElasticSearch, User can sends search quary to your website and your server send those requests to ElasticSearch, Elasticsearch then processes this request and Responds back to the user, through the Server.","title":"ElasticSearch"},{"location":"weekly/ELK/elk/#elasticsearch-architecture-key-components","text":"Elasticsearch Cluster is composed of a group of nodes that store data. Elasticsearch Node is an instance that runs in a cluster. Multiple nodes can be run on a single machine. When Elastic is running, you will have an instance of elastic known as Node. Each Node Has a unique ID and a node. Each Node belongs to a single cluster. Data is stored as documents in Elasticsearch. Indices are used to group documents that are related to each other. When an Index is created, it comes with a Shard, by default. There are the three main options to configure an Elasticsearch node: Elasticsearch master node Elasticsearch data node Elasticsearch client node Search movement - client --> servers --> ElasticSearch Shards is where data is stored. What you create an Index, you can create multiple shards that are distributed across nodes. Each shard has its replica to recover data in case of node failure. The primary shard and replica shard is always placed on different nodes. Horizontal scaling or scale-out is the main reason to shard a database. The number of shards depends heavily on the amount of data we have. Shard Example use Case 1 A BCBSNC Elastic engineer has 900k documents that they want to store. Each Shard can only Hold a maximum of 300k Documents Two more shards of 300GB each will be added bringing the total storage capacity to 900K Documents Shard Example use Case 2 Scenario 1: An Elastic engineer searches 500k documents that is stored in a single Shard The response time for searching 500k Documents in a single shard is 10 seconds. Scenario 2: An Elastic engineer runs a parallel searches of 500k documents that are distributed in 5 Shard. The response time for searching 100k Documents/Shard will be 2 seconds. So, the response time for searching all 500k Documents that are distributed amongst the 5 shard will still be 2 seconds.","title":"Elasticsearch Architecture: Key Components"},{"location":"weekly/ELK/elk/#kibana","text":"Kibana is a data visualization and exploration tool used for log and time-series analytics, application monitoring, and operational intelligence use cases. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support. Kibana Aggregations There are two main types of aggregations Metric aggregations are used to calculate a value for each bucket based on the documents inside the bucket. Bucket aggregations are used groups documents together in one bucket according to your logic and requirements, while the Metric aggregations For More information on Kibana,: Click link","title":"Kibana"},{"location":"weekly/ELK/elk/#adding-data-into-elasticsearch","text":"The main consideration for indexing or adding data into Elasticsearch largely depends on whether you are indexing general content or timestamped data. General content: Index content like HTML pages, catalogs and other files. Send data directly to Elasticseach from your application using an Elastic language client. Otherwise use Workplace Search content connectors or the Enterprise Search web crawler. Timestamped data: The preferred way to index timestamped data is to use Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host.","title":"Adding data into Elasticsearch"},{"location":"weekly/ELK/elk/#how-to-ingest-data-into-elasticsearch-service","text":"There are 4 main ways in Adding Data to ElasticSearch Service: By using Beats and Logstash By using Kibana By programmatically indexing data Manually adding data Read more about data ingestion in ElasticSearch Click Link","title":"How to ingest data into Elasticsearch Service"},{"location":"weekly/ELK/elk/#the-index","text":"The index is the basis of Elasticsearch, it is where you store your documents. You can compare it to a table in a database. An index has settings and a mapping which defines how to store and index the documents.","title":"The index"},{"location":"weekly/ELK/elk/#mapping","text":"Mapping is the process of defining how a document, and the fields it contains, are stored and indexed. Each document is a collection of fields, which each have their own data type. When mapping your data, you create a mapping definition, which contains a list of fields that are pertinent to the document.","title":"Mapping"},{"location":"weekly/ELK/elk/#what-is-elastic-integrations","text":"Elastic integrations are a streamlined way to connect your data to the Elastic Stack. Integrations are available for popular services and platforms, like Nginx, AWS, and MongoDB, as well as many generic input types like log files. For more on Data Integration Click link:","title":"What is Elastic integrations"},{"location":"weekly/ELK/elk/#elastic-agent","text":"Elastic integrations are powered by Elastic Agent. Elastic Agent is a single, unified way to add monitoring for logs, metrics, and other types of data to a host. It can also protect hosts from security threats, query data from operating systems, forward data from remote services or hardware, and more.","title":"Elastic Agent"},{"location":"weekly/ELK/elk/#sample-hands-on","text":"I will : Use curl commands to talk to the Elasticsearch REST API. Or if you choose to use Kibana Devtools, thats also ok, because Kibana\u2019s Devtools can automatically recognizes and converts curl commands . create an index Called middleware curl -X PUT \u201clocalhost:9200/middleware Delete the index Called middleware curl -X DELETE \u201clocalhost:9200/middleware\u201d Create an index Called middleware with just one shard and no replicas PUT /middleware?pretty { \"settings\" : { \"number_of_shards\" : 1, \"number_of_replicas\" : 0 } } loading data into that index Called middleware PUT /middleware/_mapping { \"properties\": { \"post_time\": { \"type\": \"date\" }, \"username\": { \"type\": \"keyword\" }, \"message\": { \"type\": \"text\" } } } Now Load some data Manually POST /middleware/_doc/?pretty { \"user\" : \"collins\", \"post_date\" : \"2023-16-03T14:10:10\", \"message\" : \"Welcome to Elasticsearch?\" } see this mediam link, its cool Click-Link","title":"Sample Hands-on"},{"location":"weekly/GitHub/github/","text":"What GitHub? git config git config --global user.name \"sosotechnologies\" git config --global user.email \"cafanwi@sosotechnologies.com\" git config --global \u2013-list Changing GIT Remote ~~~git remote -``` ***OUTPUT*** [ec2-user@ip-172-31-201-145 terraform-efs]$ git remote -v origin https://github.com/sosotechnologies/terraform-EFS-Dynamic.git (fetch) origin https://github.com/sosotechnologies/terraform-EFS-Dynamic.git (push) git remote rm origin","title":"GitHub"},{"location":"weekly/GitHub/github/#what-github","text":"","title":"What GitHub?"},{"location":"weekly/GitHub/github/#git-config","text":"git config --global user.name \"sosotechnologies\" git config --global user.email \"cafanwi@sosotechnologies.com\" git config --global \u2013-list Changing GIT Remote ~~~git remote -``` ***OUTPUT*** [ec2-user@ip-172-31-201-145 terraform-efs]$ git remote -v origin https://github.com/sosotechnologies/terraform-EFS-Dynamic.git (fetch) origin https://github.com/sosotechnologies/terraform-EFS-Dynamic.git (push) git remote rm origin","title":"git config"},{"location":"weekly/GitOps/gitops/","text":"GitOps my repo: link Example 1 my Github name is: sosotechnologies Create a new repo called: infra create a github personal Access Token(classic) called: flux Create a kubernetes cluster Bootstrap the cluster watch video: link save creds as env var export GITHUB_TOKEN=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 export GITHUB_USER=sosotechnologies export GITHUB_REPO=infra echo $GITHUB_USER echo $GITHUB_REPO echo $GITHUB_TOKEN create an eks cluster My terraform link: LINK bootstrap cluster, this bootstrap will also install flux flux bootstrap github --owner=sosotechnologies --repository=sosoflux-infra --branch=main --path=./florida/miami --personal true You should see this structure in your github repo: Next: - generate an ssh key from your ec2 terminal - Go to your github --> settings --> SSH and GPG keys and paste [id_rsa.pub] content. - clone the infra repo with [ssh] ssh-keygen cd .ssh cat id_rsa.pub git clone git@github.com:sosotechnologies/sosoflux-infra.git cd sosoflux-infra [ec2-user@ip-172-31-145-18 infra]$ tree . \u2514\u2500\u2500 florida \u2514\u2500\u2500 miami \u2514\u2500\u2500 flux-system \u251c\u2500\u2500 gotk-components.yaml \u251c\u2500\u2500 gotk-sync.yaml \u2514\u2500\u2500 kustomization.yaml add a demo folder and a yaml file in the folder [ec2-user@ip-172-31-145-18 eks]$ cd florida [ec2-user@ip-172-31-145-18 eks]$ mkdir demo && cd demo && touch sosodocs.yaml \u2514\u2500\u2500 florida \u2514\u2500\u2500 miami \u251c\u2500\u2500 demo \u2502 \u2514\u2500\u2500 sosodocs.yaml \u2514\u2500\u2500 flux-system \u251c\u2500\u2500 gotk-components.yaml \u251c\u2500\u2500 gotk-sync.yaml \u2514\u2500\u2500 kustomization.yaml commit these changes to your git repo git add -A && \\ git commit -m \"added demo folder\" && \\ git push origin main Note: if you dont wanna wait for flux to deploy, use this command flux reconcile kustomization flux-system --with-source NEXT IRSA Get the OpenCD Connect Provider URL from the EKS cluster, use that to create an IDP in IAM NEXT Create an IAM role and add the IDP. Give the AmazonEC2ContainerRegistryReadOnly PERMISSION TO THE ipd ROLE, Create role Go to the role and edit the trust policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::088789840359:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/4960280A882DD4D93CCCF19F4E3A32E7\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"oidc.eks.us-east-1.amazonaws.com/id/4960280A882DD4D93CCCF19F4E3A32E7:sub\": \"system:serviceaccount:flux-system:ecr-credentials-sync\" } } } ] } Copy the arn of the newly created role: arn:aws:iam::088789840359:role/FluxECRAccess NEXT - Create a cron job. see the repo, my cronjob is in the file named ecr-job.yaml - update the role arn in the ecr-job.yaml with your own role arn. commit these changes to your git repo git add -A && \\ git commit -m \"added demo folder\" && \\ git push origin main Note: if you dont wanna wait for flux to deploy, use this command flux reconcile kustomization flux-system --with-source Check to see that the cronjob was created: kubectl get cj -n flux-system Create a sample job since our cronjob is schedule for 6 hours intervals, and we can't wait. k create job --from=cronjob/ecr-credentials-sync -n flux-system ecr-credentials-sync-init kubectl get secret -n flux-system kubectl get po -n flux-system commit these changes to your git repo [ec2-user@ip-172-31-145-18 infra]$ git add . [ec2-user@ip-172-31-145-18 infra]$ git commit -m \"added demo folder\" [ec2-user@ip-172-31-145-18 infra]$ git push Note: if you dont wanna wait for flux to deploy, use this command flux reconcile kustomization flux-system --with-source NOTE UNfortunately, I had an error: [\u2717 Kustomization reconciliation failed: ImagePolicy/flux-system/nginx dry-run failed, error: no matches for kind \"ImagePolicy\" in version \"image.toolkit.fluxcd.io/v1alpha1\"] woerking on it https://aws.amazon.com/blogs/containers/building-a-gitops-pipeline-with-amazon-eks/ flux create tenant soso-tenant4 --with-namespace team4 --export > soso4.yaml https://devopstales.github.io/kubernetes/gitops-flux2/ Official flux: Link $ docker pull nginx:1.23.4 $ aws ecr get-login-password --region=us-east-1 RESEARCH THIS IAM roles for service accounts(IRSA) Flux IRSA link: LINK When using IRSA to enable access to ECR, add the following patch to your bootstrap repository, in the flux-system/kustomization.yaml file: apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - gotk-components.yaml - gotk-sync.yaml patches: - patch: | apiVersion: v1 kind: ServiceAccount metadata: name: image-reflector-controller annotations: eks.amazonaws.com/role-arn: <role arn> target: kind: ServiceAccount name: image-reflector-controller EX 2: install GitHub Cli Create 4 repos: flux-production/apps flux-staging/apps flux-fleet[with-bootstrapping] devops-toolkit/apps[repo-already-exists] clone them separately create namespaces: production and staging create prod and staging source files in the [app] folder create a kustomization for prod and staging source files in the [app] folder Install Git Cli wget https://github.com/cli/cli/releases/download/v2.15.0/gh_2.15.0_linux_amd64.rpm sudo rpm -i gh_2.15.0_linux_amd64.rpm gh --version copy key and create in Github-SSH, then authenticate GH gh auth login save Git creds as env var export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxx export GITHUB_USER=sosotechnologies echo $GITHUB_USER echo $GITHUB_TOKEN Repo 1. mkdir -p flux-production/apps cd flux-production git init gh repo create echo \"commit Readme\" | tee README.md git add . && \\ git commit -m \"added prod folder\" && \\ git push --set-upstream origin master Repo 2 mkdir -p flux-staging/apps cd flux-staging git init gh repo create [Select: Push an existing local repository to GitHub] echo \"commit Readme\" | tee README.md git add . && \\ git commit -m \"added stag folder\" && \\ git push --set-upstream origin master kubectl create ns production kubectl create ns staging Repo 3 Remember the path defined here [apps] is the app folder I created in staging and production repos, bootstrap will: - install flux - Create a github repo - create the deploy keys ... flux bootstrap github --owner sosotechnologies --repository flux-fleet --branch main --path apps --personal true See the resources that were created from the bootstrap kubectl get po -n flux-system kubectl get svc -n flux-system kubectl get secrets -n flux-system kubectl get cm -n flux-system Cd into flux-fleet directory and create the below resources staging source and Kustomization production source and Kustomization devops-toolkit git clone git@github.com:sosotechnologies/flux-fleet.git cd flux-fleet So far my flux-weekend folder looks like so: Create Kustomization and Source in the flux-fleet/ folder - Create source and Kustomize for staging flux create source git staging --url https://github.com/sosotechnologies/flux-staging --branch master --interval 30s --export | tee apps/staging.yaml Kustomize staging to same file: apps/staging.yaml flux create kustomization staging --source staging --path \"./\" --prune true --interval 10m --export | tee -a apps/staging.yaml Create source and Kustomize for production flux create source git production --url https://github.com/sosotechnologies/flux-production --branch master --interval 30s --export | tee apps/production.yaml Kustomize production to same file: apps/production.yaml flux create kustomization production --source production --path \"./\" --prune true --interval 10m --export | tee -a apps/production.yaml repo 4 Create devops-toolkit in thesame in the flux-fleet/ folder flux create source git devops-toolkit --url=https://github.com/sosotechnologies/devops-toolkit --branch=master --interval=30s --export | tee apps/devops-toolkit.yaml git add . && \\ git commit -m \"added staging folder, production folder and devops-toolkit\" && \\ git push --set-upstream origin main Now my flux-weekend folder looks like watch flux get sources git flux get kustomizations setup is done! NEXT: Create Helm Releases Staging Release cd flux-staging Copy this command and fun as is: echo \"image: tag: 2.9.9 ingress: host: staging.devops-toolkit.$INGRESS_HOST.nip.io\" \\ | tee values.yaml flux create helmrelease devops-toolkit-staging --source GitRepository/devops-toolkit --values values.yaml --chart \"helm\" --target-namespace staging --interval 30s --export | tee apps/devops-toolkit.yaml rm values.yaml git add . && \\ git commit -m \"added staging helm release\" && \\ git push --set-upstream origin master watch flux get helmreleases kubectl --namespace staging get pods NOTE: You can change the image tag in the staging: devops-toolkit.yaml From: tag: 2.9.9 --> tag: 2.9.17 And [commit and push to Git] and flux will automatically detect and deploy. Production Release cd .. cd flux-production Copy this command and fun as is: echo \"image: tag: 2.9.17 ingress: host: production.devops-toolkit.$INGRESS_HOST.nip.io\" \\ | tee values.yaml flux create helmrelease devops-toolkit-production --source GitRepository/devops-toolkit --values values.yaml --chart \"helm\" --target-namespace production --interval 30s --export | tee apps/devops-toolkit.yaml rm values.yaml git add . && \\ git commit -m \"added production helm release\" && \\ git push --set-upstream origin master flux get helmreleases watch kubectl --namespace production get pods Final Tree IT'S ALL FOLKS! NEXT TASK Configure ECR/OICD-IRSA/GIT tagging for CD deployment aws ecr list-images --repository=soso-repository kubectl create job --from=cronjob/ecr-credentials-sync -n flux-system ecr-credentials-sync-init --dry-run=client -o yaml > job.yaml","title":"GitOps"},{"location":"weekly/GitOps/gitops/#gitops","text":"my repo: link","title":"GitOps"},{"location":"weekly/GitOps/gitops/#example-1","text":"my Github name is: sosotechnologies Create a new repo called: infra create a github personal Access Token(classic) called: flux Create a kubernetes cluster Bootstrap the cluster watch video: link save creds as env var export GITHUB_TOKEN=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 export GITHUB_USER=sosotechnologies export GITHUB_REPO=infra echo $GITHUB_USER echo $GITHUB_REPO echo $GITHUB_TOKEN create an eks cluster My terraform link: LINK bootstrap cluster, this bootstrap will also install flux flux bootstrap github --owner=sosotechnologies --repository=sosoflux-infra --branch=main --path=./florida/miami --personal true You should see this structure in your github repo: Next: - generate an ssh key from your ec2 terminal - Go to your github --> settings --> SSH and GPG keys and paste [id_rsa.pub] content. - clone the infra repo with [ssh] ssh-keygen cd .ssh cat id_rsa.pub git clone git@github.com:sosotechnologies/sosoflux-infra.git cd sosoflux-infra [ec2-user@ip-172-31-145-18 infra]$ tree . \u2514\u2500\u2500 florida \u2514\u2500\u2500 miami \u2514\u2500\u2500 flux-system \u251c\u2500\u2500 gotk-components.yaml \u251c\u2500\u2500 gotk-sync.yaml \u2514\u2500\u2500 kustomization.yaml add a demo folder and a yaml file in the folder [ec2-user@ip-172-31-145-18 eks]$ cd florida [ec2-user@ip-172-31-145-18 eks]$ mkdir demo && cd demo && touch sosodocs.yaml \u2514\u2500\u2500 florida \u2514\u2500\u2500 miami \u251c\u2500\u2500 demo \u2502 \u2514\u2500\u2500 sosodocs.yaml \u2514\u2500\u2500 flux-system \u251c\u2500\u2500 gotk-components.yaml \u251c\u2500\u2500 gotk-sync.yaml \u2514\u2500\u2500 kustomization.yaml commit these changes to your git repo git add -A && \\ git commit -m \"added demo folder\" && \\ git push origin main Note: if you dont wanna wait for flux to deploy, use this command flux reconcile kustomization flux-system --with-source NEXT IRSA Get the OpenCD Connect Provider URL from the EKS cluster, use that to create an IDP in IAM NEXT Create an IAM role and add the IDP. Give the AmazonEC2ContainerRegistryReadOnly PERMISSION TO THE ipd ROLE, Create role Go to the role and edit the trust policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::088789840359:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/4960280A882DD4D93CCCF19F4E3A32E7\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"oidc.eks.us-east-1.amazonaws.com/id/4960280A882DD4D93CCCF19F4E3A32E7:sub\": \"system:serviceaccount:flux-system:ecr-credentials-sync\" } } } ] } Copy the arn of the newly created role: arn:aws:iam::088789840359:role/FluxECRAccess NEXT - Create a cron job. see the repo, my cronjob is in the file named ecr-job.yaml - update the role arn in the ecr-job.yaml with your own role arn. commit these changes to your git repo git add -A && \\ git commit -m \"added demo folder\" && \\ git push origin main Note: if you dont wanna wait for flux to deploy, use this command flux reconcile kustomization flux-system --with-source Check to see that the cronjob was created: kubectl get cj -n flux-system Create a sample job since our cronjob is schedule for 6 hours intervals, and we can't wait. k create job --from=cronjob/ecr-credentials-sync -n flux-system ecr-credentials-sync-init kubectl get secret -n flux-system kubectl get po -n flux-system commit these changes to your git repo [ec2-user@ip-172-31-145-18 infra]$ git add . [ec2-user@ip-172-31-145-18 infra]$ git commit -m \"added demo folder\" [ec2-user@ip-172-31-145-18 infra]$ git push Note: if you dont wanna wait for flux to deploy, use this command flux reconcile kustomization flux-system --with-source NOTE UNfortunately, I had an error: [\u2717 Kustomization reconciliation failed: ImagePolicy/flux-system/nginx dry-run failed, error: no matches for kind \"ImagePolicy\" in version \"image.toolkit.fluxcd.io/v1alpha1\"] woerking on it https://aws.amazon.com/blogs/containers/building-a-gitops-pipeline-with-amazon-eks/ flux create tenant soso-tenant4 --with-namespace team4 --export > soso4.yaml https://devopstales.github.io/kubernetes/gitops-flux2/ Official flux: Link $ docker pull nginx:1.23.4 $ aws ecr get-login-password --region=us-east-1 RESEARCH THIS IAM roles for service accounts(IRSA) Flux IRSA link: LINK When using IRSA to enable access to ECR, add the following patch to your bootstrap repository, in the flux-system/kustomization.yaml file: apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - gotk-components.yaml - gotk-sync.yaml patches: - patch: | apiVersion: v1 kind: ServiceAccount metadata: name: image-reflector-controller annotations: eks.amazonaws.com/role-arn: <role arn> target: kind: ServiceAccount name: image-reflector-controller","title":"Example 1"},{"location":"weekly/GitOps/gitops/#ex-2","text":"install GitHub Cli Create 4 repos: flux-production/apps flux-staging/apps flux-fleet[with-bootstrapping] devops-toolkit/apps[repo-already-exists] clone them separately create namespaces: production and staging create prod and staging source files in the [app] folder create a kustomization for prod and staging source files in the [app] folder Install Git Cli wget https://github.com/cli/cli/releases/download/v2.15.0/gh_2.15.0_linux_amd64.rpm sudo rpm -i gh_2.15.0_linux_amd64.rpm gh --version copy key and create in Github-SSH, then authenticate GH gh auth login save Git creds as env var export GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxx export GITHUB_USER=sosotechnologies echo $GITHUB_USER echo $GITHUB_TOKEN","title":"EX 2:"},{"location":"weekly/GitOps/gitops/#repo-1","text":"mkdir -p flux-production/apps cd flux-production git init gh repo create echo \"commit Readme\" | tee README.md git add . && \\ git commit -m \"added prod folder\" && \\ git push --set-upstream origin master","title":"Repo 1."},{"location":"weekly/GitOps/gitops/#repo-2","text":"mkdir -p flux-staging/apps cd flux-staging git init gh repo create [Select: Push an existing local repository to GitHub] echo \"commit Readme\" | tee README.md git add . && \\ git commit -m \"added stag folder\" && \\ git push --set-upstream origin master kubectl create ns production kubectl create ns staging","title":"Repo 2"},{"location":"weekly/GitOps/gitops/#repo-3","text":"Remember the path defined here [apps] is the app folder I created in staging and production repos, bootstrap will: - install flux - Create a github repo - create the deploy keys ... flux bootstrap github --owner sosotechnologies --repository flux-fleet --branch main --path apps --personal true See the resources that were created from the bootstrap kubectl get po -n flux-system kubectl get svc -n flux-system kubectl get secrets -n flux-system kubectl get cm -n flux-system","title":"Repo 3"},{"location":"weekly/GitOps/gitops/#cd-into-flux-fleet-directory-and-create-the-below-resources","text":"staging source and Kustomization production source and Kustomization devops-toolkit git clone git@github.com:sosotechnologies/flux-fleet.git cd flux-fleet So far my flux-weekend folder looks like so: Create Kustomization and Source in the flux-fleet/ folder - Create source and Kustomize for staging flux create source git staging --url https://github.com/sosotechnologies/flux-staging --branch master --interval 30s --export | tee apps/staging.yaml Kustomize staging to same file: apps/staging.yaml flux create kustomization staging --source staging --path \"./\" --prune true --interval 10m --export | tee -a apps/staging.yaml Create source and Kustomize for production flux create source git production --url https://github.com/sosotechnologies/flux-production --branch master --interval 30s --export | tee apps/production.yaml Kustomize production to same file: apps/production.yaml flux create kustomization production --source production --path \"./\" --prune true --interval 10m --export | tee -a apps/production.yaml","title":"Cd into flux-fleet directory and create the below resources"},{"location":"weekly/GitOps/gitops/#repo-4","text":"Create devops-toolkit in thesame in the flux-fleet/ folder flux create source git devops-toolkit --url=https://github.com/sosotechnologies/devops-toolkit --branch=master --interval=30s --export | tee apps/devops-toolkit.yaml git add . && \\ git commit -m \"added staging folder, production folder and devops-toolkit\" && \\ git push --set-upstream origin main Now my flux-weekend folder looks like watch flux get sources git flux get kustomizations setup is done! NEXT: Create Helm Releases Staging Release cd flux-staging Copy this command and fun as is: echo \"image: tag: 2.9.9 ingress: host: staging.devops-toolkit.$INGRESS_HOST.nip.io\" \\ | tee values.yaml flux create helmrelease devops-toolkit-staging --source GitRepository/devops-toolkit --values values.yaml --chart \"helm\" --target-namespace staging --interval 30s --export | tee apps/devops-toolkit.yaml rm values.yaml git add . && \\ git commit -m \"added staging helm release\" && \\ git push --set-upstream origin master watch flux get helmreleases kubectl --namespace staging get pods NOTE: You can change the image tag in the staging: devops-toolkit.yaml From: tag: 2.9.9 --> tag: 2.9.17 And [commit and push to Git] and flux will automatically detect and deploy. Production Release cd .. cd flux-production Copy this command and fun as is: echo \"image: tag: 2.9.17 ingress: host: production.devops-toolkit.$INGRESS_HOST.nip.io\" \\ | tee values.yaml flux create helmrelease devops-toolkit-production --source GitRepository/devops-toolkit --values values.yaml --chart \"helm\" --target-namespace production --interval 30s --export | tee apps/devops-toolkit.yaml rm values.yaml git add . && \\ git commit -m \"added production helm release\" && \\ git push --set-upstream origin master flux get helmreleases watch kubectl --namespace production get pods Final Tree IT'S ALL FOLKS! NEXT TASK Configure ECR/OICD-IRSA/GIT tagging for CD deployment aws ecr list-images --repository=soso-repository kubectl create job --from=cronjob/ecr-credentials-sync -n flux-system ecr-credentials-sync-init --dry-run=client -o yaml > job.yaml","title":"repo 4"},{"location":"weekly/Gitaction/gitaction/","text":"Getting Started GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production. Github actions has three main components: Events -> Workflows -> Actions Workflow triggers are events that cause a workflow to run. Events provide status updates from GitHub repos from code commits or other event of a project. For more information on workflows. see site-link: For more information about how to use workflow triggers: See workflow triggers Description of the below yaml file name: is straightforward, you can name what ever name you choose The events is the line thet starts with On. In the yaml file, we have an event to push. actions/checkout@v3: is already defined in Github. You can see more Here: github actions create a personal access token in your HitHub account and name: PERSONAL_TOKEN name: Publish docs via GitHub Pages on: push: branches: - main permissions: contents: write jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - uses: actions/cache@v2 with: key: ${{ github.ref }} path: .cache - run: pip install mkdocs-material - run: mkdocs gh-deploy --force env: PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }} CONFIG_FILE: folder/mkdocs.yml #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} .github/workflows mkdocs github actions managing-a-custom-domain","title":"GitHub Action"},{"location":"weekly/Gitaction/gitaction/#getting-started","text":"GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production. Github actions has three main components: Events -> Workflows -> Actions Workflow triggers are events that cause a workflow to run. Events provide status updates from GitHub repos from code commits or other event of a project. For more information on workflows. see site-link: For more information about how to use workflow triggers: See workflow triggers","title":"Getting Started"},{"location":"weekly/Gitaction/gitaction/#description-of-the-below-yaml-file","text":"name: is straightforward, you can name what ever name you choose The events is the line thet starts with On. In the yaml file, we have an event to push. actions/checkout@v3: is already defined in Github. You can see more Here: github actions create a personal access token in your HitHub account and name: PERSONAL_TOKEN name: Publish docs via GitHub Pages on: push: branches: - main permissions: contents: write jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: 3.x - uses: actions/cache@v2 with: key: ${{ github.ref }} path: .cache - run: pip install mkdocs-material - run: mkdocs gh-deploy --force env: PERSONAL_TOKEN: ${{ secrets.PERSONAL_TOKEN }} CONFIG_FILE: folder/mkdocs.yml #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} #GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} .github/workflows mkdocs github actions managing-a-custom-domain","title":"Description of the below yaml file"},{"location":"weekly/Jenkins/jenkins/","text":"kubectl exec -i -t my-pod --container main-app -- /bin/bash [ec2-user@ip-172-31-201-145 ~]$ sudo cp agent.jar ../../opt sudo chmod 777 -R /opt java -jar agent.jar -jnlpUrl http://ab5223ff23ed749b3ac51f92d244edcb-183061686.us-east-1.elb.amazonaws.com:8080/manage/computer/macaz/jenkins-agent.jnlp -secret 32d9d58f3296eb3d80c4d37836f5ed5ee6ed6cbfba258ad7e5c7c6e291f0ac37 -workDir \"/opt/build\" & Install Jdk click-here https://www.ashnik.com/install-jenkins-on-aws-ec2-instance-using-terraform/ after installing, you have to connect slave with the master if your jenkins server restarts and you forgot the password, run this command: sudo cat /var/lib/jenkins/users","title":"Jenkins"},{"location":"weekly/Kafka/kafka/","text":"Kafka Apache Kafka is a distributed event store and stream-processing platform. Source Systems --> stream data to Kafka --> Kafka --> target systems consume the data dtored in Kafka Kafka and Zookeeper Kafka and ZooKeeper work in conjunction to form a complete Kafka Cluster \u2060\u2014 with ZooKeeper providing the distributed clustering services, and Kafka handling the actual data streams and connectivity to clients. ZooKeeper kinda handles the leadership election of Kafka brokers and manages service discovery as well as cluster topology so each broker knows when brokers have entered or exited the cluster, when a broker dies and who the preferred leader node is for a given topic/partition pair. It also tracks when topics are created or deleted from the cluster and maintains a topic list. In general, ZooKeeper provides an in-sync view of the Kafka cluster. Kafka, on the other hand, is dedicated to handling the actual connections from the clients (producers and consumers) as well as managing the topic logs, topic log partitions, consumer groups ,and individual offsets. Forward messages from Pub/Sub to Kafka Use the gcloud CLI to create a Pub/Sub topic with a subscription. ... Open the file named /config/cps-source-connector.properties in a text editor. ... From the Kafka directory, run the following command: ... Use the gcloud CLI to publish a message to Pub/Sub. ... Read the message from Kafka. Streaming Kafka Messages to Google Cloud Pub/Sub Publisher publish messages that subscriber consumed. (a) Firstly, Configure the Pub/Sub topics to communicate with Kafka: Secondly to create a subscription for the to-kafka topic: create a subscription for traffic published from Kafka for Data exchange between Kafka and Pub/Sub BigQuery is Google's fully managed, serverless data warehouse that enables scalable analysis over petabytes of data.","title":"Kafka"},{"location":"weekly/Kafka/kafka/#kafka","text":"Apache Kafka is a distributed event store and stream-processing platform. Source Systems --> stream data to Kafka --> Kafka --> target systems consume the data dtored in Kafka","title":"Kafka"},{"location":"weekly/Kafka/kafka/#kafka-and-zookeeper","text":"Kafka and ZooKeeper work in conjunction to form a complete Kafka Cluster \u2060\u2014 with ZooKeeper providing the distributed clustering services, and Kafka handling the actual data streams and connectivity to clients. ZooKeeper kinda handles the leadership election of Kafka brokers and manages service discovery as well as cluster topology so each broker knows when brokers have entered or exited the cluster, when a broker dies and who the preferred leader node is for a given topic/partition pair. It also tracks when topics are created or deleted from the cluster and maintains a topic list. In general, ZooKeeper provides an in-sync view of the Kafka cluster. Kafka, on the other hand, is dedicated to handling the actual connections from the clients (producers and consumers) as well as managing the topic logs, topic log partitions, consumer groups ,and individual offsets.","title":"Kafka and Zookeeper"},{"location":"weekly/Kafka/kafka/#forward-messages-from-pubsub-to-kafka","text":"Use the gcloud CLI to create a Pub/Sub topic with a subscription. ... Open the file named /config/cps-source-connector.properties in a text editor. ... From the Kafka directory, run the following command: ... Use the gcloud CLI to publish a message to Pub/Sub. ... Read the message from Kafka.","title":"Forward messages from Pub/Sub to Kafka"},{"location":"weekly/Kafka/kafka/#streaming-kafka-messages-to-google-cloud-pubsub","text":"Publisher publish messages that subscriber consumed. (a) Firstly, Configure the Pub/Sub topics to communicate with Kafka: Secondly to create a subscription for the to-kafka topic: create a subscription for traffic published from Kafka for Data exchange between Kafka and Pub/Sub","title":"Streaming Kafka Messages to Google Cloud Pub/Sub"},{"location":"weekly/Kafka/kafka/#bigquery","text":"is Google's fully managed, serverless data warehouse that enables scalable analysis over petabytes of data.","title":"BigQuery"},{"location":"weekly/Kubernetes/kubernetes/","text":"Minikube choco install minikube minikube start kubectl config get-contexts kubectl config use-context docker-desktop KOPS Installing kOps curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64 chmod +x ./kops sudo mv ./kops /usr/local/bin/ Install Python PiP sudo apt -y update sudo apt install python3-pip Install AWSCli sudo apt install awscli -y Install Kubectl curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl Create S3 Bucket aws s3 ls aws s3 mb s3://soso-kops-bucket.local OPTIONAL aws s3api create-bucket \\ --bucket soso-kops-bucket \\ --region us-east-1 Add env variables in bashrc vi .bashrc export NAME=soso-kops-bucket.k8s.local export KOPS_STATE_STORE=s3://soso-kops-bucket.local source .bashrc Generate an sshkeys before creating cluster ssh-keygen Install Cluster preconfig file kops create cluster \\ --name=${NAME} \\ --cloud=aws \\ --zones=us-west-2a \\ --node-size t2.medium \\ --master-size t2.medium \\ --master-count 1 --node-size t2.medium --node-count=1 Edit the cluster name- OPTIONAL kops edit cluster --name ${NAME} Install Cluster kops update cluster --name ${NAME} --yes --admin NOTE: WAIT 10 Mins before Checking Nodes and Validating cluster * Get nodes kubectl get nodes Validate Cluster kops validate cluster --wait 10m Delete cluster kops delete cluster --name ${NAME} [OR] kops delete cluster --name ${NAME} --yes To deploy EFS-EKS-using Terraform, clone My repo You can clone and build this Documentation and use the image mkdocs-docker-build EKS AWS Linux 2 AMD x86_64 Install AWSCLI Install Kubectl version 1.23 EKS Version installed 1.24 Install Git Install Terraform Configure AWS Install Helm Install AWSCLI Right-Click to open Link in a New Tab curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Install Kubectl version 1.23 Right-Click to open Link in a New Tab kubectl version --short --client curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.11/2023-03-17/bin/linux/amd64/kubectl chmod +x ./kubectl mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin Install Terraform Right-Click to open Link in a New Tab Install Git Suso yum install git -y Install HelM Right-Click to open Link in a New Tab curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh Configure KubeConfig aws eks update-kubeconfig --region us-east-1 --name DevOps-prod-SoSo-Eks kubectl apply -f .","title":"Minikube"},{"location":"weekly/Kubernetes/kubernetes/#minikube","text":"choco install minikube minikube start kubectl config get-contexts kubectl config use-context docker-desktop","title":"Minikube"},{"location":"weekly/Kubernetes/kubernetes/#kops","text":"Installing kOps curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-linux-amd64 chmod +x ./kops sudo mv ./kops /usr/local/bin/ Install Python PiP sudo apt -y update sudo apt install python3-pip Install AWSCli sudo apt install awscli -y Install Kubectl curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl Create S3 Bucket aws s3 ls aws s3 mb s3://soso-kops-bucket.local OPTIONAL aws s3api create-bucket \\ --bucket soso-kops-bucket \\ --region us-east-1 Add env variables in bashrc vi .bashrc export NAME=soso-kops-bucket.k8s.local export KOPS_STATE_STORE=s3://soso-kops-bucket.local source .bashrc Generate an sshkeys before creating cluster ssh-keygen Install Cluster preconfig file kops create cluster \\ --name=${NAME} \\ --cloud=aws \\ --zones=us-west-2a \\ --node-size t2.medium \\ --master-size t2.medium \\ --master-count 1 --node-size t2.medium --node-count=1 Edit the cluster name- OPTIONAL kops edit cluster --name ${NAME} Install Cluster kops update cluster --name ${NAME} --yes --admin NOTE: WAIT 10 Mins before Checking Nodes and Validating cluster * Get nodes kubectl get nodes Validate Cluster kops validate cluster --wait 10m Delete cluster kops delete cluster --name ${NAME} [OR] kops delete cluster --name ${NAME} --yes To deploy EFS-EKS-using Terraform, clone My repo You can clone and build this Documentation and use the image mkdocs-docker-build","title":"KOPS"},{"location":"weekly/Kubernetes/kubernetes/#eks","text":"AWS Linux 2 AMD x86_64 Install AWSCLI Install Kubectl version 1.23 EKS Version installed 1.24 Install Git Install Terraform Configure AWS Install Helm","title":"EKS"},{"location":"weekly/Kubernetes/kubernetes/#install-awscli","text":"Right-Click to open Link in a New Tab curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install","title":"Install AWSCLI"},{"location":"weekly/Kubernetes/kubernetes/#install-kubectl-version-123","text":"Right-Click to open Link in a New Tab kubectl version --short --client curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.24.11/2023-03-17/bin/linux/amd64/kubectl chmod +x ./kubectl mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin","title":"Install Kubectl version 1.23"},{"location":"weekly/Kubernetes/kubernetes/#install-terraform","text":"Right-Click to open Link in a New Tab","title":"Install Terraform"},{"location":"weekly/Kubernetes/kubernetes/#install-git","text":"Suso yum install git -y","title":"Install Git"},{"location":"weekly/Kubernetes/kubernetes/#install-helm","text":"Right-Click to open Link in a New Tab curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh","title":"Install HelM"},{"location":"weekly/Kubernetes/kubernetes/#configure-kubeconfig","text":"aws eks update-kubeconfig --region us-east-1 --name DevOps-prod-SoSo-Eks kubectl apply -f .","title":"Configure KubeConfig"},{"location":"weekly/Openshift/openshift/","text":"What's Red Hat OpenShift Red Hat OpenShift, the industry's leading hybrid cloud application platform powered by Kubernetes, brings together tested and trusted services to reduce the friction of developing, modernizing, deploying, running, and managing applications. See official link: Openshift Getting Started - Steps Signup/signin to Openshift account: See-Link Navigate to: Clusters -> Cluster Type -> Amazon Web Services -> Installer-provisioned infrastructure Install OpenShift on AWS with user-provisioned infrastructure Select the option [Full control] See the site: /openshift/install/aws Install OpenShift on AWS Setup an AWS Instance and add security group number [6443] Setup a Route53 DNS make a new directory for the installation, mine is soso-dir mkdir soso-dir/ AWS Openshift installation Link: See-Link There are Two Cluster-Setup options to choose: a customized cluster or quickly install. Install the Openshift installer and client on your local computer. Move that file to your remote linux machine. You're see the client and installer. Extract the installer and client: tar xvf openshift-install-linux.tar.gz tar xvf openshift-client-linux.tar.gz Move the oc, kubectl and installer to usr/bin directoory sudo mv oc /usr/local/bin/ sudo mv kubectl /usr/local/bin/ sudo cp openshift-install /usr/local/bin/ sudo cp openshift-install /root/ which openshift-install oc help Install AWSCLI curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Copy the secret content and paste in a file. This secret is from the redhad account, used to Map the AWS Cluster with the redHat account. In the AWS Console, setup the following: Route53 public Hoster Zone Add OpenShift port 6443 to instance security groups Create Create Access and Secret Keys for the Root-user and configure aws aws configure CD to ROOT and Create an SSH Key in the root directory sudo su - ssh-keygen -t rsa -b 4096 -N '' -f id_rsa Evaluate and add to root. This will starts ssh-agent and configures the environment (via eval) of the running shell to point to that agent. eval \"$(ssh-agent -s)\" ssh-add /root/id_rsa Now start the prompt to install Openshift install Openshift using the config, so you can customize . A prompt will begin. The last prompt will be the secret Copy and paste the secret characters that we had saved earlier. openshift-install create install-config OPTIONAL: Make a copy of the file: cp -r install-config.yaml soso-config.yaml paste the below content in the file. edit the file to suite ur options. Note : Two keys will be added to the yaml file: The sshKey: cat id_rsa.pub The openShift Key we copied and pasted. apiVersion: v1 baseDomain: macazzzzz.com controlPlane: hyperthreading: Enabled name: master platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 3 compute: - hyperthreading: Enabled name: worker platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 1 metadata: name: openshift networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.0.0/20 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: aws: region: us-east-1 userTags: adminContact: Collins costCenter: 123 email: cafanwi@sosotechnologies.com publish: External pullSecret: '' sshKey: Install the cluster In the same directory of the openshift-install file, Run command to install: openshift-install create cluster --log-level debug ***Cluster done installing, you should see as below image, your creds and url. After installation, you should have your results as seen in the below image: Cat and export the konfig file cat auth/kubeconfig export KUBECONFIG=/root/auth/kubeconfig oc whoami Get Cluster URL with the below command oc cluster-info cd auth/ Working on cluster Some command commands: Create a new project called soso-project oc new-project soso-project --display-name 'Soso Project' oc project soso-project Sample use case: - Clone this repo: My docs dockerfile repo - Install Docker on terminal - Create a repo in Dockerdesktop called: sosotech/docs-repo-sosodocs docker build -t sosodocs . docker tag sosodocs sosotech/docs-repo/sosodocs:v1 oc cluster-info Openshift Image oc get is oc new-app --image=\"sosotech/docs-repo/sosodocs:v1\" --as-deployment-config oc expose service/docs-repo-sosodocs OTHER COMMANDS 1. Delete image string called: macaz oc delete is macaz Sample use case: - Clone this repo: My docs dockerfile repo - Install Docker/podman: docker build -t sosodocs . oc cluster-info sudo docker tag sosodocs default-route-openshift-image-registry.apps.openshift.macazzz.com/sosorepo:v1 docker login docker search registry.redhat.io/nginx For more commands on OpenShift: See the developer-cli-commands Link AWS Openshift installation Link: See-Link ./openshift-install create cluster --dir /root/ --log-level debug ### Destroy the cluster ./openshift-install destroy cluster --dir /root/ --log-level debug web console Link: https://console-openshift-console.apps.openshift.macazzz.com/k8s/ns/soso-project/image.openshift.io~v1~ImageStream kubeadmin 7JdHf-X34nV-ubmSh-ijSA4 docker-registry-default.127.0.0.1.nip.io Deploy Image Docker config path sudo vi /root/.docker/config.json OC TROUBLESHOOT oc get service -n default kubernetes -o 'jsonpath={.spec.clusterIP}'","title":"OpenShift"},{"location":"weekly/Openshift/openshift/#whats-red-hat-openshift","text":"Red Hat OpenShift, the industry's leading hybrid cloud application platform powered by Kubernetes, brings together tested and trusted services to reduce the friction of developing, modernizing, deploying, running, and managing applications. See official link: Openshift","title":"What's Red Hat OpenShift"},{"location":"weekly/Openshift/openshift/#getting-started-steps","text":"Signup/signin to Openshift account: See-Link Navigate to: Clusters -> Cluster Type -> Amazon Web Services -> Installer-provisioned infrastructure Install OpenShift on AWS with user-provisioned infrastructure Select the option [Full control] See the site: /openshift/install/aws","title":"Getting Started - Steps"},{"location":"weekly/Openshift/openshift/#install-openshift-on-aws","text":"Setup an AWS Instance and add security group number [6443] Setup a Route53 DNS make a new directory for the installation, mine is soso-dir mkdir soso-dir/ AWS Openshift installation Link: See-Link There are Two Cluster-Setup options to choose: a customized cluster or quickly install. Install the Openshift installer and client on your local computer. Move that file to your remote linux machine. You're see the client and installer. Extract the installer and client: tar xvf openshift-install-linux.tar.gz tar xvf openshift-client-linux.tar.gz Move the oc, kubectl and installer to usr/bin directoory sudo mv oc /usr/local/bin/ sudo mv kubectl /usr/local/bin/ sudo cp openshift-install /usr/local/bin/ sudo cp openshift-install /root/ which openshift-install oc help Install AWSCLI curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Copy the secret content and paste in a file. This secret is from the redhad account, used to Map the AWS Cluster with the redHat account. In the AWS Console, setup the following: Route53 public Hoster Zone Add OpenShift port 6443 to instance security groups Create Create Access and Secret Keys for the Root-user and configure aws aws configure CD to ROOT and Create an SSH Key in the root directory sudo su - ssh-keygen -t rsa -b 4096 -N '' -f id_rsa Evaluate and add to root. This will starts ssh-agent and configures the environment (via eval) of the running shell to point to that agent. eval \"$(ssh-agent -s)\" ssh-add /root/id_rsa Now start the prompt to install Openshift install Openshift using the config, so you can customize . A prompt will begin. The last prompt will be the secret Copy and paste the secret characters that we had saved earlier. openshift-install create install-config OPTIONAL: Make a copy of the file: cp -r install-config.yaml soso-config.yaml paste the below content in the file. edit the file to suite ur options. Note : Two keys will be added to the yaml file: The sshKey: cat id_rsa.pub The openShift Key we copied and pasted. apiVersion: v1 baseDomain: macazzzzz.com controlPlane: hyperthreading: Enabled name: master platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 3 compute: - hyperthreading: Enabled name: worker platform: aws: zones: - us-east-1a #- us-east-1b #- us-east-1c rootVolume: size: 100 type: gp2 type: m4.xlarge replicas: 1 metadata: name: openshift networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 192.168.0.0/20 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: aws: region: us-east-1 userTags: adminContact: Collins costCenter: 123 email: cafanwi@sosotechnologies.com publish: External pullSecret: '' sshKey:","title":"Install OpenShift on AWS"},{"location":"weekly/Openshift/openshift/#install-the-cluster","text":"In the same directory of the openshift-install file, Run command to install: openshift-install create cluster --log-level debug ***Cluster done installing, you should see as below image, your creds and url. After installation, you should have your results as seen in the below image: Cat and export the konfig file cat auth/kubeconfig export KUBECONFIG=/root/auth/kubeconfig oc whoami Get Cluster URL with the below command oc cluster-info cd auth/","title":"Install the cluster"},{"location":"weekly/Openshift/openshift/#working-on-cluster","text":"Some command commands: Create a new project called soso-project oc new-project soso-project --display-name 'Soso Project' oc project soso-project Sample use case: - Clone this repo: My docs dockerfile repo - Install Docker on terminal - Create a repo in Dockerdesktop called: sosotech/docs-repo-sosodocs docker build -t sosodocs . docker tag sosodocs sosotech/docs-repo/sosodocs:v1 oc cluster-info Openshift Image oc get is oc new-app --image=\"sosotech/docs-repo/sosodocs:v1\" --as-deployment-config oc expose service/docs-repo-sosodocs OTHER COMMANDS 1. Delete image string called: macaz oc delete is macaz Sample use case: - Clone this repo: My docs dockerfile repo - Install Docker/podman: docker build -t sosodocs . oc cluster-info sudo docker tag sosodocs default-route-openshift-image-registry.apps.openshift.macazzz.com/sosorepo:v1 docker login docker search registry.redhat.io/nginx For more commands on OpenShift: See the developer-cli-commands Link AWS Openshift installation Link: See-Link ./openshift-install create cluster --dir /root/ --log-level debug ### Destroy the cluster ./openshift-install destroy cluster --dir /root/ --log-level debug web console Link: https://console-openshift-console.apps.openshift.macazzz.com/k8s/ns/soso-project/image.openshift.io~v1~ImageStream kubeadmin 7JdHf-X34nV-ubmSh-ijSA4 docker-registry-default.127.0.0.1.nip.io Deploy Image Docker config path sudo vi /root/.docker/config.json OC TROUBLESHOOT oc get service -n default kubernetes -o 'jsonpath={.spec.clusterIP}'","title":"Working on cluster"},{"location":"weekly/Random/3tier/","text":"What is a 3 tier Application? A 3-tier application will compose of 3 layers: a Presentation tier, an Application tier, and a Data tier. Benefits of a 3 tier Application? The benefits of using a 3-tier architecture include improved horizontal scalability, performance, and availability. With three tiers, each part can be developed concurrently by a different team of programmers coding in different languages from the other tier developers. Because the programming for a tier can be changed or relocated without affecting the other tiers, the 3-tier model makes it easier for an enterprise or software packager to continually evolve an application as new needs and opportunities arise. Existing applications or critical parts can be permanently or temporarily retained and encapsulated within the new tier of which it becomes a component. The 3 different layers explained An example 3 tier application will compose of 3 layers: a Presentation tier, an application tier, and a Data tier. The Presentation tier is a graphical user interface (GUI) that communicates with the other two tiers\u2026in this layer, the users can directly access the web page, or an operating system's GUI. The is written in languages like built with HTML5, cascading style sheets (CSS) and JavaScript, and the presentation tier communicates with the other tiers through application program interface (API) calls. Application tier also known as the business logic tier\u2026 controls how the application functions. The application layer is written in a programming language such as Java and contains the business logic that supports the application's core functions. The underlying application tier can either be hosted on distributed servers in the cloud or on a dedicated in-house server, depending on how much processing power the application requires. Data tier contains the database servers, file shares, and anything that will be saved in the database... it can be hosted on-prem or in the cloud. Popular database systems for managing read/write access include MySQL, PostgreSQL, Microsoft Sample AWS 3-Tier Application The Presentation tier 1. A Public Route Table \u2014 associated with 2 Public Subnets (1/AZ) 2. At least 2 EC2 instances with a boot strapped Static Web Page \u2014 managed by an Auto Scaling Group 3. EC2 Web Server Security Group The Application tier 1. A Private Route Table \u2014 associated with 2 Private Subnets (1/AZ) 2. At least 2 EC2 instances managed by an Auto Scaling Group 3. EC2 Application Server Security Group The Data tier 1. A Private Route Table \u2014 associated with 2 Private Subnets (1/AZ) 2. A Database Security Group 3. A free Tier MySQL RDS Database Kubernetes API Users access the Kubernetes API using kubectl, client libraries, or by making REST requests. Both human users and Kubernetes service accounts can be authorized for API access. When a request reaches the API, it goes through several stages, illustrated in the following diagram: Steps: A request is sent through an API serves on port 443, protected by TLS. The API server presents a certificate. This certificate may be signed using a private ertificate authority (CA), or based on a public key infrastructure linked to a generally recognized CA. STEP 1: Authentication: Once TLS is established, the HTTP request moves to the Authentication step. Authentication modules include client certificates, password, and plain tokens, bootstrap tokens, and JSON Web Tokens (used for service accounts). Multiple authentication modules can be specified, in which case each one is tried in sequence, until one of them succeeds. If the request cannot be authenticated, it is rejected with HTTP status code 401. Otherwise, the user is authenticated as a specific username, and the username is available to subsequent steps to use in their decisions. Some authenticators also provide the group memberships of the user, while other authenticators do not. For more information about the different ways to Authenticate, check this url: See link STEP 2: Authorization So, after the request has been authenticated to be coming from a specific user, the request must next be authorized. A request MUST include: - the username of the requester - the requested action, and - the object affected by the action. Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode. When an administrator creates a cluster, they configure the authorization modules that should be used in the API server. If more than one authorization modules are configured, Kubernetes checks each module, and if any module authorizes the request, then the request can proceed. If all of the modules deny the request, then the request is denied (HTTP status code 403). See the below sample request: For more information about Authorization: check this url STEP 3: Admission control Admission Control modules are software modules that can modify or reject requests. They act on requests that create, modify, delete, or connect to (proxy) an object. Note: if any admission controller module rejects, then the request is immediately rejected. When multiple admission controllers are configured, they are called in order. STEP 4: Validation Once a request passes all admission controllers, it is validated using the validation routines for the corresponding API object, and then written to the object store. For more information about Admission control, check this url: For more information: check this url OpenSSL Setup steps: # Generate a ca.key with 2048bit $ openssl genrsa -out ca.key 2048 # According to the ca.key generate a ca.crt (use -days to set the certificate effective time): $ openssl req -x509 -new -nodes -key ca.key -subj \"/CN=${MASTER_IP}\" -days 10000 -out ca.crt # Generate a server.key with 2048bit: $ openssl genrsa -out server.key 2048 # Generate the certificate signing request based on the config file: $ openssl req -new -key server.key -out server.csr -config csr.conf # Generate the server certificate using the ca.key, ca.crt and server.csr: $ openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 10000 \\ -extensions v3_ext -extfile csr.conf # View the certificate signing request: $ openssl req -noout -text -in ./server.csr # View the certificate: $ openssl x509 -noout -text -in ./server.crt Cloud Native Security The 4C\u2019s of cloud Native Security Cloud: Each cloud provider makes security recommendations for running workloads securely in their environment. If you are running a Kubernetes cluster on your own hardware or a different cloud provider, consult your documentation for security best practices. For example, if you are running an EKS cluster you should understand Amazon Web Services Checkout the security rules: Common security Rules Cluster: When dealing with cluster security, there are two areas of concern for securing Kubernetes: 1.1. Securing the cluster components that are configurable 1.2. Securing the Components in the cluster (Your applications) 1.1. Securing the cluster For more info: Click-link 1.2. Securing the applications When it comes to securing the applications running n the cluster, the key question here is: how to secure the entire chain of applications in the cluster? For example: If you are running a service (Service Y) that is critical in a chain of other resources and a separate workload (Service Z) which is vulnerable to a resource exhaustion attack, then the risk of compromising Service Y is high if you do not limit the resources of Service Z. The Recommended security Areas for your application workloads To manage application security at the cluster level, a deep understanding of the following key concepts is required: RBAC Authorization (Access to the Kubernetes API) Authentication Application secrets management (and encrypting them in etcd at rest) Pod Security Quality of Service (and Cluster resource management) Network Policies TLS for Kubernetes Ingress RBAC Authorization: Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. The RBAC API declares four kinds of Kubernetes object: Role, ClusterRole, RoleBinding and ClusterRoleBinding. Role: Role contains rules that represent a set of permissions. Role is namespace specific, which means that a Role always sets permissions within a particular namespace. ClusterRole: clusterrole is used to define permissions on namespaced resources and be granted access within individual namespace(s). It\u2019s not namespace specific and can define permissions on cluster-scoped resources. Container: Container Security is a critical part of a comprehensive security assessment. For more on Container Security, read VMware Container Security Code: Some Coding Language Paradigms are comprise a variety of styles. styles are imperative, functional, logical, and object-oriented languages. Programmers can choose from these coding language paradigms to best-serve their needs for a specific project. The following are examples of each paradigm. Object-Oriented: Python, Java, C++, Imperative or Procedural: Cobol, Fortran, C Functional: Clojure Scala, Logical: Prolog, SQL Cloud code security focuses on code with several use cases, including infrastructure as code (IaC) security, application code security and software supply chain security. IaC Security: The key to a successful code security strategy for IaC is ensuring security is embedded directly in developer tools and workflows. By surfacing actionable feedback in code and embedding security guardrails in the build pipeline, IaC security empowers developers to ship infrastructure that\u2019s secure by default. Application Code Security: A strong code security strategy relies on secure coding best practices and code reviews to identify vulnerabilities. Through automated testing with technologies such as static application security testing (SAST) for custom code and software composition analysis (SCA) for open source code, code security solutions complement cloud workload protection by identifying CVEs as early as possible. Software Supply Chain Security Software supply chains comprise application and infrastructure components as well as the underlying pipelines, including version control systems (VCS), continuous integration and continuous deployment (CI/CD) pipelines, and registries. Software supply chain security is an important part of a strong code security strategy, as is understanding the connections between pipelines and infrastructure and application code across the development lifecycle. Operation systems An operating system is the most important software that runs on a computer. It manages the computer's memory and processes, as well as all of its software and hardware. It also allows you to communicate with the computer without knowing how to speak the computer's language. Without an operating system, a computer is useless. For the most part, the IT industry largely focuses on the top five OSs, including Apple macOS, Microsoft Windows, Google's Android OS, Linux Operating System, and Apple iOS Focus: Linux Linux is a family of open-source operating systems, which means they can be modified and distributed by anyone around the world. This is different from proprietary software like Windows, which can only be modified by the company that owns it. The advantages of Linux are that it is free, and there are many different distributions\u2014or versions\u2014you can choose from. Some common Linux distributions are Debian, Fedora and Red Hat, Ubuntu, and Linux Mint. SNo. Description Windows Linux 1. Directory listing dir ls -l 2. Rename a file ren mv 3. Copying a file copy cp 4. Moving a file move mv 5. Clear Screen cls clear 6. Delete file del rm 7. Check disk content chkdsk c: df 8. Search for a string in a file find grep 9. Create a new file type nul > soso.py touch/nano/vi 10. Returns your current directory location chdir pwd 11. Displays the time time date 12. Change the current directory cd cd 13. To create a new directory/folder md mkdir 14. To print something on the screen echo echo 15. To write in to files. edit vim(depends on editor) 16. To leave the terminal/command window. exit exit 17. To format a drive/partition. Format (C:) mke2fs or mformat 18. To list directory recursively. tree ls -R 19. To delete a directory. rmdir rm -rf/rmdir 22. To set environment variables. set var=value export var=value 23. To change file permissions. attribattrib +R collins.yamlattrib -R collins.yaml chown/chmod 24. To print the route packets trace to network host. tracert traceroute 25. Get systems network configuration ipconfig ifconfig 26. To print contents of a file. type cat 27. To send ICMP ECHO_REQUEST to network hosts. Ping Google.com ping 28. To query Internet name servers interactively. nslookup nslookup 29. For disk usage. chdisk du -s https://www.geeksforgeeks.org/linux-vs-windows-commands/ https://home.csulb.edu/~murdock/attrib.html#:~:text=Using%20the%20ATTRIB%20command%2C%20you,to%20as%20read%2Fwrite ). //for windows attributed","title":"3 Tier App"},{"location":"weekly/Random/3tier/#what-is-a-3-tier-application","text":"A 3-tier application will compose of 3 layers: a Presentation tier, an Application tier, and a Data tier.","title":"What is a 3 tier Application?"},{"location":"weekly/Random/3tier/#benefits-of-a-3-tier-application","text":"The benefits of using a 3-tier architecture include improved horizontal scalability, performance, and availability. With three tiers, each part can be developed concurrently by a different team of programmers coding in different languages from the other tier developers. Because the programming for a tier can be changed or relocated without affecting the other tiers, the 3-tier model makes it easier for an enterprise or software packager to continually evolve an application as new needs and opportunities arise. Existing applications or critical parts can be permanently or temporarily retained and encapsulated within the new tier of which it becomes a component.","title":"Benefits of a 3 tier Application?"},{"location":"weekly/Random/3tier/#the-3-different-layers-explained","text":"An example 3 tier application will compose of 3 layers: a Presentation tier, an application tier, and a Data tier. The Presentation tier is a graphical user interface (GUI) that communicates with the other two tiers\u2026in this layer, the users can directly access the web page, or an operating system's GUI. The is written in languages like built with HTML5, cascading style sheets (CSS) and JavaScript, and the presentation tier communicates with the other tiers through application program interface (API) calls. Application tier also known as the business logic tier\u2026 controls how the application functions. The application layer is written in a programming language such as Java and contains the business logic that supports the application's core functions. The underlying application tier can either be hosted on distributed servers in the cloud or on a dedicated in-house server, depending on how much processing power the application requires. Data tier contains the database servers, file shares, and anything that will be saved in the database... it can be hosted on-prem or in the cloud. Popular database systems for managing read/write access include MySQL, PostgreSQL, Microsoft Sample AWS 3-Tier Application The Presentation tier 1. A Public Route Table \u2014 associated with 2 Public Subnets (1/AZ) 2. At least 2 EC2 instances with a boot strapped Static Web Page \u2014 managed by an Auto Scaling Group 3. EC2 Web Server Security Group The Application tier 1. A Private Route Table \u2014 associated with 2 Private Subnets (1/AZ) 2. At least 2 EC2 instances managed by an Auto Scaling Group 3. EC2 Application Server Security Group The Data tier 1. A Private Route Table \u2014 associated with 2 Private Subnets (1/AZ) 2. A Database Security Group 3. A free Tier MySQL RDS Database","title":"The 3 different layers explained"},{"location":"weekly/Random/3tier/#kubernetes-api","text":"Users access the Kubernetes API using kubectl, client libraries, or by making REST requests. Both human users and Kubernetes service accounts can be authorized for API access. When a request reaches the API, it goes through several stages, illustrated in the following diagram:","title":"Kubernetes API"},{"location":"weekly/Random/3tier/#steps","text":"A request is sent through an API serves on port 443, protected by TLS. The API server presents a certificate. This certificate may be signed using a private ertificate authority (CA), or based on a public key infrastructure linked to a generally recognized CA. STEP 1: Authentication: Once TLS is established, the HTTP request moves to the Authentication step. Authentication modules include client certificates, password, and plain tokens, bootstrap tokens, and JSON Web Tokens (used for service accounts). Multiple authentication modules can be specified, in which case each one is tried in sequence, until one of them succeeds. If the request cannot be authenticated, it is rejected with HTTP status code 401. Otherwise, the user is authenticated as a specific username, and the username is available to subsequent steps to use in their decisions. Some authenticators also provide the group memberships of the user, while other authenticators do not. For more information about the different ways to Authenticate, check this url: See link STEP 2: Authorization So, after the request has been authenticated to be coming from a specific user, the request must next be authorized. A request MUST include: - the username of the requester - the requested action, and - the object affected by the action. Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode. When an administrator creates a cluster, they configure the authorization modules that should be used in the API server. If more than one authorization modules are configured, Kubernetes checks each module, and if any module authorizes the request, then the request can proceed. If all of the modules deny the request, then the request is denied (HTTP status code 403). See the below sample request: For more information about Authorization: check this url STEP 3: Admission control Admission Control modules are software modules that can modify or reject requests. They act on requests that create, modify, delete, or connect to (proxy) an object. Note: if any admission controller module rejects, then the request is immediately rejected. When multiple admission controllers are configured, they are called in order. STEP 4: Validation Once a request passes all admission controllers, it is validated using the validation routines for the corresponding API object, and then written to the object store. For more information about Admission control, check this url: For more information: check this url","title":"Steps:"},{"location":"weekly/Random/3tier/#openssl","text":"Setup steps: # Generate a ca.key with 2048bit $ openssl genrsa -out ca.key 2048 # According to the ca.key generate a ca.crt (use -days to set the certificate effective time): $ openssl req -x509 -new -nodes -key ca.key -subj \"/CN=${MASTER_IP}\" -days 10000 -out ca.crt # Generate a server.key with 2048bit: $ openssl genrsa -out server.key 2048 # Generate the certificate signing request based on the config file: $ openssl req -new -key server.key -out server.csr -config csr.conf # Generate the server certificate using the ca.key, ca.crt and server.csr: $ openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \\ -CAcreateserial -out server.crt -days 10000 \\ -extensions v3_ext -extfile csr.conf # View the certificate signing request: $ openssl req -noout -text -in ./server.csr # View the certificate: $ openssl x509 -noout -text -in ./server.crt","title":"OpenSSL"},{"location":"weekly/Random/3tier/#cloud-native-security","text":"The 4C\u2019s of cloud Native Security Cloud: Each cloud provider makes security recommendations for running workloads securely in their environment. If you are running a Kubernetes cluster on your own hardware or a different cloud provider, consult your documentation for security best practices. For example, if you are running an EKS cluster you should understand Amazon Web Services Checkout the security rules: Common security Rules Cluster: When dealing with cluster security, there are two areas of concern for securing Kubernetes: 1.1. Securing the cluster components that are configurable 1.2. Securing the Components in the cluster (Your applications) 1.1. Securing the cluster For more info: Click-link 1.2. Securing the applications When it comes to securing the applications running n the cluster, the key question here is: how to secure the entire chain of applications in the cluster? For example: If you are running a service (Service Y) that is critical in a chain of other resources and a separate workload (Service Z) which is vulnerable to a resource exhaustion attack, then the risk of compromising Service Y is high if you do not limit the resources of Service Z. The Recommended security Areas for your application workloads To manage application security at the cluster level, a deep understanding of the following key concepts is required: RBAC Authorization (Access to the Kubernetes API) Authentication Application secrets management (and encrypting them in etcd at rest) Pod Security Quality of Service (and Cluster resource management) Network Policies TLS for Kubernetes Ingress RBAC Authorization: Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. The RBAC API declares four kinds of Kubernetes object: Role, ClusterRole, RoleBinding and ClusterRoleBinding. Role: Role contains rules that represent a set of permissions. Role is namespace specific, which means that a Role always sets permissions within a particular namespace. ClusterRole: clusterrole is used to define permissions on namespaced resources and be granted access within individual namespace(s). It\u2019s not namespace specific and can define permissions on cluster-scoped resources. Container: Container Security is a critical part of a comprehensive security assessment. For more on Container Security, read VMware Container Security Code: Some Coding Language Paradigms are comprise a variety of styles. styles are imperative, functional, logical, and object-oriented languages. Programmers can choose from these coding language paradigms to best-serve their needs for a specific project. The following are examples of each paradigm. Object-Oriented: Python, Java, C++, Imperative or Procedural: Cobol, Fortran, C Functional: Clojure Scala, Logical: Prolog, SQL Cloud code security focuses on code with several use cases, including infrastructure as code (IaC) security, application code security and software supply chain security. IaC Security: The key to a successful code security strategy for IaC is ensuring security is embedded directly in developer tools and workflows. By surfacing actionable feedback in code and embedding security guardrails in the build pipeline, IaC security empowers developers to ship infrastructure that\u2019s secure by default. Application Code Security: A strong code security strategy relies on secure coding best practices and code reviews to identify vulnerabilities. Through automated testing with technologies such as static application security testing (SAST) for custom code and software composition analysis (SCA) for open source code, code security solutions complement cloud workload protection by identifying CVEs as early as possible. Software Supply Chain Security Software supply chains comprise application and infrastructure components as well as the underlying pipelines, including version control systems (VCS), continuous integration and continuous deployment (CI/CD) pipelines, and registries. Software supply chain security is an important part of a strong code security strategy, as is understanding the connections between pipelines and infrastructure and application code across the development lifecycle.","title":"Cloud Native Security"},{"location":"weekly/Random/3tier/#operation-systems","text":"An operating system is the most important software that runs on a computer. It manages the computer's memory and processes, as well as all of its software and hardware. It also allows you to communicate with the computer without knowing how to speak the computer's language. Without an operating system, a computer is useless. For the most part, the IT industry largely focuses on the top five OSs, including Apple macOS, Microsoft Windows, Google's Android OS, Linux Operating System, and Apple iOS","title":"Operation systems"},{"location":"weekly/Random/3tier/#focus-linux","text":"Linux is a family of open-source operating systems, which means they can be modified and distributed by anyone around the world. This is different from proprietary software like Windows, which can only be modified by the company that owns it. The advantages of Linux are that it is free, and there are many different distributions\u2014or versions\u2014you can choose from. Some common Linux distributions are Debian, Fedora and Red Hat, Ubuntu, and Linux Mint. SNo. Description Windows Linux 1. Directory listing dir ls -l 2. Rename a file ren mv 3. Copying a file copy cp 4. Moving a file move mv 5. Clear Screen cls clear 6. Delete file del rm 7. Check disk content chkdsk c: df 8. Search for a string in a file find grep 9. Create a new file type nul > soso.py touch/nano/vi 10. Returns your current directory location chdir pwd 11. Displays the time time date 12. Change the current directory cd cd 13. To create a new directory/folder md mkdir 14. To print something on the screen echo echo 15. To write in to files. edit vim(depends on editor) 16. To leave the terminal/command window. exit exit 17. To format a drive/partition. Format (C:) mke2fs or mformat 18. To list directory recursively. tree ls -R 19. To delete a directory. rmdir rm -rf/rmdir 22. To set environment variables. set var=value export var=value 23. To change file permissions. attribattrib +R collins.yamlattrib -R collins.yaml chown/chmod 24. To print the route packets trace to network host. tracert traceroute 25. Get systems network configuration ipconfig ifconfig 26. To print contents of a file. type cat 27. To send ICMP ECHO_REQUEST to network hosts. Ping Google.com ping 28. To query Internet name servers interactively. nslookup nslookup 29. For disk usage. chdisk du -s https://www.geeksforgeeks.org/linux-vs-windows-commands/ https://home.csulb.edu/~murdock/attrib.html#:~:text=Using%20the%20ATTRIB%20command%2C%20you,to%20as%20read%2Fwrite ). //for windows attributed","title":"Focus: Linux"},{"location":"weekly/Random/Disaster-rec/","text":"Disaster Recovery","title":"Disaster Recovery"},{"location":"weekly/Random/Disaster-rec/#disaster-recovery","text":"","title":"Disaster Recovery"},{"location":"weekly/Random/Helm-Terraform-jenkins/","text":"What are we going to achieve here? See the official terraform repo install terraform: Link Jenkins EKS LoadBalancer Reverse Proxy If you see this: 'It appears that your reverse proxy setup is broken', Follow the 3 image steps to resolve: see the issue apply changes Use IaC(terraform) Deploy sample application with Helm Case study: bitnami Example chart $ helm repo list //to list the repos $ helm repo add collins-bitnami https://charts.bitnami.com/bitnami //add the bitnami chart and name the chart $ helm repo list //to list the repos $ helm search repo mysql // Search for charts within the Bitnami repository $ helm install mydb collins-bitnami/mysql $ helm repo remove bitnami //to remove the chart Case study: Nginx Example chart $ helm repo add nginx-stable https://helm.nginx.com/stable $ helm repo update followed by: $ helm install nginxingress nginx-stable/nginx-ingress $ helm install ---name nginxingress nginx-stable/nginx-ingress","title":"Helm-Terraform-jenkins"},{"location":"weekly/Random/Helm-Terraform-jenkins/#what-are-we-going-to-achieve-here","text":"See the official terraform repo install terraform: Link","title":"What are we going to achieve here?"},{"location":"weekly/Random/Helm-Terraform-jenkins/#jenkins-eks-loadbalancer-reverse-proxy","text":"If you see this: 'It appears that your reverse proxy setup is broken', Follow the 3 image steps to resolve: see the issue apply changes","title":"Jenkins EKS LoadBalancer Reverse Proxy"},{"location":"weekly/Random/Helm-Terraform-jenkins/#use-iacterraform","text":"","title":"Use IaC(terraform)"},{"location":"weekly/Random/Helm-Terraform-jenkins/#deploy-sample-application-with-helm","text":"Case study: bitnami Example chart $ helm repo list //to list the repos $ helm repo add collins-bitnami https://charts.bitnami.com/bitnami //add the bitnami chart and name the chart $ helm repo list //to list the repos $ helm search repo mysql // Search for charts within the Bitnami repository $ helm install mydb collins-bitnami/mysql $ helm repo remove bitnami //to remove the chart Case study: Nginx Example chart $ helm repo add nginx-stable https://helm.nginx.com/stable $ helm repo update followed by: $ helm install nginxingress nginx-stable/nginx-ingress $ helm install ---name nginxingress nginx-stable/nginx-ingress","title":"Deploy sample application with Helm"},{"location":"weekly/Random/domain-github/","text":"GITHUB custom Domain with GOdaddy Deploy your site and go to settings --> pages Go to GoDaddy site, purchase and click on domain u wanna use and configure. Go to ths GitHub-doc: To create A records, point your apex domain to the IP addresses for GitHub Pages-Link Add the Domain to GitHub pages In GitHub, go to settings --> pages and add the Custom domain configure an APEX domain Copy the IP'S and create individual records for all the IP's As og 04/2023, these are the 4 IP's: 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Save the 4 A-RECORDS each as seen in my example of the 1'st IP Save 1 CNAME-RECORD as seen in my example Get the value that will be added in the Godaddy value section from ur github url: Go now and recheck the domain in GitHub pages and all should be good now.","title":"Domain Git GoDaddy"},{"location":"weekly/Random/domain-github/#github-custom-domain-with-godaddy","text":"Deploy your site and go to settings --> pages Go to GoDaddy site, purchase and click on domain u wanna use and configure. Go to ths GitHub-doc: To create A records, point your apex domain to the IP addresses for GitHub Pages-Link","title":"GITHUB custom Domain with GOdaddy"},{"location":"weekly/Random/domain-github/#add-the-domain-to-github-pages","text":"In GitHub, go to settings --> pages and add the Custom domain","title":"Add the Domain to GitHub pages"},{"location":"weekly/Random/domain-github/#configure-an-apex-domain","text":"Copy the IP'S and create individual records for all the IP's As og 04/2023, these are the 4 IP's: 185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 Save the 4 A-RECORDS each as seen in my example of the 1'st IP Save 1 CNAME-RECORD as seen in my example Get the value that will be added in the Godaddy value section from ur github url: Go now and recheck the domain in GitHub pages and all should be good now.","title":"configure an APEX domain"},{"location":"weekly/Random/eks-iam/","text":"EKS ADmins authentication to Clusters Setting UP Cluster IAM Groups for EKS Admins STS Assume Role Trust Policy (STS Assume Role) IAM Policy (EKS Fill Access) IAM Role (you will add this rolearn to the cluster authentication-configmap) IAM Group Policy (reference the IAM role) IAM Groups (create Iam group called: sosoadmins ) Setting UP Cluster IAM Users for the Group Create the users and add them to the sosoadmins group Ex: If you create these admins sosoadmin1 , sosoadmin2 , and add to the group, they will have access to the cluster To get the users, run command: aws sts get-caller-identity","title":"EKS-IAM"},{"location":"weekly/Random/eks-iam/#eks-admins-authentication-to-clusters","text":"","title":"EKS ADmins authentication to Clusters"},{"location":"weekly/Random/eks-iam/#setting-up-cluster-iam-groups-for-eks-admins","text":"STS Assume Role Trust Policy (STS Assume Role) IAM Policy (EKS Fill Access) IAM Role (you will add this rolearn to the cluster authentication-configmap) IAM Group Policy (reference the IAM role) IAM Groups (create Iam group called: sosoadmins )","title":"Setting UP Cluster IAM Groups for EKS Admins"},{"location":"weekly/Random/eks-iam/#setting-up-cluster-iam-users-for-the-group","text":"Create the users and add them to the sosoadmins group Ex: If you create these admins sosoadmin1 , sosoadmin2 , and add to the group, they will have access to the cluster To get the users, run command: aws sts get-caller-identity","title":"Setting UP Cluster IAM Users for the Group"},{"location":"weekly/Random/git-ecr-jenkins/","text":"What are we going to achieve here? Our developers have written some java base web-app and they are pushing that code to a git repo. We need a DevOps envineer who will comein and help setup our infrastruucture, Build an automation mechanish, that we can utilize to integrate this code for testing, Deliver, Deployment. Also, we will need these artifacts to be built into containerized application, build an ocastration and deploy this app to our end users. Steps Use IaC(terraform) to setup the required ingrastructures \"EC2, IAM, EKS\" install terraform: Link install jenkins server install jenkins server Link Install Docker Install Docker Link Install Git sudo yum install git -y Add Jenkins user to the Docker Group sudo usermod \u2013aG docker sosojenkinsadmin OR sudo usermod \u2013a -G docker jenkins restart the Jenkins server sudo service jenkins restart Reload the system Daemon sudo systemctl daemon-reload Restart the Docker Service sudo service docker restart Return to the Jenkins UI and install these 2 plugins: Install the pluginn called Docker and Docker Pipeline Update IAM role and and attach to instance, To Give instance ECR permissions Create an IAM role with Admin Or with the needed permissions for ECR and other services. Create ECR Repo vis same link, scroll to bottom of the page aws ecr create-repository --repository-name soso-repository --region us-east-1 When you create the ECR, go to the ECR in AWS Console and retreive the commands to push image. See the link for Creating a container image: Link Check authentication to ECR check to see if you can successfully login to ecr [my example ecr code below, USE YOURS ], you have to be root, sudo su - aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 088789840359.dkr.ecr.us-east-1.amazonaws.com Build and Push Docker sudo docker build -t sosodocs . Tag the sosodoc image sudo docker tag sosodocs 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Push image to repo sudo docker push 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Install AWS [ CloudBees AWS Credentials ] Plugin and configure Dashboard --> Manage Jenkins --> Plugin Manager setup aws credentials, get your AWS Access Key ID & Secret Access Key Create an AWS User called: jenkins Give this user permission to ECR: AmazonEC2ContainerRegistryFullAccess and AmazonECS_FullAccess Create/Download the Access-key for this user. Dashboard --> Manage Jenkins --> Manage Credentials --> System --> Global credentials (unrestricted) AWS Credentials |_ID |_Description |_Access Key ID |_Secret Access Key Create new Pipeline Job using below pipeline script with my aws creds use pool scm * * * pipeline { agent any environment { AWS_ACCOUNT_ID=\"088789840359\" AWS_DEFAULT_REGION=\"us-east-1\" IMAGE_REPO_NAME=\"soso-repository\" IMAGE_TAG=\"latest\" REPOSITORY_URI = \"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${IMAGE_REPO_NAME}\" } stages { stage('Logging into AWS ECR') { steps { script { sh \"aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com\" } } } stage('Cloning Git') { steps { checkout([$class: 'GitSCM', branches: [[name: '*/main']], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: '', url: 'https://github.com/sosotechnologies/aws-nodeJs-ecr-jenkins.git']]]) } } // Building Docker images stage('Building image') { steps{ script { dockerImage = docker.build \"${IMAGE_REPO_NAME}:${IMAGE_TAG}\" } } } // Uploading Docker images into AWS ECR stage('Pushing to ECR') { steps{ script { sh \"docker tag ${IMAGE_REPO_NAME}:${IMAGE_TAG} ${REPOSITORY_URI}:$IMAGE_TAG\" sh \"docker push ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${IMAGE_REPO_NAME}:${IMAGE_TAG}\" } } } } } Bonus!!! Test your pipeline with example codes Simple Jenkins pipeline Scripts for AWS Pipeline version 1 pipeline { agent any stages { stage('Welcome to sosotech') { steps { sh ''' aws --version ''' } } } } Pipeline version 2 pipeline { agent any stages { stage('Welcome to sosotech') { steps { sh ''' aws --version aws ec2 describe-instances ''' } } } } Pipeline version 3 pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } } Pipeline version 4 pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { withCredentials([aws(accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'all-in-one-devops', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY')]) { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } } } Pipeline version 5 For this step, i'll list buckets pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { withCredentials([aws(accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'all-in-one-devops', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY')]) { sh ''' aws --version aws s3api list-buckets --query \"Buckets[].Name\" ''' } } } } } Pipeline version 6 For this step, i'll create an IRSA and save IAM role in Jenkins credentials as 'irsa-creds' pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" WE_KNOW_TECHNOLOGY=credentials('irsa-creds') } stages { stage('Hello') { steps { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } }","title":"ECR-Jenkins"},{"location":"weekly/Random/git-ecr-jenkins/#what-are-we-going-to-achieve-here","text":"Our developers have written some java base web-app and they are pushing that code to a git repo. We need a DevOps envineer who will comein and help setup our infrastruucture, Build an automation mechanish, that we can utilize to integrate this code for testing, Deliver, Deployment. Also, we will need these artifacts to be built into containerized application, build an ocastration and deploy this app to our end users.","title":"What are we going to achieve here?"},{"location":"weekly/Random/git-ecr-jenkins/#steps","text":"","title":"Steps"},{"location":"weekly/Random/git-ecr-jenkins/#use-iacterraform-to-setup-the-required-ingrastructures-ec2-iam-eks","text":"install terraform: Link","title":"Use IaC(terraform) to setup the required ingrastructures \"EC2, IAM, EKS\""},{"location":"weekly/Random/git-ecr-jenkins/#install-jenkins-server","text":"install jenkins server Link","title":"install jenkins server"},{"location":"weekly/Random/git-ecr-jenkins/#install-docker","text":"Install Docker Link","title":"Install Docker"},{"location":"weekly/Random/git-ecr-jenkins/#install-git","text":"sudo yum install git -y","title":"Install Git"},{"location":"weekly/Random/git-ecr-jenkins/#add-jenkins-user-to-the-docker-group","text":"sudo usermod \u2013aG docker sosojenkinsadmin OR sudo usermod \u2013a -G docker jenkins","title":"Add Jenkins user to the Docker Group"},{"location":"weekly/Random/git-ecr-jenkins/#restart-the-jenkins-server","text":"sudo service jenkins restart","title":"restart the Jenkins server"},{"location":"weekly/Random/git-ecr-jenkins/#reload-the-system-daemon","text":"sudo systemctl daemon-reload","title":"Reload the system Daemon"},{"location":"weekly/Random/git-ecr-jenkins/#restart-the-docker-service","text":"sudo service docker restart","title":"Restart the Docker Service"},{"location":"weekly/Random/git-ecr-jenkins/#return-to-the-jenkins-ui-and-install-these-2-plugins","text":"Install the pluginn called Docker and Docker Pipeline","title":"Return to the Jenkins UI and install these 2 plugins:"},{"location":"weekly/Random/git-ecr-jenkins/#update-iam-role-and-and-attach-to-instance-to-give-instance-ecr-permissions","text":"Create an IAM role with Admin Or with the needed permissions for ECR and other services.","title":"Update IAM role and and attach to instance, To Give instance ECR permissions"},{"location":"weekly/Random/git-ecr-jenkins/#create-ecr-repo-vis-same-link-scroll-to-bottom-of-the-page","text":"aws ecr create-repository --repository-name soso-repository --region us-east-1 When you create the ECR, go to the ECR in AWS Console and retreive the commands to push image. See the link for Creating a container image: Link","title":"Create ECR Repo vis same link, scroll to bottom of the page"},{"location":"weekly/Random/git-ecr-jenkins/#check-authentication-to-ecr","text":"check to see if you can successfully login to ecr [my example ecr code below, USE YOURS ], you have to be root, sudo su - aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 088789840359.dkr.ecr.us-east-1.amazonaws.com","title":"Check authentication to ECR"},{"location":"weekly/Random/git-ecr-jenkins/#build-and-push-docker","text":"sudo docker build -t sosodocs . Tag the sosodoc image sudo docker tag sosodocs 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1 Push image to repo sudo docker push 088789840359.dkr.ecr.us-east-1.amazonaws.com/soso-repository:mkdocs-v1","title":"Build and Push Docker"},{"location":"weekly/Random/git-ecr-jenkins/#install-aws-cloudbees-aws-credentials-plugin-and-configure","text":"Dashboard --> Manage Jenkins --> Plugin Manager","title":"Install AWS [ CloudBees AWS Credentials ] Plugin and configure"},{"location":"weekly/Random/git-ecr-jenkins/#setup-aws-credentials-get-your-aws-access-key-id-secret-access-key","text":"Create an AWS User called: jenkins Give this user permission to ECR: AmazonEC2ContainerRegistryFullAccess and AmazonECS_FullAccess Create/Download the Access-key for this user. Dashboard --> Manage Jenkins --> Manage Credentials --> System --> Global credentials (unrestricted) AWS Credentials |_ID |_Description |_Access Key ID |_Secret Access Key","title":"setup aws credentials, get your AWS Access Key ID &amp; Secret Access Key"},{"location":"weekly/Random/git-ecr-jenkins/#create-new-pipeline-job-using-below-pipeline-script-with-my-aws-creds","text":"use pool scm * * * pipeline { agent any environment { AWS_ACCOUNT_ID=\"088789840359\" AWS_DEFAULT_REGION=\"us-east-1\" IMAGE_REPO_NAME=\"soso-repository\" IMAGE_TAG=\"latest\" REPOSITORY_URI = \"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${IMAGE_REPO_NAME}\" } stages { stage('Logging into AWS ECR') { steps { script { sh \"aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com\" } } } stage('Cloning Git') { steps { checkout([$class: 'GitSCM', branches: [[name: '*/main']], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: '', url: 'https://github.com/sosotechnologies/aws-nodeJs-ecr-jenkins.git']]]) } } // Building Docker images stage('Building image') { steps{ script { dockerImage = docker.build \"${IMAGE_REPO_NAME}:${IMAGE_TAG}\" } } } // Uploading Docker images into AWS ECR stage('Pushing to ECR') { steps{ script { sh \"docker tag ${IMAGE_REPO_NAME}:${IMAGE_TAG} ${REPOSITORY_URI}:$IMAGE_TAG\" sh \"docker push ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/${IMAGE_REPO_NAME}:${IMAGE_TAG}\" } } } } }","title":"Create new Pipeline Job using below pipeline script with my aws creds"},{"location":"weekly/Random/git-ecr-jenkins/#bonus","text":"Test your pipeline with example codes","title":"Bonus!!!"},{"location":"weekly/Random/git-ecr-jenkins/#simple-jenkins-pipeline-scripts-for-aws","text":"Pipeline version 1 pipeline { agent any stages { stage('Welcome to sosotech') { steps { sh ''' aws --version ''' } } } }","title":"Simple Jenkins pipeline Scripts for AWS"},{"location":"weekly/Random/git-ecr-jenkins/#pipeline-version-2","text":"pipeline { agent any stages { stage('Welcome to sosotech') { steps { sh ''' aws --version aws ec2 describe-instances ''' } } } }","title":"Pipeline version 2"},{"location":"weekly/Random/git-ecr-jenkins/#pipeline-version-3","text":"pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } }","title":"Pipeline version 3"},{"location":"weekly/Random/git-ecr-jenkins/#pipeline-version-4","text":"pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { withCredentials([aws(accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'all-in-one-devops', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY')]) { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } } }","title":"Pipeline version 4"},{"location":"weekly/Random/git-ecr-jenkins/#pipeline-version-5","text":"For this step, i'll list buckets pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" } stages { stage('Welcome to sosotech') { steps { withCredentials([aws(accessKeyVariable: 'AWS_ACCESS_KEY_ID', credentialsId: 'all-in-one-devops', secretKeyVariable: 'AWS_SECRET_ACCESS_KEY')]) { sh ''' aws --version aws s3api list-buckets --query \"Buckets[].Name\" ''' } } } } }","title":"Pipeline version 5"},{"location":"weekly/Random/git-ecr-jenkins/#pipeline-version-6","text":"For this step, i'll create an IRSA and save IAM role in Jenkins credentials as 'irsa-creds' pipeline { agent any environment { AWS_DEFAULT_REGION=\"us-east-1\" WE_KNOW_TECHNOLOGY=credentials('irsa-creds') } stages { stage('Hello') { steps { sh ''' aws --version aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.micro\" --query \"Reservations[].Instances[].InstanceId\" ''' } } } }","title":"Pipeline version 6"},{"location":"weekly/Random/k8sapi%20copy/","text":"Tables Are Cool col 1 is b sdvvrbbbbbbbbrggg $1600 SNo. Windows Linux Description","title":"K8sapi copy"},{"location":"weekly/Random/k8sapi/","text":"","title":"Kubernetes API"},{"location":"weekly/Random/operating-systems/","text":"linux commands cat /etc/os-release adduser devops","title":"OS Types Commands"},{"location":"weekly/Random/scripts/","text":"All Scripts Get Secrets expiration dates #!/bin/sh DAYS=\"604800\" YOUR_WEBHOOK_URL=\"collins-slack-channel\" echo \"Input the cluster name\" read -r cluster echo \"Input the namespace\" read -r namespace for i in `kubectl --context $cluster get cm -n $namespace | awk '{print $1}' | grep -vi name` do filename=\"cm_$i'_'$namespace.pem\" if `kubectl --context $cluster -n $namespace get cm $i -o yaml | grep pem ` then kubectl --context $cluster -n $namespace get cm $i -o yaml | yq .data > $filename cert_expr_date=$(openssl x509 -enddate -noout -in $filename | awk -F\"=\" '{print $2}') openssl x509 -enddate -noout -in $filename -checkend \"$DAYS\" | grep -q 'Certificate will expire' if [ $? -eq 0 ] then curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in configmap with name '$i' in namespace '$namespace' will expire in 7 days \"}' $YOUR_WEBHOOK_URL curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in configmap with name '$i' in namespace '$namespace' will expire on '$cert_expr_date' \"}' $YOUR_WEBHOOK_URL fi curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in configmap with name '$i' in namespace '$namespace' will expire on '$cert_expr_date' \"}' $YOUR_WEBHOOK_URL fi done for i in `kubectl --context $cluster get secret -n $namespace | awk '{print $1}' | grep -vi name` do filename=\"secret_$i'_'$namespace.pem\" if `kubectl --context $cluster -n $namespace get secret $i -o yaml | grep pem ` then kubectl --context $cluster -n $namespace get secret $i -o yaml |yq .data | grep crt | awk -F\":\" '{print $2}' | base64 -d > $filename cert_expr_date=$(openssl x509 -enddate -noout -in $filename | awk -F\"=\" '{print $2}') openssl x509 -enddate -noout -in $filename -checkend \"$DAYS\" | grep -q 'Certificate will expire' if [ $? -eq 0 ] then curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in secret with name '$i' in namespace '$namespace' will expire in 7 days \"}' $YOUR_WEBHOOK_URL curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in configmap with name '$i' in namespace '$namespace' will expire on '$cert_expr_date' \"}' $YOUR_WEBHOOK_URL fi curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in configmap with name '$i' in namespace '$namespace' will expire on '$cert_expr_date' \"}' $YOUR_WEBHOOK_URL fi done SonarQube - Ubuntu VERSION=\"18.04\" #!/bin/bash cp /etc/sysctl.conf /root/sysctl.conf_backup cat <<EOT> /etc/sysctl.conf vm.max_map_count=262144 fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 EOT cp /etc/security/limits.conf /root/sec_limit.conf_backup cat <<EOT> /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 409 EOT sudo apt-get update -y sudo apt-get install openjdk-11-jdk -y sudo update-alternatives --config java java -version sudo apt update wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' sudo apt install postgresql postgresql-contrib -y #sudo -u postgres psql -c \"SELECT version();\" sudo systemctl enable postgresql.service sudo systemctl start postgresql.service sudo echo \"postgres:admin123\" | chpasswd runuser -l postgres -c \"createuser sonar\" sudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\" sudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\" sudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\" systemctl restart postgresql #systemctl status -l postgresql netstat -tulpena | grep postgres sudo mkdir -p /sonarqube/ cd /sonarqube/ sudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip sudo apt-get install zip -y sudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/ sudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube sudo groupadd sonar sudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar sudo chown sonar:sonar /opt/sonarqube/ -R cp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup cat <<EOT> /opt/sonarqube/conf/sonar.properties sonar.jdbc.username=sonar sonar.jdbc.password=admin123 sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.web.host=0.0.0.0 sonar.web.port=9000 sonar.web.javaAdditionalOpts=-server sonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError sonar.log.level=INFO sonar.path.logs=logs EOT cat <<EOT> /etc/systemd/system/sonarqube.service [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target EOT systemctl daemon-reload systemctl enable sonarqube.service #systemctl start sonarqube.service #systemctl status -l sonarqube.service apt-get install nginx -y rm -rf /etc/nginx/sites-enabled/default rm -rf /etc/nginx/sites-available/default cat <<EOT> /etc/nginx/sites-available/sonarqube server{ listen 80; server_name sonarqube.groophy.in; access_log /var/log/nginx/sonar.access.log; error_log /var/log/nginx/sonar.error.log; proxy_buffers 16 64k; proxy_buffer_size 128k; location / { proxy_pass http://127.0.0.1:9000; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_set_header Host \\$host; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto http; } } EOT ln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube systemctl enable nginx.service #systemctl restart nginx.service sudo ufw allow 80,9000,9001/tcp echo \"System reboot in 30 sec\" sleep 30 reboot Jenkins - Ubuntu VERSION=\"20.04.6 LTS #!/bin/bash sudo apt update sudo apt install openjdk-11-jdk -y sudo apt install maven -y curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc > /dev/null echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list > /dev/null sudo apt-get update sudo apt-get install jenkins -y","title":"Scripts"},{"location":"weekly/Random/scripts/#all-scripts","text":"","title":"All Scripts"},{"location":"weekly/Random/scripts/#get-secrets-expiration-dates","text":"#!/bin/sh DAYS=\"604800\" YOUR_WEBHOOK_URL=\"collins-slack-channel\" echo \"Input the cluster name\" read -r cluster echo \"Input the namespace\" read -r namespace for i in `kubectl --context $cluster get cm -n $namespace | awk '{print $1}' | grep -vi name` do filename=\"cm_$i'_'$namespace.pem\" if `kubectl --context $cluster -n $namespace get cm $i -o yaml | grep pem ` then kubectl --context $cluster -n $namespace get cm $i -o yaml | yq .data > $filename cert_expr_date=$(openssl x509 -enddate -noout -in $filename | awk -F\"=\" '{print $2}') openssl x509 -enddate -noout -in $filename -checkend \"$DAYS\" | grep -q 'Certificate will expire' if [ $? -eq 0 ] then curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in configmap with name '$i' in namespace '$namespace' will expire in 7 days \"}' $YOUR_WEBHOOK_URL curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in configmap with name '$i' in namespace '$namespace' will expire on '$cert_expr_date' \"}' $YOUR_WEBHOOK_URL fi curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in configmap with name '$i' in namespace '$namespace' will expire on '$cert_expr_date' \"}' $YOUR_WEBHOOK_URL fi done for i in `kubectl --context $cluster get secret -n $namespace | awk '{print $1}' | grep -vi name` do filename=\"secret_$i'_'$namespace.pem\" if `kubectl --context $cluster -n $namespace get secret $i -o yaml | grep pem ` then kubectl --context $cluster -n $namespace get secret $i -o yaml |yq .data | grep crt | awk -F\":\" '{print $2}' | base64 -d > $filename cert_expr_date=$(openssl x509 -enddate -noout -in $filename | awk -F\"=\" '{print $2}') openssl x509 -enddate -noout -in $filename -checkend \"$DAYS\" | grep -q 'Certificate will expire' if [ $? -eq 0 ] then curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in secret with name '$i' in namespace '$namespace' will expire in 7 days \"}' $YOUR_WEBHOOK_URL curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in configmap with name '$i' in namespace '$namespace' will expire on '$cert_expr_date' \"}' $YOUR_WEBHOOK_URL fi curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"Certificate in configmap with name '$i' in namespace '$namespace' will expire on '$cert_expr_date' \"}' $YOUR_WEBHOOK_URL fi done","title":"Get Secrets expiration dates"},{"location":"weekly/Random/scripts/#sonarqube-ubuntu-version1804","text":"#!/bin/bash cp /etc/sysctl.conf /root/sysctl.conf_backup cat <<EOT> /etc/sysctl.conf vm.max_map_count=262144 fs.file-max=65536 ulimit -n 65536 ulimit -u 4096 EOT cp /etc/security/limits.conf /root/sec_limit.conf_backup cat <<EOT> /etc/security/limits.conf sonarqube - nofile 65536 sonarqube - nproc 409 EOT sudo apt-get update -y sudo apt-get install openjdk-11-jdk -y sudo update-alternatives --config java java -version sudo apt update wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' sudo apt install postgresql postgresql-contrib -y #sudo -u postgres psql -c \"SELECT version();\" sudo systemctl enable postgresql.service sudo systemctl start postgresql.service sudo echo \"postgres:admin123\" | chpasswd runuser -l postgres -c \"createuser sonar\" sudo -i -u postgres psql -c \"ALTER USER sonar WITH ENCRYPTED PASSWORD 'admin123';\" sudo -i -u postgres psql -c \"CREATE DATABASE sonarqube OWNER sonar;\" sudo -i -u postgres psql -c \"GRANT ALL PRIVILEGES ON DATABASE sonarqube to sonar;\" systemctl restart postgresql #systemctl status -l postgresql netstat -tulpena | grep postgres sudo mkdir -p /sonarqube/ cd /sonarqube/ sudo curl -O https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-8.3.0.34182.zip sudo apt-get install zip -y sudo unzip -o sonarqube-8.3.0.34182.zip -d /opt/ sudo mv /opt/sonarqube-8.3.0.34182/ /opt/sonarqube sudo groupadd sonar sudo useradd -c \"SonarQube - User\" -d /opt/sonarqube/ -g sonar sonar sudo chown sonar:sonar /opt/sonarqube/ -R cp /opt/sonarqube/conf/sonar.properties /root/sonar.properties_backup cat <<EOT> /opt/sonarqube/conf/sonar.properties sonar.jdbc.username=sonar sonar.jdbc.password=admin123 sonar.jdbc.url=jdbc:postgresql://localhost/sonarqube sonar.web.host=0.0.0.0 sonar.web.port=9000 sonar.web.javaAdditionalOpts=-server sonar.search.javaOpts=-Xmx512m -Xms512m -XX:+HeapDumpOnOutOfMemoryError sonar.log.level=INFO sonar.path.logs=logs EOT cat <<EOT> /etc/systemd/system/sonarqube.service [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=sonar Group=sonar Restart=always LimitNOFILE=65536 LimitNPROC=4096 [Install] WantedBy=multi-user.target EOT systemctl daemon-reload systemctl enable sonarqube.service #systemctl start sonarqube.service #systemctl status -l sonarqube.service apt-get install nginx -y rm -rf /etc/nginx/sites-enabled/default rm -rf /etc/nginx/sites-available/default cat <<EOT> /etc/nginx/sites-available/sonarqube server{ listen 80; server_name sonarqube.groophy.in; access_log /var/log/nginx/sonar.access.log; error_log /var/log/nginx/sonar.error.log; proxy_buffers 16 64k; proxy_buffer_size 128k; location / { proxy_pass http://127.0.0.1:9000; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504; proxy_redirect off; proxy_set_header Host \\$host; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto http; } } EOT ln -s /etc/nginx/sites-available/sonarqube /etc/nginx/sites-enabled/sonarqube systemctl enable nginx.service #systemctl restart nginx.service sudo ufw allow 80,9000,9001/tcp echo \"System reboot in 30 sec\" sleep 30 reboot","title":"SonarQube - Ubuntu VERSION=\"18.04\""},{"location":"weekly/Random/scripts/#jenkins-ubuntu-version20046-lts","text":"#!/bin/bash sudo apt update sudo apt install openjdk-11-jdk -y sudo apt install maven -y curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \\ /usr/share/keyrings/jenkins-keyring.asc > /dev/null echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \\ https://pkg.jenkins.io/debian-stable binary/ | sudo tee \\ /etc/apt/sources.list.d/jenkins.list > /dev/null sudo apt-get update sudo apt-get install jenkins -y","title":"Jenkins - Ubuntu VERSION=\"20.04.6 LTS"},{"location":"weekly/SSL/ssl/","text":"I will use my image deployment and service for this demo deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: angels name: angels namespace: angelnamespace spec: replicas: 1 selector: matchLabels: app: angels strategy: {} template: metadata: creationTimestamp: null labels: app: angels spec: containers: - image: sosotech/angelpalms:1.0.0 name: angelpalms ports: - containerPort: 5000 resources: {} status: {} service.yaml k expose deploy angels --name=angel-svc -n angelnamespace --port=5000 --target-port=5000 --type=ClusterIP --dry-run=client -o yaml > service.yaml apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: angels name: angel-svc namespace: angelnamespace spec: ports: - port: 5000 protocol: TCP targetPort: 5000 selector: app: angels type: ClusterIP status: loadBalancer: {} Check the helm url: Helm Link Check the istio url: Istio Helm Link kubectl create ns istio-system kubectl create ns angelnamespace helm repo ls helm repo add istio https://istio-release.storage.googleapis.com/charts helm repo update helm search repo istio helm ls -A helm install istio-base istio/base -n istio-system helm install istio-istiod istio/istiod -n istio-system helm install istio-gateway istio/gateway -n istio-system helm ls -A kubectl get po -A clone this repo: TLS Yamls Get istio-ingressgateway deployment labels to add to the label area in below yaml kubectl get deploy -n istio-system kubectl get deploy istio-gateway -n istio-system --show-labels gateway.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: general-gateway namespace: istio-system labels: app.kubernetes.io/instance: ingressgateway app: ingressgateway istio: ingressgateway spec: selector: istio: ingressgateway servers: - hosts: - '*' port: name: http number: 80 protocol: HTTP # Upgrade HTTP to HTTPS tls: httpsRedirect: true - hosts: - '*' port: name: https number: 443 protocol: HTTPS tls: mode: SIMPLE credentialName: gateway-certs # app.kubernetes.io/managed-by=Helm # app.kubernetes.io/name=ingressgateway # app.kubernetes.io/version=1.16.1 # app=ingressgateway # helm.sh/chart=gateway-1.16.1 # istio=ingressgateway virtualservices.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: labels: name: angelpalms-vir-ser namespace: angelnamespace spec: gateways: - istio-system/general-gateway # This is the name and namespace from the gateway.yaml hosts: - 'angels.angelpalmshhcare.com' http: - retries: attempts: 3 perTryTimeout: 2s match: - uri: prefix: / route: - destination: host: angel-svc.angelnamespace.svc.cluster.local #first is angelpamls service name, second is namespace port: number: 5000 sonarqube.skyfieldtechnologies.com jenkins.skyfieldtechnologies.com minio.skyfieldtechnologies.com","title":"Ssl"},{"location":"weekly/SSL/ssl/#i-will-use-my-image-deployment-and-service-for-this-demo","text":"deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: angels name: angels namespace: angelnamespace spec: replicas: 1 selector: matchLabels: app: angels strategy: {} template: metadata: creationTimestamp: null labels: app: angels spec: containers: - image: sosotech/angelpalms:1.0.0 name: angelpalms ports: - containerPort: 5000 resources: {} status: {} service.yaml k expose deploy angels --name=angel-svc -n angelnamespace --port=5000 --target-port=5000 --type=ClusterIP --dry-run=client -o yaml > service.yaml apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: angels name: angel-svc namespace: angelnamespace spec: ports: - port: 5000 protocol: TCP targetPort: 5000 selector: app: angels type: ClusterIP status: loadBalancer: {} Check the helm url: Helm Link Check the istio url: Istio Helm Link kubectl create ns istio-system kubectl create ns angelnamespace helm repo ls helm repo add istio https://istio-release.storage.googleapis.com/charts helm repo update helm search repo istio helm ls -A helm install istio-base istio/base -n istio-system helm install istio-istiod istio/istiod -n istio-system helm install istio-gateway istio/gateway -n istio-system helm ls -A kubectl get po -A clone this repo: TLS Yamls Get istio-ingressgateway deployment labels to add to the label area in below yaml kubectl get deploy -n istio-system kubectl get deploy istio-gateway -n istio-system --show-labels","title":"I will use my image deployment and service for this demo"},{"location":"weekly/SSL/ssl/#gatewayyaml","text":"apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: general-gateway namespace: istio-system labels: app.kubernetes.io/instance: ingressgateway app: ingressgateway istio: ingressgateway spec: selector: istio: ingressgateway servers: - hosts: - '*' port: name: http number: 80 protocol: HTTP # Upgrade HTTP to HTTPS tls: httpsRedirect: true - hosts: - '*' port: name: https number: 443 protocol: HTTPS tls: mode: SIMPLE credentialName: gateway-certs # app.kubernetes.io/managed-by=Helm # app.kubernetes.io/name=ingressgateway # app.kubernetes.io/version=1.16.1 # app=ingressgateway # helm.sh/chart=gateway-1.16.1 # istio=ingressgateway","title":"gateway.yaml"},{"location":"weekly/SSL/ssl/#virtualservicesyaml","text":"apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: labels: name: angelpalms-vir-ser namespace: angelnamespace spec: gateways: - istio-system/general-gateway # This is the name and namespace from the gateway.yaml hosts: - 'angels.angelpalmshhcare.com' http: - retries: attempts: 3 perTryTimeout: 2s match: - uri: prefix: / route: - destination: host: angel-svc.angelnamespace.svc.cluster.local #first is angelpamls service name, second is namespace port: number: 5000 sonarqube.skyfieldtechnologies.com jenkins.skyfieldtechnologies.com minio.skyfieldtechnologies.com","title":"virtualservices.yaml"},{"location":"weekly/SonarQube/sonarqube/","text":"Wheck Style analysis Quality Check, Gate https://docs.sonarqube.org/latest/analyzing-source-code/scanners/jenkins-extension-sonarqube/ https://docs.sonarqube.org/latest/analyzing-source-code/scanners/sonarscanner/ Create project Create a new project called: sosotech Generate a token called: sosotech-token 847ff2f312def534eb71b3a81750227f96673c24 Quality gate Create a New Quality gate Add conditions Next: Add the quality gate to the sonarqube project Go To: Projects --> Quality Gate --> Projects Setting --> webhooks Create a webhook called: soso-sonar-jenkins-webhook . you can name watever you choose - The webhook url is my jenkins url and the name - sonarqube-webhook like so http://34.230.86.243:8080/sonarqube-webhook","title":"Sonarqube"},{"location":"weekly/SonarQube/sonarqube/#create-project","text":"Create a new project called: sosotech Generate a token called: sosotech-token 847ff2f312def534eb71b3a81750227f96673c24","title":"Create project"},{"location":"weekly/SonarQube/sonarqube/#quality-gate","text":"Create a New Quality gate Add conditions Next: Add the quality gate to the sonarqube project Go To: Projects --> Quality Gate --> Projects Setting --> webhooks Create a webhook called: soso-sonar-jenkins-webhook . you can name watever you choose - The webhook url is my jenkins url and the name - sonarqube-webhook like so http://34.230.86.243:8080/sonarqube-webhook","title":"Quality gate"},{"location":"weekly/Terraform/terraform/","text":"Terraform Working Links for more information see link: Sosotech Terraform Repo For deploying Helm Charts with Terraform: Helm, Terraform, Jenkins Github Repositories used for this course Terraform on AWS EKS Kubernetes IaC SRE- 50 Real-World Demos Course Presentation Kubernetes Fundamentals Important Note: Please go to these repositories and FORK these repositories and make use of them during the course. Scorage class: Link Persistent volume: Link Note . You Must have an understanding of volume_handle - (Required) A map that specifies static properties of a volume. For more info see Kubernetes reference. This is the way I am mapping the EFS I.D to the Volume. The volume handles is not neccessary for dynamic provisioning, because you will not creating a PV file, and you will be using storage Provisioner to dynamically create the PV's. Pass the provisioner in your storage Class Object. For your mount_path = \"/data\", any data in the mountpath will be store in your EFS. You can name to whatever u choose k exec --stdin --tty soso-pod --/bin/sh MY ADVISE - use different S3 buckets for critical backends, so If an s3 is mistakenly deleted some how, you can create a new bucket and re-initialize that specific resource group. For modules: See link StateFile ```terraform state list","title":"Terraform Working Links"},{"location":"weekly/Terraform/terraform/#terraform-working-links","text":"for more information see link: Sosotech Terraform Repo For deploying Helm Charts with Terraform: Helm, Terraform, Jenkins","title":"Terraform Working Links"},{"location":"weekly/Terraform/terraform/#github-repositories-used-for-this-course","text":"Terraform on AWS EKS Kubernetes IaC SRE- 50 Real-World Demos Course Presentation Kubernetes Fundamentals Important Note: Please go to these repositories and FORK these repositories and make use of them during the course. Scorage class: Link Persistent volume: Link Note . You Must have an understanding of volume_handle - (Required) A map that specifies static properties of a volume. For more info see Kubernetes reference. This is the way I am mapping the EFS I.D to the Volume. The volume handles is not neccessary for dynamic provisioning, because you will not creating a PV file, and you will be using storage Provisioner to dynamically create the PV's. Pass the provisioner in your storage Class Object. For your mount_path = \"/data\", any data in the mountpath will be store in your EFS. You can name to whatever u choose k exec --stdin --tty soso-pod --/bin/sh MY ADVISE - use different S3 buckets for critical backends, so If an s3 is mistakenly deleted some how, you can create a new bucket and re-initialize that specific resource group. For modules: See link","title":"Github Repositories used for this course"},{"location":"weekly/Terraform/terraform/#statefile","text":"```terraform state list","title":"StateFile"},{"location":"weekly/mysql-windows/mysql-windows/","text":"MySQL Windows Installation Community Edition Link: LINK setup a minikube cluster choco install minikube minikube start Alter password in workbench alter user 'root'@'localhost' identified with mysql_native_password by 'your-soso-password'; QUIT Create a Database SHOW DATABASES; CREATE DATABASE sosotech; USE sosotech; Create Tables in the Database CREATE TABLE sosologin ( id INT unsigned NOT NULL AUTO_INCREMENT, # Unique ID for the record name VARCHAR(150) NOT NULL, # Name of the cat owner VARCHAR(150) NOT NULL, # Owner of the cat birth DATE NOT NULL, # Birthday of the cat PRIMARY KEY (id) # Make the id the primary key ); DESCRIBE sosologin Adding records into a table. INSERT INTO sosologin ( name, owner, birth) VALUES ( 'Collins', 'Macaz', '2004-01-03' ), ( 'Macaz', 'Macaz', '2014-11-13' ), ( 'Collins', 'Afa', '2012-05-21' ); Retrieving records from a table. SELECT * FROM sosologin; SELECT name FROM sosologin WHERE owner = 'afa'; For more info: SEE LINK","title":"MySQL Windows"},{"location":"weekly/mysql-windows/mysql-windows/#mysql-windows","text":"","title":"MySQL Windows"},{"location":"weekly/mysql-windows/mysql-windows/#installation","text":"Community Edition Link: LINK","title":"Installation"},{"location":"weekly/mysql-windows/mysql-windows/#setup-a-minikube-cluster","text":"choco install minikube minikube start Alter password in workbench alter user 'root'@'localhost' identified with mysql_native_password by 'your-soso-password'; QUIT Create a Database SHOW DATABASES; CREATE DATABASE sosotech; USE sosotech; Create Tables in the Database CREATE TABLE sosologin ( id INT unsigned NOT NULL AUTO_INCREMENT, # Unique ID for the record name VARCHAR(150) NOT NULL, # Name of the cat owner VARCHAR(150) NOT NULL, # Owner of the cat birth DATE NOT NULL, # Birthday of the cat PRIMARY KEY (id) # Make the id the primary key ); DESCRIBE sosologin Adding records into a table. INSERT INTO sosologin ( name, owner, birth) VALUES ( 'Collins', 'Macaz', '2004-01-03' ), ( 'Macaz', 'Macaz', '2014-11-13' ), ( 'Collins', 'Afa', '2012-05-21' ); Retrieving records from a table. SELECT * FROM sosologin; SELECT name FROM sosologin WHERE owner = 'afa'; For more info: SEE LINK","title":"setup a minikube cluster"},{"location":"weekly/web-development/web-dev/","text":"FRONTEND Frontend Development (stuff that runs in the browser) \u2013 CSS, HTML, JS Backend or Server-side (stuff that runs on the server) \u2013 Python, C, Java, \u2026 HTML \u2013 Describes the things on the page. CSS \u2013 helps describes the HTML elements, like font size, shadow, border\u2026 JS \u2013 more like providing motion, like walk, dance\u2026 HTML A typical HTML Skeleton will look like: Description <!DOCTYPE> All HTML documents must start with a <!DOCTYPE> declaration. The declaration is not an HTML tag. It is an \"information\" to the browser about what document type to expect. The element is a container for metadata (data about data) and is placed between the tag and the tag <!DOCTYPE html> <html> <head> <title>This is sosotech</title> </head> <body> <!-- Enter your content here -->> </body> </html> Here are some of the commonly used html open and closing tags. For more info: See Link Start Tag Element content End tag My First Heading My first paragraph None none *Demo * - Create a html file: soso.html - Add some content and open the file with chrome HTML Paragraphs The HTML element defines a paragraph. Example paragraphs html file <!DOCTYPE html> <html> <body> <p> This soso paragraph contains a lot of lines. </p> <p> This soso paragraph contains some spacing but that\u2019s fine. </p> </body> </html> HTML Headings HTML headings are titles or subtitles that you want to display on a webpage. Heading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 <!DOCTYPE html> <html> <body> <h1>Collins</h1> <p> This soso paragraph contains a lot of lines. </p> <p> This soso paragraph contains some spacing but that\u2019s fine. </p> </body> </html> <!DOCTYPE html> <html> <body> <h1>SoSoTechnologies</h1> <p> This soso paragraph contains a lot of lines. </p> <h2>Department</h2> <h3>Education</h3> <p> This soso paragraph contains some spacing vut that\u2019s fine </p> </body> </html>","title":"Web Development"},{"location":"weekly/web-development/web-dev/#frontend","text":"Frontend Development (stuff that runs in the browser) \u2013 CSS, HTML, JS Backend or Server-side (stuff that runs on the server) \u2013 Python, C, Java, \u2026 HTML \u2013 Describes the things on the page. CSS \u2013 helps describes the HTML elements, like font size, shadow, border\u2026 JS \u2013 more like providing motion, like walk, dance\u2026","title":"FRONTEND"},{"location":"weekly/web-development/web-dev/#html","text":"A typical HTML Skeleton will look like: Description <!DOCTYPE> All HTML documents must start with a <!DOCTYPE> declaration. The declaration is not an HTML tag. It is an \"information\" to the browser about what document type to expect. The element is a container for metadata (data about data) and is placed between the tag and the tag <!DOCTYPE html> <html> <head> <title>This is sosotech</title> </head> <body> <!-- Enter your content here -->> </body> </html> Here are some of the commonly used html open and closing tags. For more info: See Link Start Tag Element content End tag","title":"HTML"}]}